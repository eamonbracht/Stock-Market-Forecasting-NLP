{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def fix_date(df): \n",
    "    dx = df[\"Date\"].apply(lambda x: x.split(\" \"))\n",
    "    dx = dx.apply(lambda x: \"{} {} {}\".format(x[1][:-3], x[0], x[2])) \n",
    "    return dx \n",
    "\n",
    "data_names = []\n",
    "file_names = []\n",
    "companies = [\"MMM\", \"SYF\", \"BAYRY\", \"HON\"]\n",
    "for w in companies:\n",
    "    for i in np.arange(2009,2019):\n",
    "        file_names.append(w+\"_\"+str(i)+\".csv\")\n",
    "        data_names.append(w+\"_\"+str(i))\n",
    "mmm = pd.read_pickle('3m_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
    "hon = pd.read_pickle('honeywell_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
    "syf = pd.read_pickle('synchrony_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
    "bayry = pd.read_pickle('bayer_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
    "mmm[\"Date\"] = fix_date(mmm)\n",
    "hon[\"Date\"] = fix_date(hon)\n",
    "syf[\"Date\"] = fix_date(syf)\n",
    "bayry[\"Date\"] = fix_date(bayry)\n",
    "\n",
    "\n",
    "def process_pickles(company_name, frame, dict_to_add):\n",
    "    temp_dict = {}\n",
    "    frame[\"Year\"] = frame[\"Date\"].apply(lambda x: x.split(\" \")[2])\n",
    "    years = frame[\"Year\"].unique()\n",
    "    for i in years:\n",
    "        name = company_name+\"_\"+i\n",
    "        temp = frame.groupby(['Year']).get_group(str(i))\n",
    "        if int(i) != 2019:\n",
    "            dict_to_add[name].append(temp[[\"Date\", \"Tweet\"]])\n",
    "#         else: \n",
    "#             dict_to_add[name] = (temp[[\"Date\", \"Tweet\"]])\n",
    "    return dict_to_add\n",
    "\n",
    "\n",
    "file_dict = {}\n",
    "for file, name in zip(file_names, data_names):\n",
    "    file_dict[name] = pd.read_csv(file, names = [\"Date\", \"Tweet\"])\n",
    "    \n",
    "file_dict = process_pickles(\"MMM\", mmm, file_dict)\n",
    "file_dict = process_pickles(\"HON\", hon, file_dict)\n",
    "file_dict = process_pickles(\"SYF\", syf, file_dict)\n",
    "file_dict = process_pickles(\"BAYRY\", bayry, file_dict)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_proc = {}\n",
    "\n",
    "count = 1;\n",
    "for key, value in file_dict.items():\n",
    "    temp_dict = {}\n",
    "    temp_frame = pd.DataFrame(columns = [\"Tweet\"])\n",
    "    for index, row in value.iterrows():\n",
    "        temp_dict.setdefault(row['Date'], [])\n",
    "        temp_dict[row['Date']].append(row['Tweet'])\n",
    "                              \n",
    "    for keys, tweet in temp_dict.items():\n",
    "        temp_frame.loc[keys] = [tweet]\n",
    "    temp_frame = temp_frame.reset_index()\n",
    "    temp_frame.rename(columns = {\"index\": \"Date\", \"Tweet\":\"Tweet\"}, inplace = True)\n",
    "    temp_frame.sort_values(by=['Date'])\n",
    "    pre_proc[key] = temp_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = np.arange(1,32)\n",
    "feb = np.arange(1, 29)\n",
    "months = {\"Jan\": 31, \"Feb\":28, \"Mar\":31, \"Apr\":30, \"May\":31, \"Jun\":30, \"Jul\":31, \"Aug\":31, \"Sep\":30, \"Oct\": 31, \"Nov\":30, \"Dec\":31}\n",
    "index_dict = {}\n",
    "for k, v in months.items():\n",
    "    for j in range(1,v+1):\n",
    "            key = str(j) + \" \" + k\n",
    "            index_dict[key] = len(index_dict.keys())+1\n",
    "\n",
    "def make_df(stack, key):\n",
    "    temp = {}\n",
    "    year = key.split(\"_\")[-1]\n",
    "    data = [(t+\" \"+str(year)) for t in index_dict.keys()]\n",
    "    for i in data: \n",
    "        temp[i] = \"\"\n",
    "    return temp\n",
    "\n",
    "final_proc = {}\n",
    "for k, v in pre_proc.items():\n",
    "    temp = make_df(v, k)\n",
    "    for indx, val in v.iterrows():\n",
    "        temp[val[\"Date\"]] = val[\"Tweet\"]\n",
    "    out = pd.DataFrame(columns = {\"Date\", \"Tweet\"})\n",
    "    out[\"Date\"] = temp.keys()\n",
    "    out[\"Tweet\"] = list(temp.values())\n",
    "    final_proc[k] = out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dates(stack):\n",
    "    for i, v in stack.items():\n",
    "        v['Date'] = pd.to_datetime(v['Date'], dayfirst = True)\n",
    "    return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<input>:3: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\.\n",
      "\n",
      "<input>:4: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n",
      "<input>:3: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\.\n",
      "\n",
      "<input>:4: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n",
      "<input>:3: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\.\n",
      "\n",
      "<input>:4: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n",
      "<ipython-input-245-c31a6ee32950>:3: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\.\n",
      "\n",
      "<ipython-input-245-c31a6ee32950>:4: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def preprocess_text(text):\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', text)\n",
    "    text = re.sub('@[^\\s]+','USER', text)\n",
    "    text = text.lower().replace(\"ё\", \"е\")\n",
    "    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
    "    text = re.sub(' +',' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "for key, value in final_proc.items():\n",
    "    for idx, txt in value.iterrows():\n",
    "        txt[\"Tweet\"] = [preprocess_text(t) for t in txt[\"Tweet\"]]\n",
    "\n",
    "final_proc = convert_dates(final_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from CSV files\n",
    "n = ['ItemID', 'Sentiment', 'SentimentSource', 'SentimentText']\n",
    "raw_data = pd.read_csv('Sentiment Analysis Dataset 2.csv', names=n, header = 0, usecols=['Sentiment', 'SentimentText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos = raw_data.groupby('Sentiment')\n",
    "neg_c = raw_data.Sentiment.value_counts()\n",
    "sample_size = int(min(neg_c[0], neg_c[1]))\n",
    "raw_data = np.concatenate((neg[1].values[:sample_size], pos[1].values[:sample_size]), axis=0)\n",
    "labels = [1]*sample_size + [0]*sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])\n",
    "tuned_parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': [1, 1e-1, 1e-2]\n",
    "}\n",
    "data = [preprocess_text(t[1]) for t in raw_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520368\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.33, random_state=42)\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.7769849134932376, total=  17.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   27.1s remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.776203402921333, total=  17.2s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   55.1s remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.7765463544284388, total=  17.8s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.4min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.8001868421127356, total= 1.0min\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  2.9min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.799465028054792, total=  58.6s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  4.4min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.8001289161108224, total=  59.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  5.9min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.7845948962004038, total=  47.9s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  7.0min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.7843813180073147, total=  47.3s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  8.2min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.7842302089633612, total=  44.8s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  9.3min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.7805570620923477, total=  16.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  9.7min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.7800226028486404, total=  16.4s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed: 10.1min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.7804138377531115, total=  16.2s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed: 10.6min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.793681442716423, total=  51.4s\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed: 11.8min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.792973807955294, total=  49.6s\n",
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed: 13.0min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.7941118844184719, total=  51.2s\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed: 14.3min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.7800544624881804, total=  38.6s\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed: 15.2min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.7798522296176825, total=  40.1s\n",
      "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed: 16.1min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.7799879034662532, total=  39.6s\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed: 17.1min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.7723167002206327, total=  18.1s\n",
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed: 17.5min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.7716459189932078, total=  15.7s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 18.0min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.7721223169689381, total=  15.4s\n",
      "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed: 18.4min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.8011835226837115, total=  57.4s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed: 19.8min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.7998228118398037, total=  56.7s\n",
      "[Parallel(n_jobs=1)]: Done  23 out of  23 | elapsed: 21.2min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.8007621384172849, total=  57.0s\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed: 22.6min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.7855745394966621, total=  44.5s\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed: 23.7min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.7847561391154222, total=  43.1s\n",
      "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed: 24.8min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.7852496116899085, total=  43.7s\n",
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed: 25.8min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.7813123812714918, total=  14.5s\n",
      "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed: 26.2min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.7803349537720633, total=  14.7s\n",
      "[Parallel(n_jobs=1)]: Done  29 out of  29 | elapsed: 26.6min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.7809136006496917, total=  18.4s\n",
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 27.1min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.7984036073589669, total=  52.4s\n",
      "[Parallel(n_jobs=1)]: Done  31 out of  31 | elapsed: 28.4min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.7976249971604461, total=  51.4s\n",
      "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed: 29.6min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.798311596486894, total=  53.4s\n",
      "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed: 30.9min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.7850946562587848, total=  45.7s\n",
      "[Parallel(n_jobs=1)]: Done  34 out of  34 | elapsed: 32.0min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.784347243361123, total=  40.4s\n",
      "[Parallel(n_jobs=1)]: Done  35 out of  35 | elapsed: 32.9min remaining:    0.0s\n",
      "[CV] clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.7850082489273554, total=  43.4s\n",
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed: 33.9min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.7674639164719211, total=  17.8s\n",
      "[Parallel(n_jobs=1)]: Done  37 out of  37 | elapsed: 34.5min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.7663416324027169, total=  18.5s\n",
      "[Parallel(n_jobs=1)]: Done  38 out of  38 | elapsed: 34.9min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.7667583845164369, total=  16.2s\n",
      "[Parallel(n_jobs=1)]: Done  39 out of  39 | elapsed: 35.4min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.7999738761787665, total=  57.7s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed: 36.8min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.799047613638945, total=  56.7s\n",
      "[Parallel(n_jobs=1)]: Done  41 out of  41 | elapsed: 38.2min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.7995013729281847, total= 1.0min\n",
      "[Parallel(n_jobs=1)]: Done  42 out of  42 | elapsed: 39.8min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.7817752272346515, total=  46.4s\n",
      "[Parallel(n_jobs=1)]: Done  43 out of  43 | elapsed: 40.9min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.781195338588401, total=  44.6s\n",
      "[Parallel(n_jobs=1)]: Done  44 out of  44 | elapsed: 42.0min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.7809391567069033, total=  47.1s\n",
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed: 43.2min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.7807245952937368, total=  15.8s\n",
      "[Parallel(n_jobs=1)]: Done  46 out of  46 | elapsed: 43.6min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.7798777856023261, total=  15.6s\n",
      "[Parallel(n_jobs=1)]: Done  47 out of  47 | elapsed: 44.0min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.7804819872390087, total=  15.7s\n",
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed: 44.4min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.8011835226837115, total=  53.9s\n",
      "[Parallel(n_jobs=1)]: Done  49 out of  49 | elapsed: 45.7min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.8004929465482383, total=  54.2s\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed: 47.1min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.8009154747605539, total=  54.0s\n",
      "[Parallel(n_jobs=1)]: Done  51 out of  51 | elapsed: 48.3min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.7860317063682494, total=  40.6s\n",
      "[Parallel(n_jobs=1)]: Done  52 out of  52 | elapsed: 49.3min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.7856364008087049, total=  40.8s\n",
      "[Parallel(n_jobs=1)]: Done  53 out of  53 | elapsed: 50.3min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.78603617033964, total=  41.4s\n",
      "[Parallel(n_jobs=1)]: Done  54 out of  54 | elapsed: 51.2min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.7618188994488442, total=  17.5s\n",
      "[Parallel(n_jobs=1)]: Done  55 out of  55 | elapsed: 51.7min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.7606965993503101, total=  17.0s\n",
      "[Parallel(n_jobs=1)]: Done  56 out of  56 | elapsed: 52.2min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.7611616079871197, total=  15.2s\n",
      "[Parallel(n_jobs=1)]: Done  57 out of  57 | elapsed: 52.6min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.788235193898384, total= 1.0min\n",
      "[Parallel(n_jobs=1)]: Done  58 out of  58 | elapsed: 54.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.7875019876876945, total= 1.1min\n",
      "[Parallel(n_jobs=1)]: Done  59 out of  59 | elapsed: 55.8min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.7878676877731303, total= 1.0min\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed: 57.4min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.7688581334529728, total=  45.6s\n",
      "[Parallel(n_jobs=1)]: Done  61 out of  61 | elapsed: 58.4min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.7689398241748256, total=  43.8s\n",
      "[Parallel(n_jobs=1)]: Done  62 out of  62 | elapsed: 59.5min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.7688994141983775, total=  43.6s\n",
      "[Parallel(n_jobs=1)]: Done  63 out of  63 | elapsed: 60.6min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.777655046298794, total=  14.2s\n",
      "[Parallel(n_jobs=1)]: Done  64 out of  64 | elapsed: 61.0min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.7770353921991776, total=  14.0s\n",
      "[Parallel(n_jobs=1)]: Done  65 out of  65 | elapsed: 61.4min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.7775146450405631, total=  16.5s\n",
      "[Parallel(n_jobs=1)]: Done  66 out of  66 | elapsed: 61.8min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.7986620060255162, total=  50.1s\n",
      "[Parallel(n_jobs=1)]: Done  67 out of  67 | elapsed: 63.1min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.7976363553758433, total=  51.1s\n",
      "[Parallel(n_jobs=1)]: Done  68 out of  68 | elapsed: 64.3min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.7983854250966161, total=  54.2s\n",
      "[Parallel(n_jobs=1)]: Done  69 out of  69 | elapsed: 65.6min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.7799408806567302, total=  43.5s\n",
      "[Parallel(n_jobs=1)]: Done  70 out of  70 | elapsed: 66.6min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.7791622180323028, total=  41.4s\n",
      "[Parallel(n_jobs=1)]: Done  71 out of  71 | elapsed: 67.5min remaining:    0.0s\n",
      "[CV] clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.1, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.7797437011417878, total=  38.7s\n",
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed: 68.4min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.7590758982193209, total=  15.0s\n",
      "[Parallel(n_jobs=1)]: Done  73 out of  73 | elapsed: 68.9min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.7580359373935167, total=  15.0s\n",
      "[Parallel(n_jobs=1)]: Done  74 out of  74 | elapsed: 69.3min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.7581942657886741, total=  17.9s\n",
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed: 69.8min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.7823970877618416, total= 1.0min\n",
      "[Parallel(n_jobs=1)]: Done  76 out of  76 | elapsed: 71.3min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.782058562958588, total= 1.0min\n",
      "[Parallel(n_jobs=1)]: Done  77 out of  77 | elapsed: 72.8min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.7823504189773601, total= 1.0min\n",
      "[Parallel(n_jobs=1)]: Done  78 out of  78 | elapsed: 74.3min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.7600299856035029, total=  48.1s\n",
      "[Parallel(n_jobs=1)]: Done  79 out of  79 | elapsed: 75.5min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.760230912519025, total=  46.7s\n",
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed: 76.6min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.7599008424980194, total=  48.5s\n",
      "[Parallel(n_jobs=1)]: Done  81 out of  81 | elapsed: 77.8min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.7754742751349494, total=  15.7s\n",
      "[Parallel(n_jobs=1)]: Done  82 out of  82 | elapsed: 78.2min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.7747353535812453, total=  15.6s\n",
      "[Parallel(n_jobs=1)]: Done  83 out of  83 | elapsed: 78.6min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.7750016327480996, total=  15.5s\n",
      "[Parallel(n_jobs=1)]: Done  84 out of  84 | elapsed: 79.1min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.796597656238908, total=  52.0s\n",
      "[Parallel(n_jobs=1)]: Done  85 out of  85 | elapsed: 80.3min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.7957196565275664, total=  52.0s\n",
      "[Parallel(n_jobs=1)]: Done  86 out of  86 | elapsed: 81.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.7966050197775487, total=  52.0s\n",
      "[Parallel(n_jobs=1)]: Done  87 out of  87 | elapsed: 82.8min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.7734809139929977, total=  38.8s\n",
      "[Parallel(n_jobs=1)]: Done  88 out of  88 | elapsed: 83.7min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.7732701437950069, total=  38.6s\n",
      "[Parallel(n_jobs=1)]: Done  89 out of  89 | elapsed: 84.6min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l1, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.7729798646664793, total=  38.5s\n",
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed: 85.5min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.7572358725498269, total=  15.0s\n",
      "[Parallel(n_jobs=1)]: Done  91 out of  91 | elapsed: 85.9min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.756122078099089, total=  15.1s\n",
      "[Parallel(n_jobs=1)]: Done  92 out of  92 | elapsed: 86.4min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.7565331220699271, total=  15.0s\n",
      "[Parallel(n_jobs=1)]: Done  93 out of  93 | elapsed: 86.8min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.7643176997407495, total= 4.1min\n",
      "[Parallel(n_jobs=1)]: Done  94 out of  94 | elapsed: 91.3min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.764197769246496, total= 1.1min\n",
      "[Parallel(n_jobs=1)]: Done  95 out of  95 | elapsed: 93.0min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.7641204314998282, total= 1.2min\n",
      "[Parallel(n_jobs=1)]: Done  96 out of  96 | elapsed: 94.7min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.7459997898736118, total=  52.4s\n",
      "[Parallel(n_jobs=1)]: Done  97 out of  97 | elapsed: 95.9min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.7465499420731014, total=  47.4s\n",
      "[Parallel(n_jobs=1)]: Done  98 out of  98 | elapsed: 97.1min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=True, vect__ngram_range=(2, 2), score=0.7456945142503415, total=  47.9s\n",
      "[Parallel(n_jobs=1)]: Done  99 out of  99 | elapsed: 98.2min remaining:    0.0s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.771055941891535, total=  16.8s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.770328366007133, total=  16.7s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.770929700965735, total=  16.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.7845523030136099, total=  56.6s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.7841427954839736, total=  52.7s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.7845255234022495, total=  56.4s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.7599561574130602, total=  39.5s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.7603132595806547, total=  42.2s\n",
      "[CV] clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2) \n",
      "[CV]  clf__alpha=0.01, tfidf__norm=l2, tfidf__use_idf=False, vect__ngram_range=(2, 2), score=0.7599888689173034, total=  44.8s\n",
      "[Parallel(n_jobs=1)]: Done 108 out of 108 | elapsed: 106.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'vect__ngram_range': [(1, 1), (1, 2), (2, 2)], 'tfidf__use_idf': (True, False), 'tfidf__norm': ('l1', 'l2'), 'clf__alpha': [1, 0.1, 0.01]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "clf = GridSearchCV(text_clf, tuned_parameters, cv=3, verbose = 100)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.8209    0.7777    0.7987    260642\n",
      "          1     0.7881    0.8297    0.8084    259726\n",
      "\n",
      "avg / total     0.8045    0.8036    0.8035    520368\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clf.predict(x_test), digits=4))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model = \"grid.sav\"\n",
    "pickle.dump(clf, open(model, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, v in final_proc.items():\n",
    "    emot = []\n",
    "    for index, val in v.iterrows():\n",
    "        if len(val[\"Tweet\"]) != 0:\n",
    "            emot.append([clf.predict(t) for t in [val[\"Tweet\"]]])\n",
    "        else:\n",
    "            emot.append(np.NaN)\n",
    "    v[\"emot\"] = emot\n",
    "    v[\"mean\"] = [np.mean(t) for t in emot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MMM_2009':                                                  Tweet       Date  \\\n",
       " 0                                                   [] 2009-01-01   \n",
       " 1                                                   [] 2009-01-02   \n",
       " 2                                                   [] 2009-01-03   \n",
       " 3                                                   [] 2009-01-04   \n",
       " 4                                                   [] 2009-01-05   \n",
       " 5                                                   [] 2009-01-06   \n",
       " 6                                                   [] 2009-01-07   \n",
       " 7                                                   [] 2009-01-08   \n",
       " 8                                                   [] 2009-01-09   \n",
       " 9                                                   [] 2009-01-10   \n",
       " 10                                                  [] 2009-01-11   \n",
       " 11                                                  [] 2009-01-12   \n",
       " 12                                                  [] 2009-01-13   \n",
       " 13                                                  [] 2009-01-14   \n",
       " 14                                                  [] 2009-01-15   \n",
       " 15                                                  [] 2009-01-16   \n",
       " 16                                                  [] 2009-01-17   \n",
       " 17                                                  [] 2009-01-18   \n",
       " 18                                                  [] 2009-01-19   \n",
       " 19                                                  [] 2009-01-20   \n",
       " 20                                                  [] 2009-01-21   \n",
       " 21                                                  [] 2009-01-22   \n",
       " 22   [blue chip safe haven dia stocks getting hit t... 2009-01-23   \n",
       " 23                                                  [] 2009-01-24   \n",
       " 24                                                  [] 2009-01-25   \n",
       " 25                                                  [] 2009-01-26   \n",
       " 26                                                  [] 2009-01-27   \n",
       " 27                                                  [] 2009-01-28   \n",
       " 28   [most sellside movement on updown com by top t... 2009-01-29   \n",
       " 29                                                  [] 2009-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 335                                                 [] 2009-12-02   \n",
       " 336                                                 [] 2009-12-03   \n",
       " 337                                                 [] 2009-12-04   \n",
       " 338                                                 [] 2009-12-05   \n",
       " 339                                                 [] 2009-12-06   \n",
       " 340                                                 [] 2009-12-07   \n",
       " 341  [mcdonald s 3m and kroger plunge after warning... 2009-12-08   \n",
       " 342  [good morning todays s 1 am est wholesale inve... 2009-12-09   \n",
       " 343  [url mmm nice break on volume could see higher... 2009-12-10   \n",
       " 344                                                 [] 2009-12-11   \n",
       " 345                                                 [] 2009-12-12   \n",
       " 346                                                 [] 2009-12-13   \n",
       " 347  [3m medline settle lawsuits over hand cleaners... 2009-12-14   \n",
       " 348  [mmm last but not least some upgrades and the ... 2009-12-15   \n",
       " 349                                                 [] 2009-12-16   \n",
       " 350                                                 [] 2009-12-17   \n",
       " 351                                                 [] 2009-12-18   \n",
       " 352                                                 [] 2009-12-19   \n",
       " 353                                                 [] 2009-12-20   \n",
       " 354                                                 [] 2009-12-21   \n",
       " 355  [cramer 3 new dividend plays url gis kft mmm m... 2009-12-22   \n",
       " 356                                                 [] 2009-12-23   \n",
       " 357                                                 [] 2009-12-24   \n",
       " 358                                                 [] 2009-12-25   \n",
       " 359                                                 [] 2009-12-26   \n",
       " 360                                                 [] 2009-12-27   \n",
       " 361                                                 [] 2009-12-28   \n",
       " 362  [mmm dow movers included disney strength there... 2009-12-29   \n",
       " 363  [mmm 3m from new york community bancorp nyb ba... 2009-12-30   \n",
       " 364                                                 [] 2009-12-31   \n",
       " \n",
       "                            emot   mean  \n",
       " 0                           NaN    NaN  \n",
       " 1                           NaN    NaN  \n",
       " 2                           NaN    NaN  \n",
       " 3                           NaN    NaN  \n",
       " 4                           NaN    NaN  \n",
       " 5                           NaN    NaN  \n",
       " 6                           NaN    NaN  \n",
       " 7                           NaN    NaN  \n",
       " 8                           NaN    NaN  \n",
       " 9                           NaN    NaN  \n",
       " 10                          NaN    NaN  \n",
       " 11                          NaN    NaN  \n",
       " 12                          NaN    NaN  \n",
       " 13                          NaN    NaN  \n",
       " 14                          NaN    NaN  \n",
       " 15                          NaN    NaN  \n",
       " 16                          NaN    NaN  \n",
       " 17                          NaN    NaN  \n",
       " 18                          NaN    NaN  \n",
       " 19                          NaN    NaN  \n",
       " 20                          NaN    NaN  \n",
       " 21                          NaN    NaN  \n",
       " 22                        [[1]]  1.000  \n",
       " 23                          NaN    NaN  \n",
       " 24                          NaN    NaN  \n",
       " 25                          NaN    NaN  \n",
       " 26                          NaN    NaN  \n",
       " 27                          NaN    NaN  \n",
       " 28                     [[0, 0]]  0.000  \n",
       " 29                          NaN    NaN  \n",
       " ..                          ...    ...  \n",
       " 335                         NaN    NaN  \n",
       " 336                         NaN    NaN  \n",
       " 337                         NaN    NaN  \n",
       " 338                         NaN    NaN  \n",
       " 339                         NaN    NaN  \n",
       " 340                         NaN    NaN  \n",
       " 341  [[0, 0, 1, 0, 0, 1, 1, 0]]  0.375  \n",
       " 342                       [[0]]  0.000  \n",
       " 343                       [[0]]  0.000  \n",
       " 344                         NaN    NaN  \n",
       " 345                         NaN    NaN  \n",
       " 346                         NaN    NaN  \n",
       " 347                       [[0]]  0.000  \n",
       " 348                    [[0, 0]]  0.000  \n",
       " 349                         NaN    NaN  \n",
       " 350                         NaN    NaN  \n",
       " 351                         NaN    NaN  \n",
       " 352                         NaN    NaN  \n",
       " 353                         NaN    NaN  \n",
       " 354                         NaN    NaN  \n",
       " 355                       [[0]]  0.000  \n",
       " 356                         NaN    NaN  \n",
       " 357                         NaN    NaN  \n",
       " 358                         NaN    NaN  \n",
       " 359                         NaN    NaN  \n",
       " 360                         NaN    NaN  \n",
       " 361                         NaN    NaN  \n",
       " 362                       [[0]]  0.000  \n",
       " 363                       [[0]]  0.000  \n",
       " 364                         NaN    NaN  \n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'MMM_2010':                                                  Tweet       Date  \\\n",
       " 0                                                   [] 2010-01-01   \n",
       " 1                                                   [] 2010-01-02   \n",
       " 2    [vencidos adem s hay mmm en bonos a futuro rt ... 2010-01-03   \n",
       " 3    [which s p stocks hit 52 week highs url spx gi... 2010-01-04   \n",
       " 4    [mmm 3m co 82 14 crossed its 2nd pivot point s... 2010-01-05   \n",
       " 5    [dow 3 to this moment two days high 11 dia axp... 2010-01-06   \n",
       " 6    [the street s love hate relationship with 3m u... 2010-01-07   \n",
       " 7                                                   [] 2010-01-08   \n",
       " 8                                                   [] 2010-01-09   \n",
       " 9                                                   [] 2010-01-10   \n",
       " 10                                                  [] 2010-01-11   \n",
       " 11                                                  [] 2010-01-12   \n",
       " 12                                                  [] 2010-01-13   \n",
       " 13                                                  [] 2010-01-14   \n",
       " 14                                                  [] 2010-01-15   \n",
       " 15                                                  [] 2010-01-16   \n",
       " 16                                                  [] 2010-01-17   \n",
       " 17                                                  [] 2010-01-18   \n",
       " 18                                                  [] 2010-01-19   \n",
       " 19                                                  [] 2010-01-20   \n",
       " 20                                                  [] 2010-01-21   \n",
       " 21                                                  [] 2010-01-22   \n",
       " 22                                                  [] 2010-01-23   \n",
       " 23                                                  [] 2010-01-24   \n",
       " 24                                                  [] 2010-01-25   \n",
       " 25                                                  [] 2010-01-26   \n",
       " 26   [stockmarket if stocks move sideways ahead not... 2010-01-27   \n",
       " 27   [a greek tragedy url aapl abt ba bmy cl mmm pg... 2010-01-28   \n",
       " 28   [mmm 3m company nyse just got published url by... 2010-01-29   \n",
       " 29                                                  [] 2010-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 335                                                 [] 2010-12-02   \n",
       " 336                                                 [] 2010-12-03   \n",
       " 337                                                 [] 2010-12-04   \n",
       " 338                                                 [] 2010-12-05   \n",
       " 339  [1 things you need to know before the opening ... 2010-12-06   \n",
       " 340  [1 unusual stocks attracting huge interest thi... 2010-12-07   \n",
       " 341                                                 [] 2010-12-08   \n",
       " 342  [energy savings another boost for cloud comput... 2010-12-09   \n",
       " 343                                                 [] 2010-12-10   \n",
       " 344                                                 [] 2010-12-11   \n",
       " 345                                                 [] 2010-12-12   \n",
       " 346                                                 [] 2010-12-13   \n",
       " 347                                                 [] 2010-12-14   \n",
       " 348                                                 [] 2010-12-15   \n",
       " 349  [mmm url not1 i ve traded before nice possible... 2010-12-16   \n",
       " 350  [music charts recap for 12 17 ma oih fslr mmm ... 2010-12-17   \n",
       " 351                                                 [] 2010-12-18   \n",
       " 352                                                 [] 2010-12-19   \n",
       " 353  [3m is said to prepare for early exit of ceo b... 2010-12-20   \n",
       " 354  [mmm blast applications pinksheets blap is rap... 2010-12-21   \n",
       " 355                                                 [] 2010-12-22   \n",
       " 356                                                 [] 2010-12-23   \n",
       " 357                                                 [] 2010-12-24   \n",
       " 358                                                 [] 2010-12-25   \n",
       " 359                                                 [] 2010-12-26   \n",
       " 360  [jim cramer picks his top dow stocks for 2 11 ... 2010-12-27   \n",
       " 361                                                 [] 2010-12-28   \n",
       " 362                                                 [] 2010-12-29   \n",
       " 363  [always told 2 never buy a home in oakdale 4 t... 2010-12-30   \n",
       " 364                                                 [] 2010-12-31   \n",
       " \n",
       "                                        emot      mean  \n",
       " 0                                       NaN       NaN  \n",
       " 1                                       NaN       NaN  \n",
       " 2                                     [[0]]  0.000000  \n",
       " 3                                  [[0, 0]]  0.000000  \n",
       " 4                               [[0, 0, 0]]  0.000000  \n",
       " 5                                  [[0, 0]]  0.000000  \n",
       " 6                                     [[0]]  0.000000  \n",
       " 7                                       NaN       NaN  \n",
       " 8                                       NaN       NaN  \n",
       " 9                                       NaN       NaN  \n",
       " 10                                      NaN       NaN  \n",
       " 11                                      NaN       NaN  \n",
       " 12                                      NaN       NaN  \n",
       " 13                                      NaN       NaN  \n",
       " 14                                      NaN       NaN  \n",
       " 15                                      NaN       NaN  \n",
       " 16                                      NaN       NaN  \n",
       " 17                                      NaN       NaN  \n",
       " 18                                      NaN       NaN  \n",
       " 19                                      NaN       NaN  \n",
       " 20                                      NaN       NaN  \n",
       " 21                                      NaN       NaN  \n",
       " 22                                      NaN       NaN  \n",
       " 23                                      NaN       NaN  \n",
       " 24                                      NaN       NaN  \n",
       " 25                                      NaN       NaN  \n",
       " 26                                    [[1]]  1.000000  \n",
       " 27                                 [[1, 0]]  0.500000  \n",
       " 28                                    [[0]]  0.000000  \n",
       " 29                                      NaN       NaN  \n",
       " ..                                      ...       ...  \n",
       " 335                                     NaN       NaN  \n",
       " 336                                     NaN       NaN  \n",
       " 337                                     NaN       NaN  \n",
       " 338                                     NaN       NaN  \n",
       " 339                             [[0, 0, 0]]  0.000000  \n",
       " 340  [[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]  0.083333  \n",
       " 341                                     NaN       NaN  \n",
       " 342                                   [[0]]  0.000000  \n",
       " 343                                     NaN       NaN  \n",
       " 344                                     NaN       NaN  \n",
       " 345                                     NaN       NaN  \n",
       " 346                                     NaN       NaN  \n",
       " 347                                     NaN       NaN  \n",
       " 348                                     NaN       NaN  \n",
       " 349                                   [[0]]  0.000000  \n",
       " 350                                   [[0]]  0.000000  \n",
       " 351                                     NaN       NaN  \n",
       " 352                                     NaN       NaN  \n",
       " 353                             [[0, 0, 0]]  0.000000  \n",
       " 354                                [[1, 0]]  0.500000  \n",
       " 355                                     NaN       NaN  \n",
       " 356                                     NaN       NaN  \n",
       " 357                                     NaN       NaN  \n",
       " 358                                     NaN       NaN  \n",
       " 359                                     NaN       NaN  \n",
       " 360                          [[0, 0, 0, 0]]  0.000000  \n",
       " 361                                     NaN       NaN  \n",
       " 362                                     NaN       NaN  \n",
       " 363                                   [[0]]  0.000000  \n",
       " 364                                     NaN       NaN  \n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'MMM_2011':                                                  Tweet       Date  \\\n",
       " 0    [2h a 3m co sales 23 bil usd net income 3 193 ... 2011-01-01   \n",
       " 1    [mmm, slowing economic growth means greater fo... 2011-01-02   \n",
       " 2    [3m acquires privately held nida core terms un... 2011-01-03   \n",
       " 3                                                   [] 2011-01-04   \n",
       " 4                                                   [] 2011-01-05   \n",
       " 5                                                   [] 2011-01-06   \n",
       " 6    [rt user jeff matthews is not making this up s... 2011-01-07   \n",
       " 7                                                   [] 2011-01-08   \n",
       " 8                                                   [] 2011-01-09   \n",
       " 9    [state and 3m agree on cottage grove discharge... 2011-01-10   \n",
       " 10   [breakout plays to be watching morn cpwr penn ... 2011-01-11   \n",
       " 11   [2 11 manufacturing outlook slow steady growth... 2011-01-12   \n",
       " 12                                                  [] 2011-01-13   \n",
       " 13                                                  [] 2011-01-14   \n",
       " 14                                                  [] 2011-01-15   \n",
       " 15                                                  [] 2011-01-16   \n",
       " 16                                                  [] 2011-01-17   \n",
       " 17                                                  [] 2011-01-18   \n",
       " 18   [amphenol slips despite q4 ticonderoga says bu... 2011-01-19   \n",
       " 19   [pricing the dow jones industrials part 2 url ... 2011-01-20   \n",
       " 20   [global inflation nips at url aapl amgn amzn a... 2011-01-21   \n",
       " 21                                                  [] 2011-01-22   \n",
       " 22   [mmm url i like an earnings gamble here using ... 2011-01-23   \n",
       " 23   [a few pre market earnings i ll be watching mm... 2011-01-24   \n",
       " 24   [mmm anaysts games continue 9 days ago earn es... 2011-01-25   \n",
       " 25   [did you know that only 4 stocks account for o... 2011-01-26   \n",
       " 26                                                  [] 2011-01-27   \n",
       " 27                                                  [] 2011-01-28   \n",
       " 28                                                  [] 2011-01-29   \n",
       " 29                                                  [] 2011-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 335  [some unusual movement in mmm this afternoon t... 2011-12-02   \n",
       " 336                                                 [] 2011-12-03   \n",
       " 337  [watchlist mon big wdc dish jpm gs med supply ... 2011-12-04   \n",
       " 338  [semi s and industrial smh xli will be providi... 2011-12-05   \n",
       " 339  [rally fizzles out after hot rumor from europe... 2011-12-06   \n",
       " 340                                                 [] 2011-12-07   \n",
       " 341                                                 [] 2011-12-08   \n",
       " 342                                                 [] 2011-12-09   \n",
       " 343                                                 [] 2011-12-10   \n",
       " 344                                                 [] 2011-12-11   \n",
       " 345                                                 [] 2011-12-12   \n",
       " 346  [flex flexes its muscles mmm hits big resistan... 2011-12-13   \n",
       " 347  [smartmoney s top 1 stocks for 2 12 url pot ce... 2011-12-14   \n",
       " 348  [from user dividend stock guide for boomers ur... 2011-12-15   \n",
       " 349  [imation corp following in the path of kodak i... 2011-12-16   \n",
       " 350                                                 [] 2011-12-17   \n",
       " 351                                                 [] 2011-12-18   \n",
       " 352                                                 [] 2011-12-19   \n",
       " 353                                                 [] 2011-12-20   \n",
       " 354  [joel greenblatt value investing lecture 8 on ... 2011-12-21   \n",
       " 355  [the five year plans to trust url crm dd deck ... 2011-12-22   \n",
       " 356                                                 [] 2011-12-23   \n",
       " 357                                                 [] 2011-12-24   \n",
       " 358                                                 [] 2011-12-25   \n",
       " 359                                                 [] 2011-12-26   \n",
       " 360                                                 [] 2011-12-27   \n",
       " 361                                                 [] 2011-12-28   \n",
       " 362                                                 [] 2011-12-29   \n",
       " 363                                                 [] 2011-12-30   \n",
       " 364                                                 [] 2011-12-31   \n",
       " \n",
       "                emot  mean  \n",
       " 0             [[0]]  0.00  \n",
       " 1          [[0, 0]]  0.00  \n",
       " 2             [[0]]  0.00  \n",
       " 3               NaN   NaN  \n",
       " 4               NaN   NaN  \n",
       " 5               NaN   NaN  \n",
       " 6          [[0, 0]]  0.00  \n",
       " 7               NaN   NaN  \n",
       " 8               NaN   NaN  \n",
       " 9             [[0]]  0.00  \n",
       " 10         [[0, 0]]  0.00  \n",
       " 11            [[0]]  0.00  \n",
       " 12              NaN   NaN  \n",
       " 13              NaN   NaN  \n",
       " 14              NaN   NaN  \n",
       " 15              NaN   NaN  \n",
       " 16              NaN   NaN  \n",
       " 17              NaN   NaN  \n",
       " 18            [[0]]  0.00  \n",
       " 19            [[0]]  0.00  \n",
       " 20   [[0, 0, 0, 0]]  0.00  \n",
       " 21              NaN   NaN  \n",
       " 22         [[0, 0]]  0.00  \n",
       " 23   [[0, 1, 0, 0]]  0.25  \n",
       " 24   [[0, 0, 0, 0]]  0.00  \n",
       " 25            [[0]]  0.00  \n",
       " 26              NaN   NaN  \n",
       " 27              NaN   NaN  \n",
       " 28              NaN   NaN  \n",
       " 29              NaN   NaN  \n",
       " ..              ...   ...  \n",
       " 335  [[0, 0, 1, 1]]  0.50  \n",
       " 336             NaN   NaN  \n",
       " 337           [[0]]  0.00  \n",
       " 338           [[0]]  0.00  \n",
       " 339  [[0, 0, 0, 0]]  0.00  \n",
       " 340             NaN   NaN  \n",
       " 341             NaN   NaN  \n",
       " 342             NaN   NaN  \n",
       " 343             NaN   NaN  \n",
       " 344             NaN   NaN  \n",
       " 345             NaN   NaN  \n",
       " 346           [[0]]  0.00  \n",
       " 347        [[0, 0]]  0.00  \n",
       " 348           [[0]]  0.00  \n",
       " 349           [[0]]  0.00  \n",
       " 350             NaN   NaN  \n",
       " 351             NaN   NaN  \n",
       " 352             NaN   NaN  \n",
       " 353             NaN   NaN  \n",
       " 354           [[0]]  0.00  \n",
       " 355           [[0]]  0.00  \n",
       " 356             NaN   NaN  \n",
       " 357             NaN   NaN  \n",
       " 358             NaN   NaN  \n",
       " 359             NaN   NaN  \n",
       " 360             NaN   NaN  \n",
       " 361             NaN   NaN  \n",
       " 362             NaN   NaN  \n",
       " 363             NaN   NaN  \n",
       " 364             NaN   NaN  \n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'MMM_2012':                                                  Tweet       Date  \\\n",
       " 0                                                   [] 2012-01-01   \n",
       " 1                                                   [] 2012-01-02   \n",
       " 2                                                   [] 2012-01-03   \n",
       " 3                                                   [] 2012-01-04   \n",
       " 4                                                   [] 2012-01-05   \n",
       " 5                                                   [] 2012-01-06   \n",
       " 6                                                   [] 2012-01-07   \n",
       " 7                                                   [] 2012-01-08   \n",
       " 8                                                   [] 2012-01-09   \n",
       " 9                                                   [] 2012-01-10   \n",
       " 10                                                  [] 2012-01-11   \n",
       " 11                                                  [] 2012-01-12   \n",
       " 12                                                  [] 2012-01-13   \n",
       " 13                                                  [] 2012-01-14   \n",
       " 14                                                  [] 2012-01-15   \n",
       " 15                                                  [] 2012-01-16   \n",
       " 16                                                  [] 2012-01-17   \n",
       " 17                                                  [] 2012-01-18   \n",
       " 18                                                  [] 2012-01-19   \n",
       " 19                                                  [] 2012-01-20   \n",
       " 20                                                  [] 2012-01-21   \n",
       " 21                                                  [] 2012-01-22   \n",
       " 22                                                  [] 2012-01-23   \n",
       " 23                                                  [] 2012-01-24   \n",
       " 24                                                  [] 2012-01-25   \n",
       " 25   [finally stocks fall here s what you need to k... 2012-01-26   \n",
       " 26                                                  [] 2012-01-27   \n",
       " 27                                                  [] 2012-01-28   \n",
       " 28                                                  [] 2012-01-29   \n",
       " 29                                                  [] 2012-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 335                                                 [] 2012-12-02   \n",
       " 336                                                 [] 2012-12-03   \n",
       " 337  [the hidden impact of solar why you should car... 2012-12-04   \n",
       " 338  [user mmm goog amzn executive pay scorecards c... 2012-12-05   \n",
       " 339                                                 [] 2012-12-06   \n",
       " 340  [at the top its just us nigga mmm i am my brot... 2012-12-07   \n",
       " 341  [all i ever wanted was that cash money countup... 2012-12-08   \n",
       " 342                                                 [] 2012-12-09   \n",
       " 343  [mmm 3m to give 2 13 outlook on 12 12 was not ... 2012-12-10   \n",
       " 344  [rs gainers ddd auq dal cmi dow mmm sndk stx url] 2012-12-11   \n",
       " 345  [what stocks are you watching here are btv s t... 2012-12-12   \n",
       " 346  [for the first time in a long time mmm guide o... 2012-12-13   \n",
       " 347  [where did all of 3m s big innovations go why ... 2012-12-14   \n",
       " 348                                                 [] 2012-12-15   \n",
       " 349                                                 [] 2012-12-16   \n",
       " 350                                                 [] 2012-12-17   \n",
       " 351  [industrials are breaking out to the upside ur... 2012-12-18   \n",
       " 352                                                 [] 2012-12-19   \n",
       " 353                                                 [] 2012-12-20   \n",
       " 354                                                 [] 2012-12-21   \n",
       " 355  [3 dividend growth stocks for long term gains ... 2012-12-22   \n",
       " 356    [3m be patient with this best of breed url mmm] 2012-12-23   \n",
       " 357                                                 [] 2012-12-24   \n",
       " 358                                                 [] 2012-12-25   \n",
       " 359                                                 [] 2012-12-26   \n",
       " 360                                                 [] 2012-12-27   \n",
       " 361                                                 [] 2012-12-28   \n",
       " 362                                                 [] 2012-12-29   \n",
       " 363                                                 [] 2012-12-30   \n",
       " 364                                                 [] 2012-12-31   \n",
       " \n",
       "             emot      mean  \n",
       " 0            NaN       NaN  \n",
       " 1            NaN       NaN  \n",
       " 2            NaN       NaN  \n",
       " 3            NaN       NaN  \n",
       " 4            NaN       NaN  \n",
       " 5            NaN       NaN  \n",
       " 6            NaN       NaN  \n",
       " 7            NaN       NaN  \n",
       " 8            NaN       NaN  \n",
       " 9            NaN       NaN  \n",
       " 10           NaN       NaN  \n",
       " 11           NaN       NaN  \n",
       " 12           NaN       NaN  \n",
       " 13           NaN       NaN  \n",
       " 14           NaN       NaN  \n",
       " 15           NaN       NaN  \n",
       " 16           NaN       NaN  \n",
       " 17           NaN       NaN  \n",
       " 18           NaN       NaN  \n",
       " 19           NaN       NaN  \n",
       " 20           NaN       NaN  \n",
       " 21           NaN       NaN  \n",
       " 22           NaN       NaN  \n",
       " 23           NaN       NaN  \n",
       " 24           NaN       NaN  \n",
       " 25         [[0]]  0.000000  \n",
       " 26           NaN       NaN  \n",
       " 27           NaN       NaN  \n",
       " 28           NaN       NaN  \n",
       " 29           NaN       NaN  \n",
       " ..           ...       ...  \n",
       " 335          NaN       NaN  \n",
       " 336          NaN       NaN  \n",
       " 337     [[0, 0]]  0.000000  \n",
       " 338        [[0]]  0.000000  \n",
       " 339          NaN       NaN  \n",
       " 340        [[0]]  0.000000  \n",
       " 341        [[1]]  1.000000  \n",
       " 342          NaN       NaN  \n",
       " 343        [[1]]  1.000000  \n",
       " 344        [[0]]  0.000000  \n",
       " 345     [[0, 0]]  0.000000  \n",
       " 346     [[0, 1]]  0.500000  \n",
       " 347     [[1, 0]]  0.500000  \n",
       " 348          NaN       NaN  \n",
       " 349          NaN       NaN  \n",
       " 350          NaN       NaN  \n",
       " 351  [[1, 0, 0]]  0.333333  \n",
       " 352          NaN       NaN  \n",
       " 353          NaN       NaN  \n",
       " 354          NaN       NaN  \n",
       " 355        [[0]]  0.000000  \n",
       " 356        [[0]]  0.000000  \n",
       " 357          NaN       NaN  \n",
       " 358          NaN       NaN  \n",
       " 359          NaN       NaN  \n",
       " 360          NaN       NaN  \n",
       " 361          NaN       NaN  \n",
       " 362          NaN       NaN  \n",
       " 363          NaN       NaN  \n",
       " 364          NaN       NaN  \n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'MMM_2013':                                                  Tweet       Date  \\\n",
       " 0                                                   [] 2013-01-01   \n",
       " 1                                                   [] 2013-01-02   \n",
       " 2                                                   [] 2013-01-03   \n",
       " 3                                                   [] 2013-01-04   \n",
       " 4                                                   [] 2013-01-05   \n",
       " 5                                                   [] 2013-01-06   \n",
       " 6                                                   [] 2013-01-07   \n",
       " 7                                                   [] 2013-01-08   \n",
       " 8                                                   [] 2013-01-09   \n",
       " 9                                                   [] 2013-01-10   \n",
       " 10                                                  [] 2013-01-11   \n",
       " 11                                                  [] 2013-01-12   \n",
       " 12                                                  [] 2013-01-13   \n",
       " 13                                                  [] 2013-01-14   \n",
       " 14                                                  [] 2013-01-15   \n",
       " 15                                                  [] 2013-01-16   \n",
       " 16                                                  [] 2013-01-17   \n",
       " 17                                                  [] 2013-01-18   \n",
       " 18                                                  [] 2013-01-19   \n",
       " 19                                                  [] 2013-01-20   \n",
       " 20                                                  [] 2013-01-21   \n",
       " 21                                                  [] 2013-01-22   \n",
       " 22                                                  [] 2013-01-23   \n",
       " 23                                                  [] 2013-01-24   \n",
       " 24                                                  [] 2013-01-25   \n",
       " 25                                                  [] 2013-01-26   \n",
       " 26                                                  [] 2013-01-27   \n",
       " 27                                                  [] 2013-01-28   \n",
       " 28                                                  [] 2013-01-29   \n",
       " 29                                                  [] 2013-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 336  [djia down 1 pts on the day the stocks with th... 2013-12-03   \n",
       " 337                                                 [] 2013-12-04   \n",
       " 338                                                 [] 2013-12-05   \n",
       " 339                                                 [] 2013-12-06   \n",
       " 340                                                 [] 2013-12-07   \n",
       " 341  [ceo jeff immelt has delivered ge shareholders... 2013-12-08   \n",
       " 342                                                 [] 2013-12-09   \n",
       " 343                                                 [] 2013-12-10   \n",
       " 344                                                 [] 2013-12-11   \n",
       " 345  [3m s strategy to achieve growth targets url b... 2013-12-12   \n",
       " 346  [is it wrong to listen to user while pumping o... 2013-12-13   \n",
       " 347                                                 [] 2013-12-14   \n",
       " 348                                                 [] 2013-12-15   \n",
       " 349  [jt s optimum stock investing daily blog updat... 2013-12-16   \n",
       " 350  [tuesday s stocks to watch 3m boeing athersys ... 2013-12-17   \n",
       " 351  [mmm spikes on taper powering dow higher, user... 2013-12-18   \n",
       " 352  [active calls cboe aapl dec 54 55 555 mmm dec ... 2013-12-19   \n",
       " 353  [dow and s p 5 at all time highs so are these ... 2013-12-20   \n",
       " 354                                                 [] 2013-12-21   \n",
       " 355                                                 [] 2013-12-22   \n",
       " 356  [income investor 3m boosts dividend payout sto... 2013-12-23   \n",
       " 357  [what a strange trade today with mmm maybe xma... 2013-12-24   \n",
       " 358                                                 [] 2013-12-25   \n",
       " 359  [strong moves up today gogo jks twtr tsla bby ... 2013-12-26   \n",
       " 360  [csco and mmm are two dow stocks that head int... 2013-12-27   \n",
       " 361                                                 [] 2013-12-28   \n",
       " 362  [tsla twtr lnkd pxd ibm mmm all on watch this ... 2013-12-29   \n",
       " 363                                                 [] 2013-12-30   \n",
       " 364                                                 [] 2013-12-31   \n",
       " 365                                             [user] 2014-01-05   \n",
       " \n",
       "                                        emot  mean  \n",
       " 0                                       NaN   NaN  \n",
       " 1                                       NaN   NaN  \n",
       " 2                                       NaN   NaN  \n",
       " 3                                       NaN   NaN  \n",
       " 4                                       NaN   NaN  \n",
       " 5                                       NaN   NaN  \n",
       " 6                                       NaN   NaN  \n",
       " 7                                       NaN   NaN  \n",
       " 8                                       NaN   NaN  \n",
       " 9                                       NaN   NaN  \n",
       " 10                                      NaN   NaN  \n",
       " 11                                      NaN   NaN  \n",
       " 12                                      NaN   NaN  \n",
       " 13                                      NaN   NaN  \n",
       " 14                                      NaN   NaN  \n",
       " 15                                      NaN   NaN  \n",
       " 16                                      NaN   NaN  \n",
       " 17                                      NaN   NaN  \n",
       " 18                                      NaN   NaN  \n",
       " 19                                      NaN   NaN  \n",
       " 20                                      NaN   NaN  \n",
       " 21                                      NaN   NaN  \n",
       " 22                                      NaN   NaN  \n",
       " 23                                      NaN   NaN  \n",
       " 24                                      NaN   NaN  \n",
       " 25                                      NaN   NaN  \n",
       " 26                                      NaN   NaN  \n",
       " 27                                      NaN   NaN  \n",
       " 28                                      NaN   NaN  \n",
       " 29                                      NaN   NaN  \n",
       " ..                                      ...   ...  \n",
       " 336                                   [[0]]  0.00  \n",
       " 337                                     NaN   NaN  \n",
       " 338                                     NaN   NaN  \n",
       " 339                                     NaN   NaN  \n",
       " 340                                     NaN   NaN  \n",
       " 341                                   [[0]]  0.00  \n",
       " 342                                     NaN   NaN  \n",
       " 343                                     NaN   NaN  \n",
       " 344                                     NaN   NaN  \n",
       " 345                                   [[0]]  0.00  \n",
       " 346                                   [[0]]  0.00  \n",
       " 347                                     NaN   NaN  \n",
       " 348                                     NaN   NaN  \n",
       " 349                                   [[0]]  0.00  \n",
       " 350  [[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0]]  0.25  \n",
       " 351                                [[0, 0]]  0.00  \n",
       " 352                       [[0, 0, 0, 0, 1]]  0.20  \n",
       " 353                          [[0, 0, 0, 0]]  0.00  \n",
       " 354                                     NaN   NaN  \n",
       " 355                                     NaN   NaN  \n",
       " 356                          [[0, 0, 0, 0]]  0.00  \n",
       " 357                                   [[0]]  0.00  \n",
       " 358                                     NaN   NaN  \n",
       " 359                                   [[0]]  0.00  \n",
       " 360                                   [[0]]  0.00  \n",
       " 361                                     NaN   NaN  \n",
       " 362                                   [[0]]  0.00  \n",
       " 363                                     NaN   NaN  \n",
       " 364                                     NaN   NaN  \n",
       " 365                                   [[0]]  0.00  \n",
       " \n",
       " [366 rows x 4 columns],\n",
       " 'MMM_2014':                                                  Tweet       Date  \\\n",
       " 0    [what i own and why in highly abbreviated form... 2014-01-01   \n",
       " 1    [things i am watching for friday srpt ibm nq b... 2014-01-02   \n",
       " 2                                                   [] 2014-01-03   \n",
       " 3                                                   [] 2014-01-04   \n",
       " 4    [ces smart pill boxes invisible speakers home ... 2014-01-05   \n",
       " 5                                                   [] 2014-01-06   \n",
       " 6                                                   [] 2014-01-07   \n",
       " 7    [3m s ces showcase enabled by lux backed user ... 2014-01-08   \n",
       " 8         [3m company dividend stock analysis url mmm] 2014-01-09   \n",
       " 9                                                   [] 2014-01-10   \n",
       " 10   [don t miss my most recent dividend stock anal... 2014-01-11   \n",
       " 11   [mmm stock forecast based on algorithms chart ... 2014-01-12   \n",
       " 12                                                  [] 2014-01-13   \n",
       " 13                                                  [] 2014-01-14   \n",
       " 14                                                  [] 2014-01-15   \n",
       " 15                                                  [] 2014-01-16   \n",
       " 16   [the unadulterated truth about dividend growth... 2014-01-17   \n",
       " 17                                                  [] 2014-01-18   \n",
       " 18                                                  [] 2014-01-19   \n",
       " 19                                                  [] 2014-01-20   \n",
       " 20                                                  [] 2014-01-21   \n",
       " 21   [solving 1 trading frustration adapting to cha... 2014-01-22   \n",
       " 22                                                  [] 2014-01-23   \n",
       " 23                                                  [] 2014-01-24   \n",
       " 24   [charlie dreifus buying large caps amid small ... 2014-01-25   \n",
       " 25   [big week aapl amzn goog fb cat mmm all with e... 2014-01-26   \n",
       " 26   [s p 5 is this a buying opportunity aep appl c... 2014-01-27   \n",
       " 27   [daily battle plan pivotal day for market aapl... 2014-01-28   \n",
       " 28   [thurs xom v ups mmm cop mo oxy celg lly cl vi... 2014-01-29   \n",
       " 29   [thursday s stocks to watch exxon ups 3m url x... 2014-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 336                                                 [] 2014-12-03   \n",
       " 337  [after disney dis exceeded with dividend hike ... 2014-12-04   \n",
       " 338                                                 [] 2014-12-05   \n",
       " 339                                                 [] 2014-12-06   \n",
       " 340                                                 [] 2014-12-07   \n",
       " 341  [mmm trade idea url via user, user do you thin... 2014-12-08   \n",
       " 342  [stocks i m keeping my eye on should you mmm s... 2014-12-09   \n",
       " 343  [exit to close and filled i sold all 16 mmm we... 2014-12-10   \n",
       " 344  [mmm mmm up 78 percent today mmm stock high is... 2014-12-11   \n",
       " 345  [get pennystock research on ibkr clf mmm fb mo... 2014-12-12   \n",
       " 346                                                 [] 2014-12-13   \n",
       " 347  [airline stocks are among the 3 top performing... 2014-12-14   \n",
       " 348  [hon 2 15 eps light vs consensus ahead of anal... 2014-12-15   \n",
       " 349  [companies returning to shareholders cvs mmm b... 2014-12-16   \n",
       " 350                                                 [] 2014-12-17   \n",
       " 351  [more than 6 large cap stocks hit new all time... 2014-12-18   \n",
       " 352                                                 [] 2014-12-19   \n",
       " 353  [mmm up to date company activities and other i... 2014-12-20   \n",
       " 354  [jake13 hdog hub mmm opinion today click here ... 2014-12-21   \n",
       " 355  [mmm up 35 from the oct 135 jan 2 15 otm optio... 2014-12-22   \n",
       " 356  [so how did we get to dow 18k v has had the mo... 2014-12-23   \n",
       " 357  [so the 1 point gain between dow 17k 18k v add... 2014-12-24   \n",
       " 358                                                 [] 2014-12-25   \n",
       " 359  [mmrf stock message board updated friday decem... 2014-12-26   \n",
       " 360                                                 [] 2014-12-27   \n",
       " 361  [if you don t consistently find trades like hg... 2014-12-28   \n",
       " 362  [user i hope so i want to show people that any... 2014-12-29   \n",
       " 363  [dow 3 top leaders mmm csco dis hd intc msft u... 2014-12-30   \n",
       " 364                                                 [] 2014-12-31   \n",
       " 365  [dow stocks trend axp utx csco ko hd dis v ibm... 2013-12-31   \n",
       " \n",
       "                                              emot      mean  \n",
       " 0    [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]  0.000000  \n",
       " 1                                           [[0]]  0.000000  \n",
       " 2                                             NaN       NaN  \n",
       " 3                                             NaN       NaN  \n",
       " 4                                           [[0]]  0.000000  \n",
       " 5                                             NaN       NaN  \n",
       " 6                                             NaN       NaN  \n",
       " 7                                           [[0]]  0.000000  \n",
       " 8                                           [[0]]  0.000000  \n",
       " 9                                             NaN       NaN  \n",
       " 10                                       [[1, 0]]  0.500000  \n",
       " 11                                          [[1]]  1.000000  \n",
       " 12                                            NaN       NaN  \n",
       " 13                                            NaN       NaN  \n",
       " 14                                            NaN       NaN  \n",
       " 15                                            NaN       NaN  \n",
       " 16                                          [[0]]  0.000000  \n",
       " 17                                            NaN       NaN  \n",
       " 18                                            NaN       NaN  \n",
       " 19                                            NaN       NaN  \n",
       " 20                                            NaN       NaN  \n",
       " 21                                          [[0]]  0.000000  \n",
       " 22                                            NaN       NaN  \n",
       " 23                                            NaN       NaN  \n",
       " 24                                       [[0, 0]]  0.000000  \n",
       " 25                                          [[0]]  0.000000  \n",
       " 26                                          [[0]]  0.000000  \n",
       " 27                                          [[0]]  0.000000  \n",
       " 28                                       [[0, 0]]  0.000000  \n",
       " 29                                    [[0, 0, 0]]  0.000000  \n",
       " ..                                            ...       ...  \n",
       " 336                                           NaN       NaN  \n",
       " 337                                         [[0]]  0.000000  \n",
       " 338                                           NaN       NaN  \n",
       " 339                                           NaN       NaN  \n",
       " 340                                           NaN       NaN  \n",
       " 341                                   [[0, 0, 0]]  0.000000  \n",
       " 342                                      [[0, 0]]  0.000000  \n",
       " 343                                [[1, 1, 0, 0]]  0.500000  \n",
       " 344                                         [[0]]  0.000000  \n",
       " 345                                      [[0, 0]]  0.000000  \n",
       " 346                                           NaN       NaN  \n",
       " 347                                [[0, 0, 0, 0]]  0.000000  \n",
       " 348                                      [[0, 0]]  0.000000  \n",
       " 349           [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]  0.000000  \n",
       " 350                                           NaN       NaN  \n",
       " 351                          [[0, 1, 0, 0, 0, 0]]  0.166667  \n",
       " 352                                           NaN       NaN  \n",
       " 353                                         [[0]]  0.000000  \n",
       " 354                                      [[0, 0]]  0.000000  \n",
       " 355                             [[0, 0, 0, 0, 1]]  0.200000  \n",
       " 356                                [[0, 1, 0, 0]]  0.250000  \n",
       " 357                                [[0, 0, 0, 0]]  0.000000  \n",
       " 358                                           NaN       NaN  \n",
       " 359                                      [[0, 0]]  0.000000  \n",
       " 360                                           NaN       NaN  \n",
       " 361                                         [[0]]  0.000000  \n",
       " 362                                      [[0, 0]]  0.000000  \n",
       " 363                                [[0, 0, 0, 0]]  0.000000  \n",
       " 364                                           NaN       NaN  \n",
       " 365                                         [[0]]  0.000000  \n",
       " \n",
       " [366 rows x 4 columns],\n",
       " 'MMM_2015':                                                  Tweet       Date  \\\n",
       " 0    [my top 3 holdings for 2 15 url jnj mmm pep, 3... 2015-01-01   \n",
       " 1    [check out a new year s edition of longform li... 2015-01-02   \n",
       " 2                                                   [] 2015-01-03   \n",
       " 3    [7 stocks with the longest dividend growth his... 2015-01-04   \n",
       " 4                                                   [] 2015-01-05   \n",
       " 5                                                   [] 2015-01-06   \n",
       " 6                                                   [] 2015-01-07   \n",
       " 7    [members over user were alerted two days ago a... 2015-01-08   \n",
       " 8                                                   [] 2015-01-09   \n",
       " 9                                                   [] 2015-01-10   \n",
       " 10                                                  [] 2015-01-11   \n",
       " 11                                                  [] 2015-01-12   \n",
       " 12                                                  [] 2015-01-13   \n",
       " 13                                                  [] 2015-01-14   \n",
       " 14                                                  [] 2015-01-15   \n",
       " 15                                                  [] 2015-01-16   \n",
       " 16                                                  [] 2015-01-17   \n",
       " 17                                                  [] 2015-01-18   \n",
       " 18                                                  [] 2015-01-19   \n",
       " 19                                                  [] 2015-01-20   \n",
       " 20                                                  [] 2015-01-21   \n",
       " 21   [best dividend paying stocks up to 2 23 long 4... 2015-01-22   \n",
       " 22   [week ahead preview s greek elections m msft t... 2015-01-23   \n",
       " 23                                                  [] 2015-01-24   \n",
       " 24                                                  [] 2015-01-25   \n",
       " 25   [companies reporting earnings before tuesday s... 2015-01-26   \n",
       " 26   [dow futures off nearly 3 points after data mi... 2015-01-27   \n",
       " 27                                                  [] 2015-01-28   \n",
       " 28   [after visa s 4 1 split gs will be most expens... 2015-01-29   \n",
       " 29                                                  [] 2015-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 336                                                 [] 2015-12-03   \n",
       " 337                                                 [] 2015-12-04   \n",
       " 338                                                 [] 2015-12-05   \n",
       " 339  [plug powers potential market for forklifts is... 2015-12-06   \n",
       " 340                                                 [] 2015-12-07   \n",
       " 341                                                 [] 2015-12-08   \n",
       " 342  [are boeing ge and 3m better buys than the ind... 2015-12-09   \n",
       " 343                                                 [] 2015-12-10   \n",
       " 344  [merrill lynch out with top 1 stock picks for ... 2015-12-11   \n",
       " 345                                                 [] 2015-12-12   \n",
       " 346  [swing trading watch list mmm wba yhoo glw adb... 2015-12-13   \n",
       " 347  [mmm short setup, what s ahead for stocks bond... 2015-12-14   \n",
       " 348  [as we said in url mmm is a great long term ho... 2015-12-15   \n",
       " 349  [most popular tweets aapl spx twtr fb amzn dis... 2015-12-16   \n",
       " 350  [rt user top ten dividend growth stocks url mm... 2015-12-17   \n",
       " 351  [today s free stock is mmm 3m buy price 147 21... 2015-12-18   \n",
       " 352  [how to retire at 59 with 9 in net worth url a... 2015-12-19   \n",
       " 353                                                 [] 2015-12-20   \n",
       " 354  [argus says 3m s post earnings weakness is a b... 2015-12-21   \n",
       " 355  [another amazing buy sell signal from our site... 2015-12-22   \n",
       " 356                                                 [] 2015-12-23   \n",
       " 357  [mmm pg jnj safm wmt awr just updated analysis... 2015-12-24   \n",
       " 358                                                 [] 2015-12-25   \n",
       " 359                                                 [] 2015-12-26   \n",
       " 360                                                 [] 2015-12-27   \n",
       " 361  [mmm investor opinions updated monday december... 2015-12-28   \n",
       " 362  [4 blue chip stocks to buy now as dollar stren... 2015-12-29   \n",
       " 363  [mmm xpel v been reading good things about xpe... 2015-12-30   \n",
       " 364                                                 [] 2015-12-31   \n",
       " 365  [dow stocks trend hd jpm dd v gs trv wmt axp b... 2014-12-31   \n",
       " \n",
       "                                                   emot      mean  \n",
       " 0                        [[0, 0, 0, 0, 0, 0, 0, 0, 0]]  0.000000  \n",
       " 1                                       [[0, 0, 0, 0]]  0.000000  \n",
       " 2                                                  NaN       NaN  \n",
       " 3                                                [[0]]  0.000000  \n",
       " 4                                                  NaN       NaN  \n",
       " 5                                                  NaN       NaN  \n",
       " 6                                                  NaN       NaN  \n",
       " 7                                             [[0, 0]]  0.000000  \n",
       " 8                                                  NaN       NaN  \n",
       " 9                                                  NaN       NaN  \n",
       " 10                                                 NaN       NaN  \n",
       " 11                                                 NaN       NaN  \n",
       " 12                                                 NaN       NaN  \n",
       " 13                                                 NaN       NaN  \n",
       " 14                                                 NaN       NaN  \n",
       " 15                                                 NaN       NaN  \n",
       " 16                                                 NaN       NaN  \n",
       " 17                                                 NaN       NaN  \n",
       " 18                                                 NaN       NaN  \n",
       " 19                                                 NaN       NaN  \n",
       " 20                                                 NaN       NaN  \n",
       " 21                                               [[0]]  0.000000  \n",
       " 22                                            [[0, 0]]  0.000000  \n",
       " 23                                                 NaN       NaN  \n",
       " 24                                                 NaN       NaN  \n",
       " 25                             [[0, 0, 0, 0, 0, 0, 0]]  0.000000  \n",
       " 26                                [[0, 1, 0, 0, 0, 0]]  0.166667  \n",
       " 27                                                 NaN       NaN  \n",
       " 28                                            [[0, 0]]  0.000000  \n",
       " 29                                                 NaN       NaN  \n",
       " ..                                                 ...       ...  \n",
       " 336                                                NaN       NaN  \n",
       " 337                                                NaN       NaN  \n",
       " 338                                                NaN       NaN  \n",
       " 339                                           [[0, 0]]  0.000000  \n",
       " 340                                                NaN       NaN  \n",
       " 341                                                NaN       NaN  \n",
       " 342                                              [[0]]  0.000000  \n",
       " 343                                                NaN       NaN  \n",
       " 344                                              [[0]]  0.000000  \n",
       " 345                                                NaN       NaN  \n",
       " 346                                           [[0, 0]]  0.000000  \n",
       " 347                                           [[0, 0]]  0.000000  \n",
       " 348  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,...  0.212121  \n",
       " 349                                           [[0, 0]]  0.000000  \n",
       " 350                                           [[0, 0]]  0.000000  \n",
       " 351                                              [[0]]  0.000000  \n",
       " 352                                           [[1, 0]]  0.500000  \n",
       " 353                                                NaN       NaN  \n",
       " 354                                              [[0]]  0.000000  \n",
       " 355                                              [[0]]  0.000000  \n",
       " 356                                                NaN       NaN  \n",
       " 357                                              [[0]]  0.000000  \n",
       " 358                                                NaN       NaN  \n",
       " 359                                                NaN       NaN  \n",
       " 360                                                NaN       NaN  \n",
       " 361                                           [[0, 1]]  0.500000  \n",
       " 362                               [[0, 0, 0, 0, 0, 1]]  0.166667  \n",
       " 363                                           [[0, 0]]  0.000000  \n",
       " 364                                                NaN       NaN  \n",
       " 365                                              [[0]]  0.000000  \n",
       " \n",
       " [366 rows x 4 columns],\n",
       " 'MMM_2016':                                                  Tweet       Date  \\\n",
       " 0    [bert s 5 stocks to always buy url mmm emr jnj... 2016-01-01   \n",
       " 1                                                   [] 2016-01-02   \n",
       " 2                                                   [] 2016-01-03   \n",
       " 3    [top investment picks from 38 bloggers for 2 1... 2016-01-04   \n",
       " 4                                                   [] 2016-01-05   \n",
       " 5                                                   [] 2016-01-06   \n",
       " 6                                                   [] 2016-01-07   \n",
       " 7                                                   [] 2016-01-08   \n",
       " 8                                                   [] 2016-01-09   \n",
       " 9                                                   [] 2016-01-10   \n",
       " 10                                                  [] 2016-01-11   \n",
       " 11                                                  [] 2016-01-12   \n",
       " 12                                                  [] 2016-01-13   \n",
       " 13                                                  [] 2016-01-14   \n",
       " 14                                                  [] 2016-01-15   \n",
       " 15                                                  [] 2016-01-16   \n",
       " 16                                                  [] 2016-01-17   \n",
       " 17                                                  [] 2016-01-18   \n",
       " 18   [twtr is officially cheap 9 75b ex cash p s ra... 2016-01-19   \n",
       " 19                                                  [] 2016-01-20   \n",
       " 20   [thursday s top analyst upgrades downgrades pa... 2016-01-21   \n",
       " 21                                                  [] 2016-01-22   \n",
       " 22                                                  [] 2016-01-23   \n",
       " 23   [notable earnings for the week amgn lmt mmm mc... 2016-01-24   \n",
       " 24   [seasonal charts of companies reporting earnin... 2016-01-25   \n",
       " 25   [johnson johnson proctor gamble dupont and 3m ... 2016-01-26   \n",
       " 26   [only 4 stocks remain higher on the dow vz 95 ... 2016-01-27   \n",
       " 27                                                  [] 2016-01-28   \n",
       " 28                                                  [] 2016-01-29   \n",
       " 29                                                  [] 2016-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 336                                                 [] 2016-12-03   \n",
       " 337                                  [mmm axp aapl ba] 2016-12-04   \n",
       " 338                                                 [] 2016-12-05   \n",
       " 339                                                 [] 2016-12-06   \n",
       " 340  [early movers wen luv low unh mmm mu k ma more... 2016-12-07   \n",
       " 341  [intriguing dividend ideas some of our analyst... 2016-12-08   \n",
       " 342  [mmm 2 month high in 3rd day of move above med... 2016-12-09   \n",
       " 343                                                 [] 2016-12-10   \n",
       " 344                                                 [] 2016-12-11   \n",
       " 345  [5 rocket stocks to buy before the fed s rate ... 2016-12-12   \n",
       " 346  [user presents 2 17 outlook url mmm, gm all es... 2016-12-13   \n",
       " 347  [top sentiment for sp15 stocks at market open ... 2016-12-14   \n",
       " 348                                                 [] 2016-12-15   \n",
       " 349  [don t be around sick people to catch their co... 2016-12-16   \n",
       " 350  [long mmm, dj3 wkly elder impulse system green... 2016-12-17   \n",
       " 351  [themotleyfool 3 top dividend aristocrats to b... 2016-12-18   \n",
       " 352       [5 key takeaways from 3m s 2 17 outlook mmm] 2016-12-19   \n",
       " 353  [these are the best and worst performing stock... 2016-12-20   \n",
       " 354  [mmm up to resistance 18 re jul sep overhead o... 2016-12-21   \n",
       " 355  [today s watchlist includes ge mfst f sl mmm d... 2016-12-22   \n",
       " 356  [next year remind me to buy mmm in october so ... 2016-12-23   \n",
       " 357  [themotleyfool 5 key takeaways from 3m company... 2016-12-24   \n",
       " 358  [4 reasons i prefer 3m over generalelectric in... 2016-12-25   \n",
       " 359  [how hedge funds trade options gpro fit feye b... 2016-12-26   \n",
       " 360  [open positions 12 23 16 4pm est bita ttmi mmm... 2016-12-27   \n",
       " 361                                                 [] 2016-12-28   \n",
       " 362  [introduction to the dividends forever portfol... 2016-12-29   \n",
       " 363  [mmm nearing big level into 2 17 mar 18 calls ... 2016-12-30   \n",
       " 364                                                 [] 2016-12-31   \n",
       " 365  [mufg americas sells 14 697 shares of 3m co mm... 2015-12-31   \n",
       " \n",
       "                                        emot      mean  \n",
       " 0    [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]  0.000000  \n",
       " 1                                       NaN       NaN  \n",
       " 2                                       NaN       NaN  \n",
       " 3                                     [[0]]  0.000000  \n",
       " 4                                       NaN       NaN  \n",
       " 5                                       NaN       NaN  \n",
       " 6                                       NaN       NaN  \n",
       " 7                                       NaN       NaN  \n",
       " 8                                       NaN       NaN  \n",
       " 9                                       NaN       NaN  \n",
       " 10                                      NaN       NaN  \n",
       " 11                                      NaN       NaN  \n",
       " 12                                      NaN       NaN  \n",
       " 13                                      NaN       NaN  \n",
       " 14                                      NaN       NaN  \n",
       " 15                                      NaN       NaN  \n",
       " 16                                      NaN       NaN  \n",
       " 17                                      NaN       NaN  \n",
       " 18                                    [[0]]  0.000000  \n",
       " 19                                      NaN       NaN  \n",
       " 20                                    [[0]]  0.000000  \n",
       " 21                                      NaN       NaN  \n",
       " 22                                      NaN       NaN  \n",
       " 23                                 [[0, 0]]  0.000000  \n",
       " 24                                    [[0]]  0.000000  \n",
       " 25                        [[0, 0, 1, 0, 0]]  0.200000  \n",
       " 26                                 [[0, 0]]  0.000000  \n",
       " 27                                      NaN       NaN  \n",
       " 28                                      NaN       NaN  \n",
       " 29                                      NaN       NaN  \n",
       " ..                                      ...       ...  \n",
       " 336                                     NaN       NaN  \n",
       " 337                                   [[0]]  0.000000  \n",
       " 338                                     NaN       NaN  \n",
       " 339                                     NaN       NaN  \n",
       " 340                 [[0, 0, 0, 0, 0, 0, 0]]  0.000000  \n",
       " 341                             [[0, 0, 0]]  0.000000  \n",
       " 342                             [[0, 0, 0]]  0.000000  \n",
       " 343                                     NaN       NaN  \n",
       " 344                                     NaN       NaN  \n",
       " 345                                   [[0]]  0.000000  \n",
       " 346                       [[0, 0, 0, 0, 0]]  0.000000  \n",
       " 347                       [[0, 0, 0, 1, 0]]  0.200000  \n",
       " 348                                     NaN       NaN  \n",
       " 349                                [[1, 0]]  0.500000  \n",
       " 350                             [[0, 0, 0]]  0.000000  \n",
       " 351                                   [[0]]  0.000000  \n",
       " 352                                   [[0]]  0.000000  \n",
       " 353                                [[0, 0]]  0.000000  \n",
       " 354                                [[0, 1]]  0.500000  \n",
       " 355                                [[0, 0]]  0.000000  \n",
       " 356                                   [[1]]  1.000000  \n",
       " 357                                   [[0]]  0.000000  \n",
       " 358                                   [[0]]  0.000000  \n",
       " 359                                   [[0]]  0.000000  \n",
       " 360                                   [[0]]  0.000000  \n",
       " 361                                     NaN       NaN  \n",
       " 362                                   [[0]]  0.000000  \n",
       " 363                             [[1, 0, 0]]  0.333333  \n",
       " 364                                     NaN       NaN  \n",
       " 365                          [[0, 0, 0, 0]]  0.000000  \n",
       " \n",
       " [366 rows x 4 columns],\n",
       " 'MMM_2017':                                                  Tweet       Date  \\\n",
       " 0    [3m company high quality dividend champion but... 2017-01-01   \n",
       " 1    [tomorrow s watch list includes ge mfst f sl m... 2017-01-02   \n",
       " 2    [4 ways to trade 3m bonus idea url mmm spy, 4 ... 2017-01-03   \n",
       " 3    [mmm is showing a nice setup don t miss this o... 2017-01-04   \n",
       " 4                                                   [] 2017-01-05   \n",
       " 5                                                   [] 2017-01-06   \n",
       " 6                                                   [] 2017-01-07   \n",
       " 7    [just updated 2 17 dividend kings list url awr... 2017-01-08   \n",
       " 8                                                   [] 2017-01-09   \n",
       " 9                                                   [] 2017-01-10   \n",
       " 10   [3m sells patents relating to lithium ion batt... 2017-01-11   \n",
       " 11                                                  [] 2017-01-12   \n",
       " 12                                                  [] 2017-01-13   \n",
       " 13   [current triggered trades w entry dates ts tra... 2017-01-14   \n",
       " 14                                                  [] 2017-01-15   \n",
       " 15                                                  [] 2017-01-16   \n",
       " 16                                                  [] 2017-01-17   \n",
       " 17                                                  [] 2017-01-18   \n",
       " 18                                                  [] 2017-01-19   \n",
       " 19   [trump s 1st week a busy one for market earnin... 2017-01-20   \n",
       " 20   [3m investors look for a dividend increase soo... 2017-01-21   \n",
       " 21   [lol gotta love ducktape my mmm was up close t... 2017-01-22   \n",
       " 22   [earnings tomorrow morning baba aks vz jnj glw... 2017-01-23   \n",
       " 23   [the new user is born never made 4 earning tra... 2017-01-24   \n",
       " 24                                                  [] 2017-01-25   \n",
       " 25                                                  [] 2017-01-26   \n",
       " 26                                                  [] 2017-01-27   \n",
       " 27                                                  [] 2017-01-28   \n",
       " 28                                                  [] 2017-01-29   \n",
       " 29                                                  [] 2017-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 336  [2 17 dividend aristocrats list url dov emr gp... 2017-12-03   \n",
       " 337  [77 s p 5 record highs so far today incl mcdon... 2017-12-04   \n",
       " 338  [someone long a ton of industrials just bought... 2017-12-05   \n",
       " 339  [toro adds 3m executive to its board url user ... 2017-12-06   \n",
       " 340  [was aware mmm scanners could submit but i was... 2017-12-07   \n",
       " 341  [this analyst predicted ge s decline is he rig... 2017-12-08   \n",
       " 342                                                 [] 2017-12-09   \n",
       " 343                                                 [] 2017-12-10   \n",
       " 344  [earnings top abm adbe arwr band casy cost irm... 2017-12-11   \n",
       " 345  [our top trade this hour with some big dow mov... 2017-12-12   \n",
       " 346  [3m sees growth at home and away but twin citi... 2017-12-13   \n",
       " 347  [user thanks for follow fridaymotivation see v... 2017-12-14   \n",
       " 348  [current swing trades alerted scaling out of m... 2017-12-15   \n",
       " 349                                                 [] 2017-12-16   \n",
       " 350  [what people are just starting to realize is t... 2017-12-17   \n",
       " 351  [general electric slumps in q4 as 3m texas ins... 2017-12-18   \n",
       " 352  [5 another green day for our swing trade portf... 2017-12-19   \n",
       " 353  [mmm 3m bearish macd divergence on the daily c... 2017-12-20   \n",
       " 354  [capital market laboratories mmm options backt... 2017-12-21   \n",
       " 355                                                 [] 2017-12-22   \n",
       " 356                                                 [] 2017-12-23   \n",
       " 357  [that s a good one coach love those dividend m... 2017-12-24   \n",
       " 358                                                 [] 2017-12-25   \n",
       " 359  [3m dupont ppg corning new entrants in car ele... 2017-12-26   \n",
       " 360                                                 [] 2017-12-27   \n",
       " 361  [2 17 dow jones industrial avg top performers ... 2017-12-28   \n",
       " 362  [countdown to 2 18 can 3m make it a three peat... 2017-12-29   \n",
       " 363                                                 [] 2017-12-30   \n",
       " 364                                                 [] 2017-12-31   \n",
       " 365  [themotleyfool a strong case for buying parker... 2016-12-31   \n",
       " \n",
       "                                        emot      mean  \n",
       " 0                                     [[0]]  0.000000  \n",
       " 1                      [[0, 0, 0, 0, 0, 0]]  0.000000  \n",
       " 2                   [[0, 0, 0, 0, 0, 0, 0]]  0.000000  \n",
       " 3                                     [[0]]  0.000000  \n",
       " 4                                       NaN       NaN  \n",
       " 5                                       NaN       NaN  \n",
       " 6                                       NaN       NaN  \n",
       " 7                                     [[0]]  0.000000  \n",
       " 8                                       NaN       NaN  \n",
       " 9                                       NaN       NaN  \n",
       " 10                                    [[0]]  0.000000  \n",
       " 11                                      NaN       NaN  \n",
       " 12                                      NaN       NaN  \n",
       " 13                                    [[0]]  0.000000  \n",
       " 14                                      NaN       NaN  \n",
       " 15                                      NaN       NaN  \n",
       " 16                                      NaN       NaN  \n",
       " 17                                      NaN       NaN  \n",
       " 18                                      NaN       NaN  \n",
       " 19                                    [[0]]  0.000000  \n",
       " 20                                 [[0, 0]]  0.000000  \n",
       " 21                                 [[0, 0]]  0.000000  \n",
       " 22                     [[0, 0, 0, 0, 0, 0]]  0.000000  \n",
       " 23                        [[0, 0, 0, 0, 1]]  0.200000  \n",
       " 24                                      NaN       NaN  \n",
       " 25                                      NaN       NaN  \n",
       " 26                                      NaN       NaN  \n",
       " 27                                      NaN       NaN  \n",
       " 28                                      NaN       NaN  \n",
       " 29                                      NaN       NaN  \n",
       " ..                                      ...       ...  \n",
       " 336                                   [[0]]  0.000000  \n",
       " 337                                [[0, 0]]  0.000000  \n",
       " 338                                   [[1]]  1.000000  \n",
       " 339                                [[0, 0]]  0.000000  \n",
       " 340                                [[1, 0]]  0.500000  \n",
       " 341                                   [[0]]  0.000000  \n",
       " 342                                     NaN       NaN  \n",
       " 343                                     NaN       NaN  \n",
       " 344                 [[0, 0, 0, 1, 0, 0, 0]]  0.142857  \n",
       " 345  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]  0.000000  \n",
       " 346                             [[1, 0, 0]]  0.333333  \n",
       " 347                                   [[0]]  0.000000  \n",
       " 348                                [[0, 0]]  0.000000  \n",
       " 349                                     NaN       NaN  \n",
       " 350                                [[0, 0]]  0.000000  \n",
       " 351                          [[0, 0, 0, 0]]  0.000000  \n",
       " 352                                   [[0]]  0.000000  \n",
       " 353                                   [[0]]  0.000000  \n",
       " 354                                [[0, 0]]  0.000000  \n",
       " 355                                     NaN       NaN  \n",
       " 356                                     NaN       NaN  \n",
       " 357                                   [[0]]  0.000000  \n",
       " 358                                     NaN       NaN  \n",
       " 359                             [[0, 0, 0]]  0.000000  \n",
       " 360                                     NaN       NaN  \n",
       " 361                                [[0, 0]]  0.000000  \n",
       " 362                       [[0, 0, 0, 0, 0]]  0.000000  \n",
       " 363                                     NaN       NaN  \n",
       " 364                                     NaN       NaN  \n",
       " 365                       [[0, 0, 0, 0, 0]]  0.000000  \n",
       " \n",
       " [366 rows x 4 columns],\n",
       " 'MMM_2018':                                                  Tweet       Date  \\\n",
       " 0    [at 37 x ev ebitda neogen neog is above all co... 2018-01-01   \n",
       " 1    [mmm xli mmm outperformed the industrial selec... 2018-01-02   \n",
       " 2                                                   [] 2018-01-03   \n",
       " 3    [lunch money on user dow crosses 25 mark for f... 2018-01-04   \n",
       " 4                                                   [] 2018-01-05   \n",
       " 5                                                   [] 2018-01-06   \n",
       " 6    [dragonfly capital 5 trade ideas for monday bs... 2018-01-07   \n",
       " 7    [mmm 175 puts selling activity expiring on 16t... 2018-01-08   \n",
       " 8    [xli industrials if this isnt beast mode i don... 2018-01-09   \n",
       " 9    [spy back from 22 hour flight traded from lax ... 2018-01-10   \n",
       " 10                                                  [] 2018-01-11   \n",
       " 11   [dlpal dq suggested long mrk mmm and short ge ... 2018-01-12   \n",
       " 12   [anyone and everyone including user hmu for be... 2018-01-13   \n",
       " 13                                                  [] 2018-01-14   \n",
       " 14   [tomorrow s watchlist all longs adbe baba qqq ... 2018-01-15   \n",
       " 15   [largest notional sell on close order imbalanc... 2018-01-16   \n",
       " 16   [the dow gained 322 79 points yesterday 7 of t... 2018-01-17   \n",
       " 17   [3m mmm receives daily news sentiment score of... 2018-01-18   \n",
       " 18   [stocks mostly up why apple may have more thru... 2018-01-19   \n",
       " 19   [earnings for the week nflx ge hal intc cat lr... 2018-01-20   \n",
       " 20                                                  [] 2018-01-21   \n",
       " 21                                                  [] 2018-01-22   \n",
       " 22                                                  [] 2018-01-23   \n",
       " 23   [earnings schedule before tomorrow s open cat ... 2018-01-24   \n",
       " 24                                                  [] 2018-01-25   \n",
       " 25                                                  [] 2018-01-26   \n",
       " 26                                                  [] 2018-01-27   \n",
       " 27                                                  [] 2018-01-28   \n",
       " 28                                                  [] 2018-01-29   \n",
       " 29                                                  [] 2018-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 336  [did you know that investing in stocks can not... 2018-12-03   \n",
       " 337                                                 [] 2018-12-04   \n",
       " 338                 [long mmm selling all those vests] 2018-12-05   \n",
       " 339  [a 9 fucking boat what the hell for as a taxpa... 2018-12-06   \n",
       " 340                                                 [] 2018-12-07   \n",
       " 341  [weekly bear flag re test of 185 93 likely mmm... 2018-12-08   \n",
       " 342                                                 [] 2018-12-09   \n",
       " 343  [and mmm is cheap, mmm below 19 what a grab le... 2018-12-10   \n",
       " 344                                                 [] 2018-12-11   \n",
       " 345  [opening put sales spreads from tuesday 12 11 ... 2018-12-12   \n",
       " 346                                                 [] 2018-12-13   \n",
       " 347  [dividend kings list for 2 19 cbsh abm awr ci ... 2018-12-14   \n",
       " 348                                                 [] 2018-12-15   \n",
       " 349                                                 [] 2018-12-16   \n",
       " 350  [all the biggest stocks that are now in a bear... 2018-12-17   \n",
       " 351                                                 [] 2018-12-18   \n",
       " 352  [3m co mmm reports agreement to acquire the te... 2018-12-19   \n",
       " 353                                                 [] 2018-12-20   \n",
       " 354                    [mmm sold entire long 189 78 8] 2018-12-21   \n",
       " 355  [to the doj when is maxie waters going to be i... 2018-12-22   \n",
       " 356                                                 [] 2018-12-23   \n",
       " 357  [mmm mmm a dow jones industrial stock moved be... 2018-12-24   \n",
       " 358  [unusual put buying wsm 13x average volume tza... 2018-12-25   \n",
       " 359  [yes i bought t and mmm, to the best of my abi... 2018-12-26   \n",
       " 360  [yea i ll figure it out i can t believe mmm ju... 2018-12-27   \n",
       " 361  [bought amzn brk b mmm pypl sq wp, market reco... 2018-12-28   \n",
       " 362  [3 of 5 jnj jpm ko mcd mmm mrk dark red arrows... 2018-12-29   \n",
       " 363                                                 [] 2018-12-30   \n",
       " 364                                                 [] 2018-12-31   \n",
       " 365  [q4 2 17 earnings estimate for 3m co mmm issue... 2017-12-31   \n",
       " \n",
       "                         emot      mean  \n",
       " 0       [[0, 0, 0, 0, 0, 0]]  0.000000  \n",
       " 1    [[0, 0, 0, 0, 1, 0, 0]]  0.142857  \n",
       " 2                        NaN       NaN  \n",
       " 3                [[0, 0, 0]]  0.000000  \n",
       " 4                        NaN       NaN  \n",
       " 5                        NaN       NaN  \n",
       " 6                      [[0]]  0.000000  \n",
       " 7                      [[0]]  0.000000  \n",
       " 8                      [[1]]  1.000000  \n",
       " 9                      [[0]]  0.000000  \n",
       " 10                       NaN       NaN  \n",
       " 11                     [[0]]  0.000000  \n",
       " 12               [[0, 0, 0]]  0.000000  \n",
       " 13                       NaN       NaN  \n",
       " 14                  [[0, 0]]  0.000000  \n",
       " 15                     [[0]]  0.000000  \n",
       " 16                     [[0]]  0.000000  \n",
       " 17               [[0, 0, 0]]  0.000000  \n",
       " 18                  [[1, 1]]  1.000000  \n",
       " 19                     [[0]]  0.000000  \n",
       " 20                       NaN       NaN  \n",
       " 21                       NaN       NaN  \n",
       " 22                       NaN       NaN  \n",
       " 23                     [[0]]  0.000000  \n",
       " 24                       NaN       NaN  \n",
       " 25                       NaN       NaN  \n",
       " 26                       NaN       NaN  \n",
       " 27                       NaN       NaN  \n",
       " 28                       NaN       NaN  \n",
       " 29                       NaN       NaN  \n",
       " ..                       ...       ...  \n",
       " 336                 [[0, 0]]  0.000000  \n",
       " 337                      NaN       NaN  \n",
       " 338                    [[0]]  0.000000  \n",
       " 339                    [[1]]  1.000000  \n",
       " 340                      NaN       NaN  \n",
       " 341              [[0, 0, 0]]  0.000000  \n",
       " 342                      NaN       NaN  \n",
       " 343                 [[0, 0]]  0.000000  \n",
       " 344                      NaN       NaN  \n",
       " 345                    [[0]]  0.000000  \n",
       " 346                      NaN       NaN  \n",
       " 347                 [[0, 0]]  0.000000  \n",
       " 348                      NaN       NaN  \n",
       " 349                      NaN       NaN  \n",
       " 350                    [[0]]  0.000000  \n",
       " 351                      NaN       NaN  \n",
       " 352                 [[0, 1]]  0.500000  \n",
       " 353                      NaN       NaN  \n",
       " 354                    [[1]]  1.000000  \n",
       " 355                    [[0]]  0.000000  \n",
       " 356                      NaN       NaN  \n",
       " 357                    [[0]]  0.000000  \n",
       " 358                 [[0, 0]]  0.000000  \n",
       " 359        [[0, 0, 0, 0, 0]]  0.000000  \n",
       " 360              [[0, 0, 0]]  0.000000  \n",
       " 361     [[0, 0, 0, 0, 0, 0]]  0.000000  \n",
       " 362              [[0, 0, 0]]  0.000000  \n",
       " 363                      NaN       NaN  \n",
       " 364                      NaN       NaN  \n",
       " 365              [[0, 0, 0]]  0.000000  \n",
       " \n",
       " [366 rows x 4 columns],\n",
       " 'SYF_2009':     Tweet       Date  emot  mean\n",
       " 0      [] 2009-01-01   NaN   NaN\n",
       " 1      [] 2009-01-02   NaN   NaN\n",
       " 2      [] 2009-01-03   NaN   NaN\n",
       " 3      [] 2009-01-04   NaN   NaN\n",
       " 4      [] 2009-01-05   NaN   NaN\n",
       " 5      [] 2009-01-06   NaN   NaN\n",
       " 6      [] 2009-01-07   NaN   NaN\n",
       " 7      [] 2009-01-08   NaN   NaN\n",
       " 8      [] 2009-01-09   NaN   NaN\n",
       " 9      [] 2009-01-10   NaN   NaN\n",
       " 10     [] 2009-01-11   NaN   NaN\n",
       " 11     [] 2009-01-12   NaN   NaN\n",
       " 12     [] 2009-01-13   NaN   NaN\n",
       " 13     [] 2009-01-14   NaN   NaN\n",
       " 14     [] 2009-01-15   NaN   NaN\n",
       " 15     [] 2009-01-16   NaN   NaN\n",
       " 16     [] 2009-01-17   NaN   NaN\n",
       " 17     [] 2009-01-18   NaN   NaN\n",
       " 18     [] 2009-01-19   NaN   NaN\n",
       " 19     [] 2009-01-20   NaN   NaN\n",
       " 20     [] 2009-01-21   NaN   NaN\n",
       " 21     [] 2009-01-22   NaN   NaN\n",
       " 22     [] 2009-01-23   NaN   NaN\n",
       " 23     [] 2009-01-24   NaN   NaN\n",
       " 24     [] 2009-01-25   NaN   NaN\n",
       " 25     [] 2009-01-26   NaN   NaN\n",
       " 26     [] 2009-01-27   NaN   NaN\n",
       " 27     [] 2009-01-28   NaN   NaN\n",
       " 28     [] 2009-01-29   NaN   NaN\n",
       " 29     [] 2009-01-30   NaN   NaN\n",
       " ..    ...        ...   ...   ...\n",
       " 335    [] 2009-12-02   NaN   NaN\n",
       " 336    [] 2009-12-03   NaN   NaN\n",
       " 337    [] 2009-12-04   NaN   NaN\n",
       " 338    [] 2009-12-05   NaN   NaN\n",
       " 339    [] 2009-12-06   NaN   NaN\n",
       " 340    [] 2009-12-07   NaN   NaN\n",
       " 341    [] 2009-12-08   NaN   NaN\n",
       " 342    [] 2009-12-09   NaN   NaN\n",
       " 343    [] 2009-12-10   NaN   NaN\n",
       " 344    [] 2009-12-11   NaN   NaN\n",
       " 345    [] 2009-12-12   NaN   NaN\n",
       " 346    [] 2009-12-13   NaN   NaN\n",
       " 347    [] 2009-12-14   NaN   NaN\n",
       " 348    [] 2009-12-15   NaN   NaN\n",
       " 349    [] 2009-12-16   NaN   NaN\n",
       " 350    [] 2009-12-17   NaN   NaN\n",
       " 351    [] 2009-12-18   NaN   NaN\n",
       " 352    [] 2009-12-19   NaN   NaN\n",
       " 353    [] 2009-12-20   NaN   NaN\n",
       " 354    [] 2009-12-21   NaN   NaN\n",
       " 355    [] 2009-12-22   NaN   NaN\n",
       " 356    [] 2009-12-23   NaN   NaN\n",
       " 357    [] 2009-12-24   NaN   NaN\n",
       " 358    [] 2009-12-25   NaN   NaN\n",
       " 359    [] 2009-12-26   NaN   NaN\n",
       " 360    [] 2009-12-27   NaN   NaN\n",
       " 361    [] 2009-12-28   NaN   NaN\n",
       " 362    [] 2009-12-29   NaN   NaN\n",
       " 363    [] 2009-12-30   NaN   NaN\n",
       " 364    [] 2009-12-31   NaN   NaN\n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'SYF_2010':     Tweet       Date  emot  mean\n",
       " 0      [] 2010-01-01   NaN   NaN\n",
       " 1      [] 2010-01-02   NaN   NaN\n",
       " 2      [] 2010-01-03   NaN   NaN\n",
       " 3      [] 2010-01-04   NaN   NaN\n",
       " 4      [] 2010-01-05   NaN   NaN\n",
       " 5      [] 2010-01-06   NaN   NaN\n",
       " 6      [] 2010-01-07   NaN   NaN\n",
       " 7      [] 2010-01-08   NaN   NaN\n",
       " 8      [] 2010-01-09   NaN   NaN\n",
       " 9      [] 2010-01-10   NaN   NaN\n",
       " 10     [] 2010-01-11   NaN   NaN\n",
       " 11     [] 2010-01-12   NaN   NaN\n",
       " 12     [] 2010-01-13   NaN   NaN\n",
       " 13     [] 2010-01-14   NaN   NaN\n",
       " 14     [] 2010-01-15   NaN   NaN\n",
       " 15     [] 2010-01-16   NaN   NaN\n",
       " 16     [] 2010-01-17   NaN   NaN\n",
       " 17     [] 2010-01-18   NaN   NaN\n",
       " 18     [] 2010-01-19   NaN   NaN\n",
       " 19     [] 2010-01-20   NaN   NaN\n",
       " 20     [] 2010-01-21   NaN   NaN\n",
       " 21     [] 2010-01-22   NaN   NaN\n",
       " 22     [] 2010-01-23   NaN   NaN\n",
       " 23     [] 2010-01-24   NaN   NaN\n",
       " 24     [] 2010-01-25   NaN   NaN\n",
       " 25     [] 2010-01-26   NaN   NaN\n",
       " 26     [] 2010-01-27   NaN   NaN\n",
       " 27     [] 2010-01-28   NaN   NaN\n",
       " 28     [] 2010-01-29   NaN   NaN\n",
       " 29     [] 2010-01-30   NaN   NaN\n",
       " ..    ...        ...   ...   ...\n",
       " 335    [] 2010-12-02   NaN   NaN\n",
       " 336    [] 2010-12-03   NaN   NaN\n",
       " 337    [] 2010-12-04   NaN   NaN\n",
       " 338    [] 2010-12-05   NaN   NaN\n",
       " 339    [] 2010-12-06   NaN   NaN\n",
       " 340    [] 2010-12-07   NaN   NaN\n",
       " 341    [] 2010-12-08   NaN   NaN\n",
       " 342    [] 2010-12-09   NaN   NaN\n",
       " 343    [] 2010-12-10   NaN   NaN\n",
       " 344    [] 2010-12-11   NaN   NaN\n",
       " 345    [] 2010-12-12   NaN   NaN\n",
       " 346    [] 2010-12-13   NaN   NaN\n",
       " 347    [] 2010-12-14   NaN   NaN\n",
       " 348    [] 2010-12-15   NaN   NaN\n",
       " 349    [] 2010-12-16   NaN   NaN\n",
       " 350    [] 2010-12-17   NaN   NaN\n",
       " 351    [] 2010-12-18   NaN   NaN\n",
       " 352    [] 2010-12-19   NaN   NaN\n",
       " 353    [] 2010-12-20   NaN   NaN\n",
       " 354    [] 2010-12-21   NaN   NaN\n",
       " 355    [] 2010-12-22   NaN   NaN\n",
       " 356    [] 2010-12-23   NaN   NaN\n",
       " 357    [] 2010-12-24   NaN   NaN\n",
       " 358    [] 2010-12-25   NaN   NaN\n",
       " 359    [] 2010-12-26   NaN   NaN\n",
       " 360    [] 2010-12-27   NaN   NaN\n",
       " 361    [] 2010-12-28   NaN   NaN\n",
       " 362    [] 2010-12-29   NaN   NaN\n",
       " 363    [] 2010-12-30   NaN   NaN\n",
       " 364    [] 2010-12-31   NaN   NaN\n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'SYF_2011':     Tweet       Date emot  mean\n",
       " 0      [] 2011-01-01  NaN   NaN\n",
       " 1      [] 2011-01-02  NaN   NaN\n",
       " 2      [] 2011-01-03  NaN   NaN\n",
       " 3      [] 2011-01-04  NaN   NaN\n",
       " 4      [] 2011-01-05  NaN   NaN\n",
       " 5      [] 2011-01-06  NaN   NaN\n",
       " 6      [] 2011-01-07  NaN   NaN\n",
       " 7      [] 2011-01-08  NaN   NaN\n",
       " 8      [] 2011-01-09  NaN   NaN\n",
       " 9      [] 2011-01-10  NaN   NaN\n",
       " 10     [] 2011-01-11  NaN   NaN\n",
       " 11     [] 2011-01-12  NaN   NaN\n",
       " 12     [] 2011-01-13  NaN   NaN\n",
       " 13     [] 2011-01-14  NaN   NaN\n",
       " 14     [] 2011-01-15  NaN   NaN\n",
       " 15     [] 2011-01-16  NaN   NaN\n",
       " 16     [] 2011-01-17  NaN   NaN\n",
       " 17     [] 2011-01-18  NaN   NaN\n",
       " 18     [] 2011-01-19  NaN   NaN\n",
       " 19     [] 2011-01-20  NaN   NaN\n",
       " 20     [] 2011-01-21  NaN   NaN\n",
       " 21     [] 2011-01-22  NaN   NaN\n",
       " 22     [] 2011-01-23  NaN   NaN\n",
       " 23     [] 2011-01-24  NaN   NaN\n",
       " 24     [] 2011-01-25  NaN   NaN\n",
       " 25     [] 2011-01-26  NaN   NaN\n",
       " 26     [] 2011-01-27  NaN   NaN\n",
       " 27     [] 2011-01-28  NaN   NaN\n",
       " 28     [] 2011-01-29  NaN   NaN\n",
       " 29     [] 2011-01-30  NaN   NaN\n",
       " ..    ...        ...  ...   ...\n",
       " 335    [] 2011-12-02  NaN   NaN\n",
       " 336    [] 2011-12-03  NaN   NaN\n",
       " 337    [] 2011-12-04  NaN   NaN\n",
       " 338    [] 2011-12-05  NaN   NaN\n",
       " 339    [] 2011-12-06  NaN   NaN\n",
       " 340    [] 2011-12-07  NaN   NaN\n",
       " 341    [] 2011-12-08  NaN   NaN\n",
       " 342    [] 2011-12-09  NaN   NaN\n",
       " 343    [] 2011-12-10  NaN   NaN\n",
       " 344    [] 2011-12-11  NaN   NaN\n",
       " 345    [] 2011-12-12  NaN   NaN\n",
       " 346    [] 2011-12-13  NaN   NaN\n",
       " 347    [] 2011-12-14  NaN   NaN\n",
       " 348    [] 2011-12-15  NaN   NaN\n",
       " 349    [] 2011-12-16  NaN   NaN\n",
       " 350    [] 2011-12-17  NaN   NaN\n",
       " 351    [] 2011-12-18  NaN   NaN\n",
       " 352    [] 2011-12-19  NaN   NaN\n",
       " 353    [] 2011-12-20  NaN   NaN\n",
       " 354    [] 2011-12-21  NaN   NaN\n",
       " 355    [] 2011-12-22  NaN   NaN\n",
       " 356    [] 2011-12-23  NaN   NaN\n",
       " 357    [] 2011-12-24  NaN   NaN\n",
       " 358    [] 2011-12-25  NaN   NaN\n",
       " 359    [] 2011-12-26  NaN   NaN\n",
       " 360    [] 2011-12-27  NaN   NaN\n",
       " 361    [] 2011-12-28  NaN   NaN\n",
       " 362    [] 2011-12-29  NaN   NaN\n",
       " 363    [] 2011-12-30  NaN   NaN\n",
       " 364    [] 2011-12-31  NaN   NaN\n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'SYF_2012':     Tweet       Date  emot  mean\n",
       " 0      [] 2012-01-01   NaN   NaN\n",
       " 1      [] 2012-01-02   NaN   NaN\n",
       " 2      [] 2012-01-03   NaN   NaN\n",
       " 3      [] 2012-01-04   NaN   NaN\n",
       " 4      [] 2012-01-05   NaN   NaN\n",
       " 5      [] 2012-01-06   NaN   NaN\n",
       " 6      [] 2012-01-07   NaN   NaN\n",
       " 7      [] 2012-01-08   NaN   NaN\n",
       " 8      [] 2012-01-09   NaN   NaN\n",
       " 9      [] 2012-01-10   NaN   NaN\n",
       " 10     [] 2012-01-11   NaN   NaN\n",
       " 11     [] 2012-01-12   NaN   NaN\n",
       " 12     [] 2012-01-13   NaN   NaN\n",
       " 13     [] 2012-01-14   NaN   NaN\n",
       " 14     [] 2012-01-15   NaN   NaN\n",
       " 15     [] 2012-01-16   NaN   NaN\n",
       " 16     [] 2012-01-17   NaN   NaN\n",
       " 17     [] 2012-01-18   NaN   NaN\n",
       " 18     [] 2012-01-19   NaN   NaN\n",
       " 19     [] 2012-01-20   NaN   NaN\n",
       " 20     [] 2012-01-21   NaN   NaN\n",
       " 21     [] 2012-01-22   NaN   NaN\n",
       " 22     [] 2012-01-23   NaN   NaN\n",
       " 23     [] 2012-01-24   NaN   NaN\n",
       " 24     [] 2012-01-25   NaN   NaN\n",
       " 25     [] 2012-01-26   NaN   NaN\n",
       " 26     [] 2012-01-27   NaN   NaN\n",
       " 27     [] 2012-01-28   NaN   NaN\n",
       " 28     [] 2012-01-29   NaN   NaN\n",
       " 29     [] 2012-01-30   NaN   NaN\n",
       " ..    ...        ...   ...   ...\n",
       " 335    [] 2012-12-02   NaN   NaN\n",
       " 336    [] 2012-12-03   NaN   NaN\n",
       " 337    [] 2012-12-04   NaN   NaN\n",
       " 338    [] 2012-12-05   NaN   NaN\n",
       " 339    [] 2012-12-06   NaN   NaN\n",
       " 340    [] 2012-12-07   NaN   NaN\n",
       " 341    [] 2012-12-08   NaN   NaN\n",
       " 342    [] 2012-12-09   NaN   NaN\n",
       " 343    [] 2012-12-10   NaN   NaN\n",
       " 344    [] 2012-12-11   NaN   NaN\n",
       " 345    [] 2012-12-12   NaN   NaN\n",
       " 346    [] 2012-12-13   NaN   NaN\n",
       " 347    [] 2012-12-14   NaN   NaN\n",
       " 348    [] 2012-12-15   NaN   NaN\n",
       " 349    [] 2012-12-16   NaN   NaN\n",
       " 350    [] 2012-12-17   NaN   NaN\n",
       " 351    [] 2012-12-18   NaN   NaN\n",
       " 352    [] 2012-12-19   NaN   NaN\n",
       " 353    [] 2012-12-20   NaN   NaN\n",
       " 354    [] 2012-12-21   NaN   NaN\n",
       " 355    [] 2012-12-22   NaN   NaN\n",
       " 356    [] 2012-12-23   NaN   NaN\n",
       " 357    [] 2012-12-24   NaN   NaN\n",
       " 358    [] 2012-12-25   NaN   NaN\n",
       " 359    [] 2012-12-26   NaN   NaN\n",
       " 360    [] 2012-12-27   NaN   NaN\n",
       " 361    [] 2012-12-28   NaN   NaN\n",
       " 362    [] 2012-12-29   NaN   NaN\n",
       " 363    [] 2012-12-30   NaN   NaN\n",
       " 364    [] 2012-12-31   NaN   NaN\n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'SYF_2013':     Tweet       Date  emot  mean\n",
       " 0      [] 2013-01-01   NaN   NaN\n",
       " 1      [] 2013-01-02   NaN   NaN\n",
       " 2      [] 2013-01-03   NaN   NaN\n",
       " 3      [] 2013-01-04   NaN   NaN\n",
       " 4      [] 2013-01-05   NaN   NaN\n",
       " 5      [] 2013-01-06   NaN   NaN\n",
       " 6      [] 2013-01-07   NaN   NaN\n",
       " 7      [] 2013-01-08   NaN   NaN\n",
       " 8      [] 2013-01-09   NaN   NaN\n",
       " 9      [] 2013-01-10   NaN   NaN\n",
       " 10     [] 2013-01-11   NaN   NaN\n",
       " 11     [] 2013-01-12   NaN   NaN\n",
       " 12     [] 2013-01-13   NaN   NaN\n",
       " 13     [] 2013-01-14   NaN   NaN\n",
       " 14     [] 2013-01-15   NaN   NaN\n",
       " 15     [] 2013-01-16   NaN   NaN\n",
       " 16     [] 2013-01-17   NaN   NaN\n",
       " 17     [] 2013-01-18   NaN   NaN\n",
       " 18     [] 2013-01-19   NaN   NaN\n",
       " 19     [] 2013-01-20   NaN   NaN\n",
       " 20     [] 2013-01-21   NaN   NaN\n",
       " 21     [] 2013-01-22   NaN   NaN\n",
       " 22     [] 2013-01-23   NaN   NaN\n",
       " 23     [] 2013-01-24   NaN   NaN\n",
       " 24     [] 2013-01-25   NaN   NaN\n",
       " 25     [] 2013-01-26   NaN   NaN\n",
       " 26     [] 2013-01-27   NaN   NaN\n",
       " 27     [] 2013-01-28   NaN   NaN\n",
       " 28     [] 2013-01-29   NaN   NaN\n",
       " 29     [] 2013-01-30   NaN   NaN\n",
       " ..    ...        ...   ...   ...\n",
       " 335    [] 2013-12-02   NaN   NaN\n",
       " 336    [] 2013-12-03   NaN   NaN\n",
       " 337    [] 2013-12-04   NaN   NaN\n",
       " 338    [] 2013-12-05   NaN   NaN\n",
       " 339    [] 2013-12-06   NaN   NaN\n",
       " 340    [] 2013-12-07   NaN   NaN\n",
       " 341    [] 2013-12-08   NaN   NaN\n",
       " 342    [] 2013-12-09   NaN   NaN\n",
       " 343    [] 2013-12-10   NaN   NaN\n",
       " 344    [] 2013-12-11   NaN   NaN\n",
       " 345    [] 2013-12-12   NaN   NaN\n",
       " 346    [] 2013-12-13   NaN   NaN\n",
       " 347    [] 2013-12-14   NaN   NaN\n",
       " 348    [] 2013-12-15   NaN   NaN\n",
       " 349    [] 2013-12-16   NaN   NaN\n",
       " 350    [] 2013-12-17   NaN   NaN\n",
       " 351    [] 2013-12-18   NaN   NaN\n",
       " 352    [] 2013-12-19   NaN   NaN\n",
       " 353    [] 2013-12-20   NaN   NaN\n",
       " 354    [] 2013-12-21   NaN   NaN\n",
       " 355    [] 2013-12-22   NaN   NaN\n",
       " 356    [] 2013-12-23   NaN   NaN\n",
       " 357    [] 2013-12-24   NaN   NaN\n",
       " 358    [] 2013-12-25   NaN   NaN\n",
       " 359    [] 2013-12-26   NaN   NaN\n",
       " 360    [] 2013-12-27   NaN   NaN\n",
       " 361    [] 2013-12-28   NaN   NaN\n",
       " 362    [] 2013-12-29   NaN   NaN\n",
       " 363    [] 2013-12-30   NaN   NaN\n",
       " 364    [] 2013-12-31   NaN   NaN\n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'SYF_2014':                                                  Tweet       Date   emot  mean\n",
       " 0                                                   [] 2014-01-01    NaN   NaN\n",
       " 1                                                   [] 2014-01-02    NaN   NaN\n",
       " 2                                                   [] 2014-01-03    NaN   NaN\n",
       " 3                                                   [] 2014-01-04    NaN   NaN\n",
       " 4                                                   [] 2014-01-05    NaN   NaN\n",
       " 5                                                   [] 2014-01-06    NaN   NaN\n",
       " 6                                                   [] 2014-01-07    NaN   NaN\n",
       " 7                                                   [] 2014-01-08    NaN   NaN\n",
       " 8                                                   [] 2014-01-09    NaN   NaN\n",
       " 9                                                   [] 2014-01-10    NaN   NaN\n",
       " 10                                                  [] 2014-01-11    NaN   NaN\n",
       " 11                                                  [] 2014-01-12    NaN   NaN\n",
       " 12                                                  [] 2014-01-13    NaN   NaN\n",
       " 13                                                  [] 2014-01-14    NaN   NaN\n",
       " 14                                                  [] 2014-01-15    NaN   NaN\n",
       " 15                                                  [] 2014-01-16    NaN   NaN\n",
       " 16                                                  [] 2014-01-17    NaN   NaN\n",
       " 17                                                  [] 2014-01-18    NaN   NaN\n",
       " 18                                                  [] 2014-01-19    NaN   NaN\n",
       " 19                                                  [] 2014-01-20    NaN   NaN\n",
       " 20                                                  [] 2014-01-21    NaN   NaN\n",
       " 21                                                  [] 2014-01-22    NaN   NaN\n",
       " 22                                                  [] 2014-01-23    NaN   NaN\n",
       " 23                                                  [] 2014-01-24    NaN   NaN\n",
       " 24                                                  [] 2014-01-25    NaN   NaN\n",
       " 25                                                  [] 2014-01-26    NaN   NaN\n",
       " 26                                                  [] 2014-01-27    NaN   NaN\n",
       " 27                                                  [] 2014-01-28    NaN   NaN\n",
       " 28                                                  [] 2014-01-29    NaN   NaN\n",
       " 29                                                  [] 2014-01-30    NaN   NaN\n",
       " ..                                                 ...        ...    ...   ...\n",
       " 335  [synchrony syf should have highlighted it s pi... 2014-12-02  [[0]]   0.0\n",
       " 336                                                 [] 2014-12-03    NaN   NaN\n",
       " 337                                                 [] 2014-12-04    NaN   NaN\n",
       " 338                                                 [] 2014-12-05    NaN   NaN\n",
       " 339                                                 [] 2014-12-06    NaN   NaN\n",
       " 340                                                 [] 2014-12-07    NaN   NaN\n",
       " 341  [syf outperformed today and managed to hold th... 2014-12-08  [[0]]   0.0\n",
       " 342                                                 [] 2014-12-09    NaN   NaN\n",
       " 343                                                 [] 2014-12-10    NaN   NaN\n",
       " 344                                                 [] 2014-12-11    NaN   NaN\n",
       " 345                                                 [] 2014-12-12    NaN   NaN\n",
       " 346                                                 [] 2014-12-13    NaN   NaN\n",
       " 347                                                 [] 2014-12-14    NaN   NaN\n",
       " 348                                                 [] 2014-12-15    NaN   NaN\n",
       " 349                                                 [] 2014-12-16    NaN   NaN\n",
       " 350                                                 [] 2014-12-17    NaN   NaN\n",
       " 351  [renaissance capital s 2 14 us ipo annual revi... 2014-12-18  [[0]]   0.0\n",
       " 352                                                 [] 2014-12-19    NaN   NaN\n",
       " 353                                                 [] 2014-12-20    NaN   NaN\n",
       " 354                                                 [] 2014-12-21    NaN   NaN\n",
       " 355  [synchrony financial syf trading at new post i... 2014-12-22  [[0]]   0.0\n",
       " 356                                                 [] 2014-12-23    NaN   NaN\n",
       " 357                                                 [] 2014-12-24    NaN   NaN\n",
       " 358                                                 [] 2014-12-25    NaN   NaN\n",
       " 359                                                 [] 2014-12-26    NaN   NaN\n",
       " 360                                                 [] 2014-12-27    NaN   NaN\n",
       " 361                                                 [] 2014-12-28    NaN   NaN\n",
       " 362  [7 best performing ipos of record breaking 2 1... 2014-12-29  [[0]]   0.0\n",
       " 363  [peg mas glw alsn syf at the top of the watchl... 2014-12-30  [[0]]   0.0\n",
       " 364                                                 [] 2014-12-31    NaN   NaN\n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'SYF_2015':                                                  Tweet       Date  \\\n",
       " 0    [dividend investing at work december 2 14 url ... 2015-01-01   \n",
       " 1    [commented on general electric is ge stock a g... 2015-01-02   \n",
       " 2                                                   [] 2015-01-03   \n",
       " 3    [worst performing ipos of 2 14 cfg ndaq v rbs ... 2015-01-04   \n",
       " 4    [watchlist rsg awk wfc syf peg tss kim glw als... 2015-01-05   \n",
       " 5    [the only financial i would buy now is syf if ... 2015-01-06   \n",
       " 6    [fslr syf wall street analysts upgrades downgr... 2015-01-07   \n",
       " 7    [3 contrarian picks msi syf kn from dan kozlow... 2015-01-08   \n",
       " 8                                                   [] 2015-01-09   \n",
       " 9                                                   [] 2015-01-10   \n",
       " 10                                                  [] 2015-01-11   \n",
       " 11                                                  [] 2015-01-12   \n",
       " 12                                                  [] 2015-01-13   \n",
       " 13                                                  [] 2015-01-14   \n",
       " 14                                                  [] 2015-01-15   \n",
       " 15                                                  [] 2015-01-16   \n",
       " 16                                                  [] 2015-01-17   \n",
       " 17                                                  [] 2015-01-18   \n",
       " 18                                                  [] 2015-01-19   \n",
       " 19                                                  [] 2015-01-20   \n",
       " 20                                                  [] 2015-01-21   \n",
       " 21                                                  [] 2015-01-22   \n",
       " 22                                                  [] 2015-01-23   \n",
       " 23                                                  [] 2015-01-24   \n",
       " 24                                                  [] 2015-01-25   \n",
       " 25                                                  [] 2015-01-26   \n",
       " 26                                                  [] 2015-01-27   \n",
       " 27   [wednesday top analyst upgrades downgrades par... 2015-01-28   \n",
       " 28                                                  [] 2015-01-29   \n",
       " 29                                                  [] 2015-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 335  [sleep well companies from mlvex url cboe ahl ... 2015-12-02   \n",
       " 336                                                 [] 2015-12-03   \n",
       " 337                                                 [] 2015-12-04   \n",
       " 338                                                 [] 2015-12-05   \n",
       " 339                                                 [] 2015-12-06   \n",
       " 340  [xom spdr s p oil gas exploration production i... 2015-12-07   \n",
       " 341  [syf ready for selloff, block trade syf 7 shar... 2015-12-08   \n",
       " 342                    [user great interview syf user] 2015-12-09   \n",
       " 343                                                 [] 2015-12-10   \n",
       " 344                                                 [] 2015-12-11   \n",
       " 345  [the best way to play retail for growth c hd l... 2015-12-12   \n",
       " 346                                                 [] 2015-12-13   \n",
       " 347                                                 [] 2015-12-14   \n",
       " 348                                                 [] 2015-12-15   \n",
       " 349  [want the latest upgrades downgrades for rnr s... 2015-12-16   \n",
       " 350  [general electric 2 16 shaping up to be anothe... 2015-12-17   \n",
       " 351                                                 [] 2015-12-18   \n",
       " 352                                                 [] 2015-12-19   \n",
       " 353                                                 [] 2015-12-20   \n",
       " 354  [stocks is there life after fang url via user ... 2015-12-21   \n",
       " 355  [peer to peer lending is not a serious threat ... 2015-12-22   \n",
       " 356                                                 [] 2015-12-23   \n",
       " 357                                                 [] 2015-12-24   \n",
       " 358  [explore fundamental numbers and pros and cons... 2015-12-25   \n",
       " 359                                                 [] 2015-12-26   \n",
       " 360  [synchrony financial s buy rating reiterated a... 2015-12-27   \n",
       " 361  [syf analyst price target on synchrony financi... 2015-12-28   \n",
       " 362  [is general electric a safe haven at 31 url sy... 2015-12-29   \n",
       " 363                                                 [] 2015-12-30   \n",
       " 364                                                 [] 2015-12-31   \n",
       " \n",
       "                      emot  mean  \n",
       " 0                   [[1]]  1.00  \n",
       " 1                [[0, 1]]  0.50  \n",
       " 2                     NaN   NaN  \n",
       " 3                [[0, 0]]  0.00  \n",
       " 4                   [[1]]  1.00  \n",
       " 5                [[0, 0]]  0.00  \n",
       " 6    [[0, 0, 0, 0, 0, 0]]  0.00  \n",
       " 7                   [[0]]  0.00  \n",
       " 8                     NaN   NaN  \n",
       " 9                     NaN   NaN  \n",
       " 10                    NaN   NaN  \n",
       " 11                    NaN   NaN  \n",
       " 12                    NaN   NaN  \n",
       " 13                    NaN   NaN  \n",
       " 14                    NaN   NaN  \n",
       " 15                    NaN   NaN  \n",
       " 16                    NaN   NaN  \n",
       " 17                    NaN   NaN  \n",
       " 18                    NaN   NaN  \n",
       " 19                    NaN   NaN  \n",
       " 20                    NaN   NaN  \n",
       " 21                    NaN   NaN  \n",
       " 22                    NaN   NaN  \n",
       " 23                    NaN   NaN  \n",
       " 24                    NaN   NaN  \n",
       " 25                    NaN   NaN  \n",
       " 26                    NaN   NaN  \n",
       " 27                  [[0]]  0.00  \n",
       " 28                    NaN   NaN  \n",
       " 29                    NaN   NaN  \n",
       " ..                    ...   ...  \n",
       " 335                 [[0]]  0.00  \n",
       " 336                   NaN   NaN  \n",
       " 337                   NaN   NaN  \n",
       " 338                   NaN   NaN  \n",
       " 339                   NaN   NaN  \n",
       " 340                 [[1]]  1.00  \n",
       " 341        [[0, 0, 0, 0]]  0.00  \n",
       " 342                 [[0]]  0.00  \n",
       " 343                   NaN   NaN  \n",
       " 344                   NaN   NaN  \n",
       " 345                 [[0]]  0.00  \n",
       " 346                   NaN   NaN  \n",
       " 347                   NaN   NaN  \n",
       " 348                   NaN   NaN  \n",
       " 349                 [[1]]  1.00  \n",
       " 350                 [[0]]  0.00  \n",
       " 351                   NaN   NaN  \n",
       " 352                   NaN   NaN  \n",
       " 353                   NaN   NaN  \n",
       " 354        [[0, 0, 1, 0]]  0.25  \n",
       " 355                 [[1]]  1.00  \n",
       " 356                   NaN   NaN  \n",
       " 357                   NaN   NaN  \n",
       " 358                 [[0]]  0.00  \n",
       " 359                   NaN   NaN  \n",
       " 360              [[0, 0]]  0.00  \n",
       " 361                 [[0]]  0.00  \n",
       " 362                 [[0]]  0.00  \n",
       " 363                   NaN   NaN  \n",
       " 364                   NaN   NaN  \n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'SYF_2016':                                                  Tweet       Date  \\\n",
       " 0    [investment analysts recent ratings updates fo... 2016-01-01   \n",
       " 1                                                   [] 2016-01-02   \n",
       " 2    [bank branches are an albatros banking now don... 2016-01-03   \n",
       " 3                                                   [] 2016-01-04   \n",
       " 4                                                   [] 2016-01-05   \n",
       " 5                                                   [] 2016-01-06   \n",
       " 6                                                   [] 2016-01-07   \n",
       " 7                                                   [] 2016-01-08   \n",
       " 8                                                   [] 2016-01-09   \n",
       " 9                                                   [] 2016-01-10   \n",
       " 10                                                  [] 2016-01-11   \n",
       " 11                                                  [] 2016-01-12   \n",
       " 12                                                  [] 2016-01-13   \n",
       " 13                                                  [] 2016-01-14   \n",
       " 14                                                  [] 2016-01-15   \n",
       " 15                                                  [] 2016-01-16   \n",
       " 16                                                  [] 2016-01-17   \n",
       " 17                                                  [] 2016-01-18   \n",
       " 18                                                  [] 2016-01-19   \n",
       " 19                                                  [] 2016-01-20   \n",
       " 20                                                  [] 2016-01-21   \n",
       " 21                                                  [] 2016-01-22   \n",
       " 22                                                  [] 2016-01-23   \n",
       " 23                                                  [] 2016-01-24   \n",
       " 24                                                  [] 2016-01-25   \n",
       " 25                                                  [] 2016-01-26   \n",
       " 26                                                  [] 2016-01-27   \n",
       " 27                                                  [] 2016-01-28   \n",
       " 28                                                  [] 2016-01-29   \n",
       " 29                                                  [] 2016-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 335                     [a lot to like about syf user] 2016-12-02   \n",
       " 336                                                 [] 2016-12-03   \n",
       " 337                                                 [] 2016-12-04   \n",
       " 338                                                 [] 2016-12-05   \n",
       " 339  [buy any dip in bank stocks morgan stanley url... 2016-12-06   \n",
       " 340  [syf wedbush cuts to neutral, wednesday top an... 2016-12-07   \n",
       " 341  [dti trading results syf 18 3 laz 1 12 url tel... 2016-12-08   \n",
       " 342  [nomura advises selling american express url a... 2016-12-09   \n",
       " 343                                                 [] 2016-12-10   \n",
       " 344                                                 [] 2016-12-11   \n",
       " 345                                                 [] 2016-12-12   \n",
       " 346  [a preview of monthly credit card metrics for ... 2016-12-13   \n",
       " 347                                                 [] 2016-12-14   \n",
       " 348  [dfs syf ma v all hiked to buy at baml, unusua... 2016-12-15   \n",
       " 349  [syf mar 38 call buyer 5k for 1 6, syf been on... 2016-12-16   \n",
       " 350  [syf synchrony financial has fallen to a 2 sta... 2016-12-17   \n",
       " 351                                                 [] 2016-12-18   \n",
       " 352                                                 [] 2016-12-19   \n",
       " 353  [synchrony financial downgraded by vetr inc to... 2016-12-20   \n",
       " 354  [new all time highs jpm dow bbt hds awh wen sy... 2016-12-21   \n",
       " 355  [synchronyfinancial earns honors for diversity... 2016-12-22   \n",
       " 356                                                 [] 2016-12-23   \n",
       " 357                                                 [] 2016-12-24   \n",
       " 358                                                 [] 2016-12-25   \n",
       " 359  [synchrony financial syf stake increased by se... 2016-12-26   \n",
       " 360  [synchrony financial downgraded by zacks inves... 2016-12-27   \n",
       " 361  [syf big june 37 call buyers, sweeperbriefs 12... 2016-12-28   \n",
       " 362  [top university of chicago economist says watc... 2016-12-29   \n",
       " 363  [synchrony financial syf shares bought by midd... 2016-12-30   \n",
       " 364                                                 [] 2016-12-31   \n",
       " \n",
       "                                  emot      mean  \n",
       " 0                               [[0]]  0.000000  \n",
       " 1                                 NaN       NaN  \n",
       " 2                               [[0]]  0.000000  \n",
       " 3                                 NaN       NaN  \n",
       " 4                                 NaN       NaN  \n",
       " 5                                 NaN       NaN  \n",
       " 6                                 NaN       NaN  \n",
       " 7                                 NaN       NaN  \n",
       " 8                                 NaN       NaN  \n",
       " 9                                 NaN       NaN  \n",
       " 10                                NaN       NaN  \n",
       " 11                                NaN       NaN  \n",
       " 12                                NaN       NaN  \n",
       " 13                                NaN       NaN  \n",
       " 14                                NaN       NaN  \n",
       " 15                                NaN       NaN  \n",
       " 16                                NaN       NaN  \n",
       " 17                                NaN       NaN  \n",
       " 18                                NaN       NaN  \n",
       " 19                                NaN       NaN  \n",
       " 20                                NaN       NaN  \n",
       " 21                                NaN       NaN  \n",
       " 22                                NaN       NaN  \n",
       " 23                                NaN       NaN  \n",
       " 24                                NaN       NaN  \n",
       " 25                                NaN       NaN  \n",
       " 26                                NaN       NaN  \n",
       " 27                                NaN       NaN  \n",
       " 28                                NaN       NaN  \n",
       " 29                                NaN       NaN  \n",
       " ..                                ...       ...  \n",
       " 335                             [[0]]  0.000000  \n",
       " 336                               NaN       NaN  \n",
       " 337                               NaN       NaN  \n",
       " 338                               NaN       NaN  \n",
       " 339                          [[0, 0]]  0.000000  \n",
       " 340     [[1, 0, 0, 0, 0, 0, 1, 0, 0]]  0.222222  \n",
       " 341                             [[0]]  0.000000  \n",
       " 342                          [[0, 0]]  0.000000  \n",
       " 343                               NaN       NaN  \n",
       " 344                               NaN       NaN  \n",
       " 345                               NaN       NaN  \n",
       " 346                          [[0, 0]]  0.000000  \n",
       " 347                               NaN       NaN  \n",
       " 348  [[0, 0, 0, 0, 1, 0, 1, 0, 1, 1]]  0.400000  \n",
       " 349                 [[0, 1, 1, 0, 0]]  0.400000  \n",
       " 350                             [[0]]  0.000000  \n",
       " 351                               NaN       NaN  \n",
       " 352                               NaN       NaN  \n",
       " 353                             [[0]]  0.000000  \n",
       " 354                             [[0]]  0.000000  \n",
       " 355                             [[0]]  0.000000  \n",
       " 356                               NaN       NaN  \n",
       " 357                               NaN       NaN  \n",
       " 358                               NaN       NaN  \n",
       " 359                             [[0]]  0.000000  \n",
       " 360                             [[1]]  1.000000  \n",
       " 361                 [[0, 1, 0, 0, 0]]  0.200000  \n",
       " 362                       [[0, 1, 1]]  0.666667  \n",
       " 363              [[0, 0, 0, 0, 1, 1]]  0.333333  \n",
       " 364                               NaN       NaN  \n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'SYF_2017':                                                  Tweet       Date  \\\n",
       " 0    [conning inc boosts stake in synchrony financi... 2017-01-01   \n",
       " 1    [1 stocks with unusual options activity url vi... 2017-01-02   \n",
       " 2                                                   [] 2017-01-03   \n",
       " 3                                                   [] 2017-01-04   \n",
       " 4                                                   [] 2017-01-05   \n",
       " 5                                                   [] 2017-01-06   \n",
       " 6                                                   [] 2017-01-07   \n",
       " 7                                                   [] 2017-01-08   \n",
       " 8                                                   [] 2017-01-09   \n",
       " 9                                                   [] 2017-01-10   \n",
       " 10                                                  [] 2017-01-11   \n",
       " 11                                                  [] 2017-01-12   \n",
       " 12                                                  [] 2017-01-13   \n",
       " 13                                                  [] 2017-01-14   \n",
       " 14                                                  [] 2017-01-15   \n",
       " 15                                                  [] 2017-01-16   \n",
       " 16                                                  [] 2017-01-17   \n",
       " 17                                                  [] 2017-01-18   \n",
       " 18   [earnings before the open tomorrow ge slb rf p... 2017-01-19   \n",
       " 19   [top gainers 9 swks 11 uwt 6 cf 5 qrvo 4 pot a... 2017-01-20   \n",
       " 20                                                  [] 2017-01-21   \n",
       " 21                                                  [] 2017-01-22   \n",
       " 22                                                  [] 2017-01-23   \n",
       " 23                                                  [] 2017-01-24   \n",
       " 24                                                  [] 2017-01-25   \n",
       " 25                                                  [] 2017-01-26   \n",
       " 26                                                  [] 2017-01-27   \n",
       " 27                                                  [] 2017-01-28   \n",
       " 28                                                  [] 2017-01-29   \n",
       " 29                                                  [] 2017-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 335  [stocks that will benefit from the tax changes... 2017-12-02   \n",
       " 336                                                 [] 2017-12-03   \n",
       " 337            [block trade syf 5 shares 37 4 15 33 2] 2017-12-04   \n",
       " 338  [recent bond issue from synchrony financial sy... 2017-12-05   \n",
       " 339                                                 [] 2017-12-06   \n",
       " 340                                                 [] 2017-12-07   \n",
       " 341  [in investing value is in the eye of the behol... 2017-12-08   \n",
       " 342  [great article going to have to think more on ... 2017-12-09   \n",
       " 343                                                 [] 2017-12-10   \n",
       " 344                                                 [] 2017-12-11   \n",
       " 345  [oppenheimer asset management inc has 2 32 mil... 2017-12-12   \n",
       " 346                                                 [] 2017-12-13   \n",
       " 347                                                 [] 2017-12-14   \n",
       " 348                                                 [] 2017-12-15   \n",
       " 349  [week in review how trump s policies moved sto... 2017-12-16   \n",
       " 350                                                 [] 2017-12-17   \n",
       " 351  [upgrades lyb twtr vmw klac bld cof syf dfs co... 2017-12-18   \n",
       " 352  [chartsmarter tuesday game plan url seems like... 2017-12-19   \n",
       " 353                                                 [] 2017-12-20   \n",
       " 354                                                 [] 2017-12-21   \n",
       " 355                                                 [] 2017-12-22   \n",
       " 356                                                 [] 2017-12-23   \n",
       " 357                                                 [] 2017-12-24   \n",
       " 358                                                 [] 2017-12-25   \n",
       " 359                                                 [] 2017-12-26   \n",
       " 360                                                 [] 2017-12-27   \n",
       " 361  [amd vale m fcx nbr mon watt fcx avgo bac mcd ... 2017-12-28   \n",
       " 362  [12 29 high iv watch list syf infy kbh stz ung... 2017-12-29   \n",
       " 363                                                 [] 2017-12-30   \n",
       " 364                                                 [] 2017-12-31   \n",
       " \n",
       "                      emot      mean  \n",
       " 0                [[0, 0]]  0.000000  \n",
       " 1    [[0, 0, 0, 0, 1, 0]]  0.166667  \n",
       " 2                     NaN       NaN  \n",
       " 3                     NaN       NaN  \n",
       " 4                     NaN       NaN  \n",
       " 5                     NaN       NaN  \n",
       " 6                     NaN       NaN  \n",
       " 7                     NaN       NaN  \n",
       " 8                     NaN       NaN  \n",
       " 9                     NaN       NaN  \n",
       " 10                    NaN       NaN  \n",
       " 11                    NaN       NaN  \n",
       " 12                    NaN       NaN  \n",
       " 13                    NaN       NaN  \n",
       " 14                    NaN       NaN  \n",
       " 15                    NaN       NaN  \n",
       " 16                    NaN       NaN  \n",
       " 17                    NaN       NaN  \n",
       " 18                  [[0]]  0.000000  \n",
       " 19                  [[0]]  0.000000  \n",
       " 20                    NaN       NaN  \n",
       " 21                    NaN       NaN  \n",
       " 22                    NaN       NaN  \n",
       " 23                    NaN       NaN  \n",
       " 24                    NaN       NaN  \n",
       " 25                    NaN       NaN  \n",
       " 26                    NaN       NaN  \n",
       " 27                    NaN       NaN  \n",
       " 28                    NaN       NaN  \n",
       " 29                    NaN       NaN  \n",
       " ..                    ...       ...  \n",
       " 335                 [[1]]  1.000000  \n",
       " 336                   NaN       NaN  \n",
       " 337                 [[0]]  0.000000  \n",
       " 338                 [[1]]  1.000000  \n",
       " 339                   NaN       NaN  \n",
       " 340                   NaN       NaN  \n",
       " 341                 [[1]]  1.000000  \n",
       " 342                 [[0]]  0.000000  \n",
       " 343                   NaN       NaN  \n",
       " 344                   NaN       NaN  \n",
       " 345                 [[1]]  1.000000  \n",
       " 346                   NaN       NaN  \n",
       " 347                   NaN       NaN  \n",
       " 348                   NaN       NaN  \n",
       " 349                 [[1]]  1.000000  \n",
       " 350                   NaN       NaN  \n",
       " 351  [[1, 0, 1, 0, 1, 0]]  0.500000  \n",
       " 352              [[0, 1]]  0.500000  \n",
       " 353                   NaN       NaN  \n",
       " 354                   NaN       NaN  \n",
       " 355                   NaN       NaN  \n",
       " 356                   NaN       NaN  \n",
       " 357                   NaN       NaN  \n",
       " 358                   NaN       NaN  \n",
       " 359                   NaN       NaN  \n",
       " 360                   NaN       NaN  \n",
       " 361              [[1, 1]]  1.000000  \n",
       " 362                 [[0]]  0.000000  \n",
       " 363                   NaN       NaN  \n",
       " 364                   NaN       NaN  \n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'SYF_2018':                                                  Tweet       Date  \\\n",
       " 0    [i highly recommend checking this out for disc... 2018-01-01   \n",
       " 1    [syf synchrony financial syf pt raised to 46 a... 2018-01-02   \n",
       " 2    [must read our top pick ended 2 17 at a 52 wee... 2018-01-03   \n",
       " 3    [syf synchrony financial to announce fourth qu... 2018-01-04   \n",
       " 4                                                   [] 2018-01-05   \n",
       " 5                                                   [] 2018-01-06   \n",
       " 6    [synchrony financial syf has little margin of ... 2018-01-07   \n",
       " 7                                                   [] 2018-01-08   \n",
       " 8    [2 warren buffett stocks to buy in january use... 2018-01-09   \n",
       " 9    [syf 39 puts buying activity expiring on 19th ... 2018-01-10   \n",
       " 10   [my fave stocks for this year syf adding more ... 2018-01-11   \n",
       " 11   [wedbush raises synchrony financial fy2 18 ear... 2018-01-12   \n",
       " 12   [most anticipated earnings releases for the we... 2018-01-13   \n",
       " 13                                                  [] 2018-01-14   \n",
       " 14   [earnings this week tue c unh csx ibkr ozrk we... 2018-01-15   \n",
       " 15                                                  [] 2018-01-16   \n",
       " 16                                                  [] 2018-01-17   \n",
       " 17   [here s who reports earnings over the next 24 ... 2018-01-18   \n",
       " 18   [insideout brent slava user kevin hincks morni... 2018-01-19   \n",
       " 19                                                  [] 2018-01-20   \n",
       " 20                                                  [] 2018-01-21   \n",
       " 21                                                  [] 2018-01-22   \n",
       " 22   [let s see if cof ir and mgmt can top the 15 r... 2018-01-23   \n",
       " 23                                                  [] 2018-01-24   \n",
       " 24                                                  [] 2018-01-25   \n",
       " 25                                                  [] 2018-01-26   \n",
       " 26                                                  [] 2018-01-27   \n",
       " 27                                                  [] 2018-01-28   \n",
       " 28                                                  [] 2018-01-29   \n",
       " 29                                                  [] 2018-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 336                                                 [] 2018-12-03   \n",
       " 337                                                 [] 2018-12-04   \n",
       " 338                                                 [] 2018-12-05   \n",
       " 339  [looks very different to this from url almost ... 2018-12-06   \n",
       " 340  [syf shareholder alert pomerantz law firm inve... 2018-12-07   \n",
       " 341  [synchrony financial syf expected to announce ... 2018-12-08   \n",
       " 342  [synchrony financial syf expected to announce ... 2018-12-09   \n",
       " 343  [ba fdx duk vfc tpr bbt usb syf bll acn sq ant... 2018-12-10   \n",
       " 344  [made 2 k today thanks to the option trades fr... 2018-12-11   \n",
       " 345  [should you avoid rok url col syf ibkr ctl, ur... 2018-12-12   \n",
       " 346  [made 2 k today thanks to option trade alerts ... 2018-12-13   \n",
       " 347  [made 2 k today thanks to the option trade ale... 2018-12-14   \n",
       " 348  [value portfolio update stock shares strong bu... 2018-12-15   \n",
       " 349                                                 [] 2018-12-16   \n",
       " 350  [3 stocks value investing legend seth klarman ... 2018-12-17   \n",
       " 351  [4 bruised stocks at bargain prices set to rec... 2018-12-18   \n",
       " 352  [syf synchrony financial berkshire hathaway in... 2018-12-19   \n",
       " 353  [news out on this 1 9 million float pten nwl t... 2018-12-20   \n",
       " 354  [recap 12 21 unusual calls syf cs cag czr pcg ... 2018-12-21   \n",
       " 355  [weekend review video 12 23 url vix spy iwm qq... 2018-12-22   \n",
       " 356                                                 [] 2018-12-23   \n",
       " 357  [this is not what i wanted to wake up to but i... 2018-12-24   \n",
       " 358  [unusual call buying fang 15x average volume c... 2018-12-25   \n",
       " 359                                                 [] 2018-12-26   \n",
       " 360                                    [my hys at syf] 2018-12-27   \n",
       " 361  [synchrony finl syf shares declined while baup... 2018-12-28   \n",
       " 362  [syf option volume was 3x normal on friday wit... 2018-12-29   \n",
       " 363                                                 [] 2018-12-30   \n",
       " 364                                                 [] 2018-12-31   \n",
       " 365  [2 17 12 29 short volume percent for syf is 45... 2017-12-31   \n",
       " \n",
       "                                  emot      mean  \n",
       " 0    [[0, 1, 1, 1, 0, 0, 0, 1, 1, 0]]  0.500000  \n",
       " 1                         [[1, 0, 0]]  0.333333  \n",
       " 2                         [[0, 0, 0]]  0.000000  \n",
       " 3                            [[0, 0]]  0.000000  \n",
       " 4                                 NaN       NaN  \n",
       " 5                                 NaN       NaN  \n",
       " 6                               [[1]]  1.000000  \n",
       " 7                                 NaN       NaN  \n",
       " 8                            [[0, 0]]  0.000000  \n",
       " 9                            [[0, 0]]  0.000000  \n",
       " 10                           [[0, 0]]  0.000000  \n",
       " 11                        [[0, 0, 0]]  0.000000  \n",
       " 12                              [[0]]  0.000000  \n",
       " 13                                NaN       NaN  \n",
       " 14                              [[1]]  1.000000  \n",
       " 15                                NaN       NaN  \n",
       " 16                                NaN       NaN  \n",
       " 17                        [[1, 1, 0]]  0.666667  \n",
       " 18                  [[0, 1, 0, 0, 1]]  0.400000  \n",
       " 19                                NaN       NaN  \n",
       " 20                                NaN       NaN  \n",
       " 21                                NaN       NaN  \n",
       " 22                              [[0]]  0.000000  \n",
       " 23                                NaN       NaN  \n",
       " 24                                NaN       NaN  \n",
       " 25                                NaN       NaN  \n",
       " 26                                NaN       NaN  \n",
       " 27                                NaN       NaN  \n",
       " 28                                NaN       NaN  \n",
       " 29                                NaN       NaN  \n",
       " ..                                ...       ...  \n",
       " 336                               NaN       NaN  \n",
       " 337                               NaN       NaN  \n",
       " 338                               NaN       NaN  \n",
       " 339                          [[0, 1]]  0.500000  \n",
       " 340                       [[0, 0, 0]]  0.000000  \n",
       " 341                             [[0]]  0.000000  \n",
       " 342                          [[0, 0]]  0.000000  \n",
       " 343                 [[0, 1, 1, 0, 1]]  0.600000  \n",
       " 344                          [[0, 1]]  0.500000  \n",
       " 345                          [[0, 0]]  0.000000  \n",
       " 346                             [[0]]  0.000000  \n",
       " 347                          [[0, 0]]  0.000000  \n",
       " 348                             [[1]]  1.000000  \n",
       " 349                               NaN       NaN  \n",
       " 350                          [[0, 0]]  0.000000  \n",
       " 351                             [[0]]  0.000000  \n",
       " 352                       [[0, 0, 1]]  0.333333  \n",
       " 353                             [[0]]  0.000000  \n",
       " 354                          [[0, 0]]  0.000000  \n",
       " 355                             [[0]]  0.000000  \n",
       " 356                               NaN       NaN  \n",
       " 357                       [[1, 0, 1]]  0.666667  \n",
       " 358                             [[0]]  0.000000  \n",
       " 359                               NaN       NaN  \n",
       " 360                             [[1]]  1.000000  \n",
       " 361                             [[1]]  1.000000  \n",
       " 362                             [[0]]  0.000000  \n",
       " 363                               NaN       NaN  \n",
       " 364                               NaN       NaN  \n",
       " 365                             [[1]]  1.000000  \n",
       " \n",
       " [366 rows x 4 columns],\n",
       " 'BAYRY_2009':     Tweet       Date emot  mean\n",
       " 0      [] 2009-01-01  NaN   NaN\n",
       " 1      [] 2009-01-02  NaN   NaN\n",
       " 2      [] 2009-01-03  NaN   NaN\n",
       " 3      [] 2009-01-04  NaN   NaN\n",
       " 4      [] 2009-01-05  NaN   NaN\n",
       " 5      [] 2009-01-06  NaN   NaN\n",
       " 6      [] 2009-01-07  NaN   NaN\n",
       " 7      [] 2009-01-08  NaN   NaN\n",
       " 8      [] 2009-01-09  NaN   NaN\n",
       " 9      [] 2009-01-10  NaN   NaN\n",
       " 10     [] 2009-01-11  NaN   NaN\n",
       " 11     [] 2009-01-12  NaN   NaN\n",
       " 12     [] 2009-01-13  NaN   NaN\n",
       " 13     [] 2009-01-14  NaN   NaN\n",
       " 14     [] 2009-01-15  NaN   NaN\n",
       " 15     [] 2009-01-16  NaN   NaN\n",
       " 16     [] 2009-01-17  NaN   NaN\n",
       " 17     [] 2009-01-18  NaN   NaN\n",
       " 18     [] 2009-01-19  NaN   NaN\n",
       " 19     [] 2009-01-20  NaN   NaN\n",
       " 20     [] 2009-01-21  NaN   NaN\n",
       " 21     [] 2009-01-22  NaN   NaN\n",
       " 22     [] 2009-01-23  NaN   NaN\n",
       " 23     [] 2009-01-24  NaN   NaN\n",
       " 24     [] 2009-01-25  NaN   NaN\n",
       " 25     [] 2009-01-26  NaN   NaN\n",
       " 26     [] 2009-01-27  NaN   NaN\n",
       " 27     [] 2009-01-28  NaN   NaN\n",
       " 28     [] 2009-01-29  NaN   NaN\n",
       " 29     [] 2009-01-30  NaN   NaN\n",
       " ..    ...        ...  ...   ...\n",
       " 335    [] 2009-12-02  NaN   NaN\n",
       " 336    [] 2009-12-03  NaN   NaN\n",
       " 337    [] 2009-12-04  NaN   NaN\n",
       " 338    [] 2009-12-05  NaN   NaN\n",
       " 339    [] 2009-12-06  NaN   NaN\n",
       " 340    [] 2009-12-07  NaN   NaN\n",
       " 341    [] 2009-12-08  NaN   NaN\n",
       " 342    [] 2009-12-09  NaN   NaN\n",
       " 343    [] 2009-12-10  NaN   NaN\n",
       " 344    [] 2009-12-11  NaN   NaN\n",
       " 345    [] 2009-12-12  NaN   NaN\n",
       " 346    [] 2009-12-13  NaN   NaN\n",
       " 347    [] 2009-12-14  NaN   NaN\n",
       " 348    [] 2009-12-15  NaN   NaN\n",
       " 349    [] 2009-12-16  NaN   NaN\n",
       " 350    [] 2009-12-17  NaN   NaN\n",
       " 351    [] 2009-12-18  NaN   NaN\n",
       " 352    [] 2009-12-19  NaN   NaN\n",
       " 353    [] 2009-12-20  NaN   NaN\n",
       " 354    [] 2009-12-21  NaN   NaN\n",
       " 355    [] 2009-12-22  NaN   NaN\n",
       " 356    [] 2009-12-23  NaN   NaN\n",
       " 357    [] 2009-12-24  NaN   NaN\n",
       " 358    [] 2009-12-25  NaN   NaN\n",
       " 359    [] 2009-12-26  NaN   NaN\n",
       " 360    [] 2009-12-27  NaN   NaN\n",
       " 361    [] 2009-12-28  NaN   NaN\n",
       " 362    [] 2009-12-29  NaN   NaN\n",
       " 363    [] 2009-12-30  NaN   NaN\n",
       " 364    [] 2009-12-31  NaN   NaN\n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'BAYRY_2010':                                                  Tweet       Date   emot  mean\n",
       " 0                                                   [] 2010-01-01    NaN   NaN\n",
       " 1                                                   [] 2010-01-02    NaN   NaN\n",
       " 2                                                   [] 2010-01-03    NaN   NaN\n",
       " 3                                                   [] 2010-01-04    NaN   NaN\n",
       " 4                                                   [] 2010-01-05    NaN   NaN\n",
       " 5                                                   [] 2010-01-06    NaN   NaN\n",
       " 6                                                   [] 2010-01-07    NaN   NaN\n",
       " 7                                                   [] 2010-01-08    NaN   NaN\n",
       " 8                                                   [] 2010-01-09    NaN   NaN\n",
       " 9                                                   [] 2010-01-10    NaN   NaN\n",
       " 10                                                  [] 2010-01-11    NaN   NaN\n",
       " 11                                                  [] 2010-01-12    NaN   NaN\n",
       " 12                                                  [] 2010-01-13    NaN   NaN\n",
       " 13                                                  [] 2010-01-14    NaN   NaN\n",
       " 14                                                  [] 2010-01-15    NaN   NaN\n",
       " 15                                                  [] 2010-01-16    NaN   NaN\n",
       " 16                                                  [] 2010-01-17    NaN   NaN\n",
       " 17                                                  [] 2010-01-18    NaN   NaN\n",
       " 18                                                  [] 2010-01-19    NaN   NaN\n",
       " 19                                                  [] 2010-01-20    NaN   NaN\n",
       " 20                                                  [] 2010-01-21    NaN   NaN\n",
       " 21                                                  [] 2010-01-22    NaN   NaN\n",
       " 22                                                  [] 2010-01-23    NaN   NaN\n",
       " 23                                                  [] 2010-01-24    NaN   NaN\n",
       " 24                                                  [] 2010-01-25    NaN   NaN\n",
       " 25                                                  [] 2010-01-26    NaN   NaN\n",
       " 26                                                  [] 2010-01-27    NaN   NaN\n",
       " 27                                                  [] 2010-01-28    NaN   NaN\n",
       " 28                                                  [] 2010-01-29    NaN   NaN\n",
       " 29                                                  [] 2010-01-30    NaN   NaN\n",
       " ..                                                 ...        ...    ...   ...\n",
       " 335                                                 [] 2010-12-02    NaN   NaN\n",
       " 336                                                 [] 2010-12-03    NaN   NaN\n",
       " 337                                                 [] 2010-12-04    NaN   NaN\n",
       " 338                                                 [] 2010-12-05    NaN   NaN\n",
       " 339                                                 [] 2010-12-06    NaN   NaN\n",
       " 340  [credit suisse shuffles pharma sector ratings ... 2010-12-07  [[0]]   0.0\n",
       " 341                                                 [] 2010-12-08    NaN   NaN\n",
       " 342                                                 [] 2010-12-09    NaN   NaN\n",
       " 343                                                 [] 2010-12-10    NaN   NaN\n",
       " 344                                                 [] 2010-12-11    NaN   NaN\n",
       " 345                                                 [] 2010-12-12    NaN   NaN\n",
       " 346                                                 [] 2010-12-13    NaN   NaN\n",
       " 347                                                 [] 2010-12-14    NaN   NaN\n",
       " 348                                                 [] 2010-12-15    NaN   NaN\n",
       " 349                                                 [] 2010-12-16    NaN   NaN\n",
       " 350                                                 [] 2010-12-17    NaN   NaN\n",
       " 351                                                 [] 2010-12-18    NaN   NaN\n",
       " 352                                                 [] 2010-12-19    NaN   NaN\n",
       " 353                                                 [] 2010-12-20    NaN   NaN\n",
       " 354                                                 [] 2010-12-21    NaN   NaN\n",
       " 355                                                 [] 2010-12-22    NaN   NaN\n",
       " 356                                                 [] 2010-12-23    NaN   NaN\n",
       " 357                                                 [] 2010-12-24    NaN   NaN\n",
       " 358                                                 [] 2010-12-25    NaN   NaN\n",
       " 359                                                 [] 2010-12-26    NaN   NaN\n",
       " 360                                                 [] 2010-12-27    NaN   NaN\n",
       " 361                                                 [] 2010-12-28    NaN   NaN\n",
       " 362                                                 [] 2010-12-29    NaN   NaN\n",
       " 363                                                 [] 2010-12-30    NaN   NaN\n",
       " 364                                                 [] 2010-12-31    NaN   NaN\n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'BAYRY_2011':                                                  Tweet       Date   emot  mean\n",
       " 0                                                   [] 2011-01-01    NaN   NaN\n",
       " 1                                                   [] 2011-01-02    NaN   NaN\n",
       " 2                                                   [] 2011-01-03    NaN   NaN\n",
       " 3                                                   [] 2011-01-04    NaN   NaN\n",
       " 4                                                   [] 2011-01-05    NaN   NaN\n",
       " 5                                                   [] 2011-01-06    NaN   NaN\n",
       " 6                                                   [] 2011-01-07    NaN   NaN\n",
       " 7                                                   [] 2011-01-08    NaN   NaN\n",
       " 8                                                   [] 2011-01-09    NaN   NaN\n",
       " 9                                                   [] 2011-01-10    NaN   NaN\n",
       " 10                                                  [] 2011-01-11    NaN   NaN\n",
       " 11                                                  [] 2011-01-12    NaN   NaN\n",
       " 12   [rt user just one stock the heathcare giant wi... 2011-01-13  [[0]]   0.0\n",
       " 13                                                  [] 2011-01-14    NaN   NaN\n",
       " 14                                                  [] 2011-01-15    NaN   NaN\n",
       " 15                                                  [] 2011-01-16    NaN   NaN\n",
       " 16                                                  [] 2011-01-17    NaN   NaN\n",
       " 17                                                  [] 2011-01-18    NaN   NaN\n",
       " 18                                                  [] 2011-01-19    NaN   NaN\n",
       " 19                                                  [] 2011-01-20    NaN   NaN\n",
       " 20                                                  [] 2011-01-21    NaN   NaN\n",
       " 21                                                  [] 2011-01-22    NaN   NaN\n",
       " 22                                                  [] 2011-01-23    NaN   NaN\n",
       " 23                                                  [] 2011-01-24    NaN   NaN\n",
       " 24                                                  [] 2011-01-25    NaN   NaN\n",
       " 25                                                  [] 2011-01-26    NaN   NaN\n",
       " 26                                                  [] 2011-01-27    NaN   NaN\n",
       " 27                                                  [] 2011-01-28    NaN   NaN\n",
       " 28                                                  [] 2011-01-29    NaN   NaN\n",
       " 29                                                  [] 2011-01-30    NaN   NaN\n",
       " ..                                                 ...        ...    ...   ...\n",
       " 335                                                 [] 2011-12-02    NaN   NaN\n",
       " 336                                                 [] 2011-12-03    NaN   NaN\n",
       " 337                                                 [] 2011-12-04    NaN   NaN\n",
       " 338                                                 [] 2011-12-05    NaN   NaN\n",
       " 339                                                 [] 2011-12-06    NaN   NaN\n",
       " 340                                                 [] 2011-12-07    NaN   NaN\n",
       " 341                                                 [] 2011-12-08    NaN   NaN\n",
       " 342                                                 [] 2011-12-09    NaN   NaN\n",
       " 343                                                 [] 2011-12-10    NaN   NaN\n",
       " 344                                                 [] 2011-12-11    NaN   NaN\n",
       " 345                                                 [] 2011-12-12    NaN   NaN\n",
       " 346                                                 [] 2011-12-13    NaN   NaN\n",
       " 347                                                 [] 2011-12-14    NaN   NaN\n",
       " 348                                                 [] 2011-12-15    NaN   NaN\n",
       " 349                                                 [] 2011-12-16    NaN   NaN\n",
       " 350                                                 [] 2011-12-17    NaN   NaN\n",
       " 351                                                 [] 2011-12-18    NaN   NaN\n",
       " 352  [bayry bayer ag bayry shares upgraded to a neu... 2011-12-19  [[0]]   0.0\n",
       " 353                                                 [] 2011-12-20    NaN   NaN\n",
       " 354                                                 [] 2011-12-21    NaN   NaN\n",
       " 355                                                 [] 2011-12-22    NaN   NaN\n",
       " 356                                                 [] 2011-12-23    NaN   NaN\n",
       " 357                                                 [] 2011-12-24    NaN   NaN\n",
       " 358                                                 [] 2011-12-25    NaN   NaN\n",
       " 359                                                 [] 2011-12-26    NaN   NaN\n",
       " 360                                                 [] 2011-12-27    NaN   NaN\n",
       " 361                                                 [] 2011-12-28    NaN   NaN\n",
       " 362                                                 [] 2011-12-29    NaN   NaN\n",
       " 363                                                 [] 2011-12-30    NaN   NaN\n",
       " 364                                                 [] 2011-12-31    NaN   NaN\n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'BAYRY_2012':                                                  Tweet       Date   emot  mean\n",
       " 0                                                   [] 2012-01-01    NaN   NaN\n",
       " 1                                                   [] 2012-01-02    NaN   NaN\n",
       " 2                                                   [] 2012-01-03    NaN   NaN\n",
       " 3                                                   [] 2012-01-04    NaN   NaN\n",
       " 4                                                   [] 2012-01-05    NaN   NaN\n",
       " 5                                                   [] 2012-01-06    NaN   NaN\n",
       " 6                                                   [] 2012-01-07    NaN   NaN\n",
       " 7                                                   [] 2012-01-08    NaN   NaN\n",
       " 8                                                   [] 2012-01-09    NaN   NaN\n",
       " 9                                                   [] 2012-01-10    NaN   NaN\n",
       " 10                                                  [] 2012-01-11    NaN   NaN\n",
       " 11                                                  [] 2012-01-12    NaN   NaN\n",
       " 12                                                  [] 2012-01-13    NaN   NaN\n",
       " 13                                                  [] 2012-01-14    NaN   NaN\n",
       " 14                                                  [] 2012-01-15    NaN   NaN\n",
       " 15                                                  [] 2012-01-16    NaN   NaN\n",
       " 16                                                  [] 2012-01-17    NaN   NaN\n",
       " 17                                                  [] 2012-01-18    NaN   NaN\n",
       " 18                                                  [] 2012-01-19    NaN   NaN\n",
       " 19                                                  [] 2012-01-20    NaN   NaN\n",
       " 20                                                  [] 2012-01-21    NaN   NaN\n",
       " 21                                                  [] 2012-01-22    NaN   NaN\n",
       " 22                                                  [] 2012-01-23    NaN   NaN\n",
       " 23                                                  [] 2012-01-24    NaN   NaN\n",
       " 24   [bayry otcbb stock analysis online discount br... 2012-01-25  [[1]]   1.0\n",
       " 25                                                  [] 2012-01-26    NaN   NaN\n",
       " 26                                                  [] 2012-01-27    NaN   NaN\n",
       " 27                                                  [] 2012-01-28    NaN   NaN\n",
       " 28                                                  [] 2012-01-29    NaN   NaN\n",
       " 29                                                  [] 2012-01-30    NaN   NaN\n",
       " ..                                                 ...        ...    ...   ...\n",
       " 335                                                 [] 2012-12-02    NaN   NaN\n",
       " 336  [german ewg open all up bayry db leading the w... 2012-12-03  [[0]]   0.0\n",
       " 337                                                 [] 2012-12-04    NaN   NaN\n",
       " 338                                                 [] 2012-12-05    NaN   NaN\n",
       " 339                                                 [] 2012-12-06    NaN   NaN\n",
       " 340                                                 [] 2012-12-07    NaN   NaN\n",
       " 341                                                 [] 2012-12-08    NaN   NaN\n",
       " 342                                                 [] 2012-12-09    NaN   NaN\n",
       " 343                                                 [] 2012-12-10    NaN   NaN\n",
       " 344                                                 [] 2012-12-11    NaN   NaN\n",
       " 345                                                 [] 2012-12-12    NaN   NaN\n",
       " 346                                                 [] 2012-12-13    NaN   NaN\n",
       " 347  [bayry bayer ag submits new drug application f... 2012-12-14  [[0]]   0.0\n",
       " 348                                                 [] 2012-12-15    NaN   NaN\n",
       " 349                                                 [] 2012-12-16    NaN   NaN\n",
       " 350                                                 [] 2012-12-17    NaN   NaN\n",
       " 351  [tried to type in ebay and typed in bay which ... 2012-12-18  [[1]]   1.0\n",
       " 352  [bayry bayer approaching 1 sh as per randy s f... 2012-12-19  [[1]]   1.0\n",
       " 353                                                 [] 2012-12-20    NaN   NaN\n",
       " 354                                                 [] 2012-12-21    NaN   NaN\n",
       " 355                                                 [] 2012-12-22    NaN   NaN\n",
       " 356                                                 [] 2012-12-23    NaN   NaN\n",
       " 357  [bayry bayer ag downgraded to neutral at zacks... 2012-12-24  [[0]]   0.0\n",
       " 358                                                 [] 2012-12-25    NaN   NaN\n",
       " 359                                                 [] 2012-12-26    NaN   NaN\n",
       " 360                                                 [] 2012-12-27    NaN   NaN\n",
       " 361                                                 [] 2012-12-28    NaN   NaN\n",
       " 362                                                 [] 2012-12-29    NaN   NaN\n",
       " 363                                                 [] 2012-12-30    NaN   NaN\n",
       " 364                                                 [] 2012-12-31    NaN   NaN\n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'BAYRY_2013':                                                  Tweet       Date  \\\n",
       " 0                                                   [] 2013-01-01   \n",
       " 1                                                   [] 2013-01-02   \n",
       " 2                                                   [] 2013-01-03   \n",
       " 3    [bayer acquisition of teva s us animal health ... 2013-01-04   \n",
       " 4                                                   [] 2013-01-05   \n",
       " 5                                                   [] 2013-01-06   \n",
       " 6                                                   [] 2013-01-07   \n",
       " 7                                                   [] 2013-01-08   \n",
       " 8                                                   [] 2013-01-09   \n",
       " 9                                                   [] 2013-01-10   \n",
       " 10   [to be clear re earlier tweet on pfe bayry les... 2013-01-11   \n",
       " 11                                                  [] 2013-01-12   \n",
       " 12                                                  [] 2013-01-13   \n",
       " 13   [bayer stock surge in 2 12 gives way to pfizer... 2013-01-14   \n",
       " 14                                                  [] 2013-01-15   \n",
       " 15                                                  [] 2013-01-16   \n",
       " 16                                                  [] 2013-01-17   \n",
       " 17                                                  [] 2013-01-18   \n",
       " 18                                                  [] 2013-01-19   \n",
       " 19                                                  [] 2013-01-20   \n",
       " 20                                                  [] 2013-01-21   \n",
       " 21                                                  [] 2013-01-22   \n",
       " 22                                                  [] 2013-01-23   \n",
       " 23            [user bayry is kind of pricey right now] 2013-01-24   \n",
       " 24                                                  [] 2013-01-25   \n",
       " 25                                                  [] 2013-01-26   \n",
       " 26   [pharma giant bayer ag popped on friday boa re... 2013-01-27   \n",
       " 27                                                  [] 2013-01-28   \n",
       " 28                                                  [] 2013-01-29   \n",
       " 29                                                  [] 2013-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 335                                                 [] 2013-12-02   \n",
       " 336  [rt user bayer s dekkers says they didn t deve... 2013-12-03   \n",
       " 337  [novartis veterinary unit said to draw interes... 2013-12-04   \n",
       " 338  [bayry oncomed progresses with oncology candid... 2013-12-05   \n",
       " 339  [bayry another r d failure for eli lilly url s... 2013-12-06   \n",
       " 340                                                 [] 2013-12-07   \n",
       " 341                                                 [] 2013-12-08   \n",
       " 342                                                 [] 2013-12-09   \n",
       " 343                                                 [] 2013-12-10   \n",
       " 344  [why pfizer s expiring viagra patent hurts lil... 2013-12-11   \n",
       " 345                                                 [] 2013-12-12   \n",
       " 346                                                 [] 2013-12-13   \n",
       " 347  [bayry mad catz uses bayer for video game tech... 2013-12-14   \n",
       " 348                                                 [] 2013-12-15   \n",
       " 349  [evgn mon dd bayry evogene slides after piper ... 2013-12-16   \n",
       " 350                                                 [] 2013-12-17   \n",
       " 351  [bayry kemal malik appointed bayer ag board of... 2013-12-18   \n",
       " 352  [bayry bayer ag plans to acquire norwegian pha... 2013-12-19   \n",
       " 353                                                 [] 2013-12-20   \n",
       " 354  [bayry bayer looks to nuke cancer url stock st... 2013-12-21   \n",
       " 355                                                 [] 2013-12-22   \n",
       " 356  [new ed drug analysis of vivus s prospects url... 2013-12-23   \n",
       " 357                                                 [] 2013-12-24   \n",
       " 358                                                 [] 2013-12-25   \n",
       " 359                                                 [] 2013-12-26   \n",
       " 360  [bayry woah heavy volume today, bayry bayer lo... 2013-12-27   \n",
       " 361  [bayry high vol 414 511 lt bullish current 142... 2013-12-28   \n",
       " 362                                                 [] 2013-12-29   \n",
       " 363  [bayer ag receives neutral rating from zacks b... 2013-12-30   \n",
       " 364                                                 [] 2013-12-31   \n",
       " \n",
       "                            emot      mean  \n",
       " 0                           NaN       NaN  \n",
       " 1                           NaN       NaN  \n",
       " 2                           NaN       NaN  \n",
       " 3                         [[0]]  0.000000  \n",
       " 4                           NaN       NaN  \n",
       " 5                           NaN       NaN  \n",
       " 6                           NaN       NaN  \n",
       " 7                           NaN       NaN  \n",
       " 8                           NaN       NaN  \n",
       " 9                           NaN       NaN  \n",
       " 10                     [[1, 0]]  0.500000  \n",
       " 11                          NaN       NaN  \n",
       " 12                          NaN       NaN  \n",
       " 13                        [[0]]  0.000000  \n",
       " 14                          NaN       NaN  \n",
       " 15                          NaN       NaN  \n",
       " 16                          NaN       NaN  \n",
       " 17                          NaN       NaN  \n",
       " 18                          NaN       NaN  \n",
       " 19                          NaN       NaN  \n",
       " 20                          NaN       NaN  \n",
       " 21                          NaN       NaN  \n",
       " 22                          NaN       NaN  \n",
       " 23                        [[1]]  1.000000  \n",
       " 24                          NaN       NaN  \n",
       " 25                          NaN       NaN  \n",
       " 26                        [[0]]  0.000000  \n",
       " 27                          NaN       NaN  \n",
       " 28                          NaN       NaN  \n",
       " 29                          NaN       NaN  \n",
       " ..                          ...       ...  \n",
       " 335                         NaN       NaN  \n",
       " 336  [[1, 0, 1, 0, 0, 0, 0, 0]]  0.250000  \n",
       " 337                       [[0]]  0.000000  \n",
       " 338                    [[0, 1]]  0.500000  \n",
       " 339                       [[1]]  1.000000  \n",
       " 340                         NaN       NaN  \n",
       " 341                         NaN       NaN  \n",
       " 342                         NaN       NaN  \n",
       " 343                         NaN       NaN  \n",
       " 344                       [[1]]  1.000000  \n",
       " 345                         NaN       NaN  \n",
       " 346                         NaN       NaN  \n",
       " 347                       [[0]]  0.000000  \n",
       " 348                         NaN       NaN  \n",
       " 349                    [[0, 0]]  0.000000  \n",
       " 350                         NaN       NaN  \n",
       " 351                    [[0, 0]]  0.000000  \n",
       " 352              [[0, 0, 0, 0]]  0.000000  \n",
       " 353                         NaN       NaN  \n",
       " 354                       [[1]]  1.000000  \n",
       " 355                         NaN       NaN  \n",
       " 356              [[1, 0, 0, 0]]  0.250000  \n",
       " 357                         NaN       NaN  \n",
       " 358                         NaN       NaN  \n",
       " 359                         NaN       NaN  \n",
       " 360        [[1, 1, 0, 0, 0, 0]]  0.333333  \n",
       " 361                       [[1]]  1.000000  \n",
       " 362                         NaN       NaN  \n",
       " 363                       [[0]]  0.000000  \n",
       " 364                         NaN       NaN  \n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'BAYRY_2014':                                                  Tweet       Date  \\\n",
       " 0    [bayry what you need to know about blood thinn... 2014-01-01   \n",
       " 1    [german authority can t determine efficacy sup... 2014-01-02   \n",
       " 2    [bayry bright prospects at bayer url stock sto... 2014-01-03   \n",
       " 3                                                   [] 2014-01-04   \n",
       " 4                                                   [] 2014-01-05   \n",
       " 5    [bayry a busy week for cardiac health at the f... 2014-01-06   \n",
       " 6                                                   [] 2014-01-07   \n",
       " 7    [new ed drug analysis of vivus s prospects url... 2014-01-08   \n",
       " 8    [bayry are multivitamins a waste of money url ... 2014-01-09   \n",
       " 9                                                   [] 2014-01-10   \n",
       " 10   [bayry bayer ag adr gets regulatory approval f... 2014-01-11   \n",
       " 11                                                  [] 2014-01-12   \n",
       " 12                                                  [] 2014-01-13   \n",
       " 13   [bayry why regeneron pharmaceuticals inc share... 2014-01-14   \n",
       " 14   [jpm14 day 2 report now available for download... 2014-01-15   \n",
       " 15   [top 2 13 drug approvals pharma stocks url gil... 2014-01-16   \n",
       " 16                                                  [] 2014-01-17   \n",
       " 17                                                  [] 2014-01-18   \n",
       " 18                                                  [] 2014-01-19   \n",
       " 19                                                  [] 2014-01-20   \n",
       " 20                                                  [] 2014-01-21   \n",
       " 21                                                  [] 2014-01-22   \n",
       " 22                                                  [] 2014-01-23   \n",
       " 23                                                  [] 2014-01-24   \n",
       " 24                                                  [] 2014-01-25   \n",
       " 25                                                  [] 2014-01-26   \n",
       " 26                                                  [] 2014-01-27   \n",
       " 27                                                  [] 2014-01-28   \n",
       " 28                                                  [] 2014-01-29   \n",
       " 29   [user whoops we may have done it again fda wro... 2014-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 335                                                 [] 2014-12-02   \n",
       " 336                                                 [] 2014-12-03   \n",
       " 337                                                 [] 2014-12-04   \n",
       " 338  [johnson johnson bayer against consolidation o... 2014-12-05   \n",
       " 339                                                 [] 2014-12-06   \n",
       " 340                                                 [] 2014-12-07   \n",
       " 341                                                 [] 2014-12-08   \n",
       " 342                                                 [] 2014-12-09   \n",
       " 343                                                 [] 2014-12-10   \n",
       " 344                                                 [] 2014-12-11   \n",
       " 345                                                 [] 2014-12-12   \n",
       " 346                                                 [] 2014-12-13   \n",
       " 347                                                 [] 2014-12-14   \n",
       " 348  [bayer ag s patent defeat raises questions on ... 2014-12-15   \n",
       " 349                                                 [] 2014-12-16   \n",
       " 350                                                 [] 2014-12-17   \n",
       " 351  [most active otcpink stocks vol m bayry 115 9 ... 2014-12-18   \n",
       " 352                                                 [] 2014-12-19   \n",
       " 353                                                 [] 2014-12-20   \n",
       " 354                                                 [] 2014-12-21   \n",
       " 355                                                 [] 2014-12-22   \n",
       " 356                                                 [] 2014-12-23   \n",
       " 357                                                 [] 2014-12-24   \n",
       " 358                                                 [] 2014-12-25   \n",
       " 359                                                 [] 2014-12-26   \n",
       " 360                                                 [] 2014-12-27   \n",
       " 361                                                 [] 2014-12-28   \n",
       " 362                                                 [] 2014-12-29   \n",
       " 363                                                 [] 2014-12-30   \n",
       " 364                                                 [] 2014-12-31   \n",
       " \n",
       "                emot      mean  \n",
       " 0          [[1, 0]]  0.500000  \n",
       " 1          [[0, 0]]  0.000000  \n",
       " 2          [[1, 1]]  1.000000  \n",
       " 3               NaN       NaN  \n",
       " 4               NaN       NaN  \n",
       " 5             [[1]]  1.000000  \n",
       " 6               NaN       NaN  \n",
       " 7             [[1]]  1.000000  \n",
       " 8       [[1, 0, 1]]  0.666667  \n",
       " 9               NaN       NaN  \n",
       " 10            [[0]]  0.000000  \n",
       " 11              NaN       NaN  \n",
       " 12              NaN       NaN  \n",
       " 13   [[1, 0, 1, 0]]  0.500000  \n",
       " 14            [[0]]  0.000000  \n",
       " 15            [[0]]  0.000000  \n",
       " 16              NaN       NaN  \n",
       " 17              NaN       NaN  \n",
       " 18              NaN       NaN  \n",
       " 19              NaN       NaN  \n",
       " 20              NaN       NaN  \n",
       " 21              NaN       NaN  \n",
       " 22              NaN       NaN  \n",
       " 23              NaN       NaN  \n",
       " 24              NaN       NaN  \n",
       " 25              NaN       NaN  \n",
       " 26              NaN       NaN  \n",
       " 27              NaN       NaN  \n",
       " 28              NaN       NaN  \n",
       " 29            [[1]]  1.000000  \n",
       " ..              ...       ...  \n",
       " 335             NaN       NaN  \n",
       " 336             NaN       NaN  \n",
       " 337             NaN       NaN  \n",
       " 338           [[0]]  0.000000  \n",
       " 339             NaN       NaN  \n",
       " 340             NaN       NaN  \n",
       " 341             NaN       NaN  \n",
       " 342             NaN       NaN  \n",
       " 343             NaN       NaN  \n",
       " 344             NaN       NaN  \n",
       " 345             NaN       NaN  \n",
       " 346             NaN       NaN  \n",
       " 347             NaN       NaN  \n",
       " 348           [[0]]  0.000000  \n",
       " 349             NaN       NaN  \n",
       " 350             NaN       NaN  \n",
       " 351        [[0, 0]]  0.000000  \n",
       " 352             NaN       NaN  \n",
       " 353             NaN       NaN  \n",
       " 354             NaN       NaN  \n",
       " 355             NaN       NaN  \n",
       " 356             NaN       NaN  \n",
       " 357             NaN       NaN  \n",
       " 358             NaN       NaN  \n",
       " 359             NaN       NaN  \n",
       " 360             NaN       NaN  \n",
       " 361             NaN       NaN  \n",
       " 362             NaN       NaN  \n",
       " 363             NaN       NaN  \n",
       " 364             NaN       NaN  \n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'BAYRY_2015':                                                  Tweet       Date  \\\n",
       " 0                                                   [] 2015-01-01   \n",
       " 1    [bayer bayry is poised to benefit from a weak ... 2015-01-02   \n",
       " 2                                                   [] 2015-01-03   \n",
       " 3                                                   [] 2015-01-04   \n",
       " 4    [fda ok s bayry gadavist gadobutrol as 1st mri... 2015-01-05   \n",
       " 5    [opportunities in falling euro knife url addyy... 2015-01-06   \n",
       " 6                                                   [] 2015-01-07   \n",
       " 7                                                   [] 2015-01-08   \n",
       " 8                                                   [] 2015-01-09   \n",
       " 9                                                   [] 2015-01-10   \n",
       " 10   [don t wait untill 2 16 to realize this powerf... 2015-01-11   \n",
       " 11                                                  [] 2015-01-12   \n",
       " 12                                                  [] 2015-01-13   \n",
       " 13                                                  [] 2015-01-14   \n",
       " 14                                                  [] 2015-01-15   \n",
       " 15   [bayer ag adr s underweight rating reiterated ... 2015-01-16   \n",
       " 16                                                  [] 2015-01-17   \n",
       " 17                                                  [] 2015-01-18   \n",
       " 18                                                  [] 2015-01-19   \n",
       " 19                                                  [] 2015-01-20   \n",
       " 20                                                  [] 2015-01-21   \n",
       " 21                                                  [] 2015-01-22   \n",
       " 22                                                  [] 2015-01-23   \n",
       " 23                                                  [] 2015-01-24   \n",
       " 24   [asmvf message board posts click here url also... 2015-01-25   \n",
       " 25                                                  [] 2015-01-26   \n",
       " 26   [chartguy89 bayry 1 75 stock charts bayry baye... 2015-01-27   \n",
       " 27                                                  [] 2015-01-28   \n",
       " 28                                                  [] 2015-01-29   \n",
       " 29   [bayer ag receives consensus recommendation of... 2015-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 335  [bayer ag downgraded by citigroup inc to neutr... 2015-12-02   \n",
       " 336                                                 [] 2015-12-03   \n",
       " 337                                                 [] 2015-12-04   \n",
       " 338                                                 [] 2015-12-05   \n",
       " 339                                                 [] 2015-12-06   \n",
       " 340                                                 [] 2015-12-07   \n",
       " 341                                                 [] 2015-12-08   \n",
       " 342                                                 [] 2015-12-09   \n",
       " 343  [notable analyst upgrades 12 1 azsey bayry cyb... 2015-12-10   \n",
       " 344  [compugen earns 7 8m milestone from bayer for ... 2015-12-11   \n",
       " 345                                                 [] 2015-12-12   \n",
       " 346  [farmers worry about dow chemical dupont merge... 2015-12-13   \n",
       " 347                                                 [] 2015-12-14   \n",
       " 348                                                 [] 2015-12-15   \n",
       " 349  [array is sitting on a billion dollar melanoma... 2015-12-16   \n",
       " 350                                                 [] 2015-12-17   \n",
       " 351  [report bayer is setting its eyes on a big acq... 2015-12-18   \n",
       " 352                                                 [] 2015-12-19   \n",
       " 353  [bayry invests 3 mn to establish gene editing ... 2015-12-20   \n",
       " 354  [bayry bayer crispr therapeutics joint venture... 2015-12-21   \n",
       " 355  [bayry gives more than 3 million to crispr gen... 2015-12-22   \n",
       " 356                                                 [] 2015-12-23   \n",
       " 357                                                 [] 2015-12-24   \n",
       " 358                                                 [] 2015-12-25   \n",
       " 359                                                 [] 2015-12-26   \n",
       " 360                                                 [] 2015-12-27   \n",
       " 361                                                 [] 2015-12-28   \n",
       " 362                                                 [] 2015-12-29   \n",
       " 363                                                 [] 2015-12-30   \n",
       " 364                                                 [] 2015-12-31   \n",
       " \n",
       "             emot  mean  \n",
       " 0            NaN   NaN  \n",
       " 1    [[0, 0, 0]]   0.0  \n",
       " 2            NaN   NaN  \n",
       " 3            NaN   NaN  \n",
       " 4          [[1]]   1.0  \n",
       " 5          [[0]]   0.0  \n",
       " 6            NaN   NaN  \n",
       " 7            NaN   NaN  \n",
       " 8            NaN   NaN  \n",
       " 9            NaN   NaN  \n",
       " 10         [[0]]   0.0  \n",
       " 11           NaN   NaN  \n",
       " 12           NaN   NaN  \n",
       " 13           NaN   NaN  \n",
       " 14           NaN   NaN  \n",
       " 15         [[0]]   0.0  \n",
       " 16           NaN   NaN  \n",
       " 17           NaN   NaN  \n",
       " 18           NaN   NaN  \n",
       " 19           NaN   NaN  \n",
       " 20           NaN   NaN  \n",
       " 21           NaN   NaN  \n",
       " 22           NaN   NaN  \n",
       " 23           NaN   NaN  \n",
       " 24         [[0]]   0.0  \n",
       " 25           NaN   NaN  \n",
       " 26         [[0]]   0.0  \n",
       " 27           NaN   NaN  \n",
       " 28           NaN   NaN  \n",
       " 29         [[0]]   0.0  \n",
       " ..           ...   ...  \n",
       " 335     [[0, 0]]   0.0  \n",
       " 336          NaN   NaN  \n",
       " 337          NaN   NaN  \n",
       " 338          NaN   NaN  \n",
       " 339          NaN   NaN  \n",
       " 340          NaN   NaN  \n",
       " 341          NaN   NaN  \n",
       " 342          NaN   NaN  \n",
       " 343     [[1, 0]]   0.5  \n",
       " 344        [[0]]   0.0  \n",
       " 345          NaN   NaN  \n",
       " 346        [[0]]   0.0  \n",
       " 347          NaN   NaN  \n",
       " 348          NaN   NaN  \n",
       " 349        [[0]]   0.0  \n",
       " 350          NaN   NaN  \n",
       " 351        [[0]]   0.0  \n",
       " 352          NaN   NaN  \n",
       " 353        [[0]]   0.0  \n",
       " 354        [[0]]   0.0  \n",
       " 355     [[0, 0]]   0.0  \n",
       " 356          NaN   NaN  \n",
       " 357          NaN   NaN  \n",
       " 358          NaN   NaN  \n",
       " 359          NaN   NaN  \n",
       " 360          NaN   NaN  \n",
       " 361          NaN   NaN  \n",
       " 362          NaN   NaN  \n",
       " 363          NaN   NaN  \n",
       " 364          NaN   NaN  \n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'BAYRY_2016':                                                  Tweet       Date  \\\n",
       " 0                                                   [] 2016-01-01   \n",
       " 1                                                   [] 2016-01-02   \n",
       " 2                                                   [] 2016-01-03   \n",
       " 3    [most active otcpink stocks vol m bayry 82 3 n... 2016-01-04   \n",
       " 4    [oncomed pharma bags 72 5m in milestones from ... 2016-01-05   \n",
       " 5                                                   [] 2016-01-06   \n",
       " 6                                                   [] 2016-01-07   \n",
       " 7                                                   [] 2016-01-08   \n",
       " 8                                                   [] 2016-01-09   \n",
       " 9                                                   [] 2016-01-10   \n",
       " 10                                                  [] 2016-01-11   \n",
       " 11                                                  [] 2016-01-12   \n",
       " 12                                                  [] 2016-01-13   \n",
       " 13                                                  [] 2016-01-14   \n",
       " 14                                                  [] 2016-01-15   \n",
       " 15                                                  [] 2016-01-16   \n",
       " 16                                                  [] 2016-01-17   \n",
       " 17                                                  [] 2016-01-18   \n",
       " 18                                                  [] 2016-01-19   \n",
       " 19                                                  [] 2016-01-20   \n",
       " 20                                                  [] 2016-01-21   \n",
       " 21                                                  [] 2016-01-22   \n",
       " 22                                                  [] 2016-01-23   \n",
       " 23                                                  [] 2016-01-24   \n",
       " 24                                                  [] 2016-01-25   \n",
       " 25                                                  [] 2016-01-26   \n",
       " 26                                                  [] 2016-01-27   \n",
       " 27                                                  [] 2016-01-28   \n",
       " 28                                                  [] 2016-01-29   \n",
       " 29                                                  [] 2016-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 335                                                 [] 2016-12-02   \n",
       " 336                                                 [] 2016-12-03   \n",
       " 337                                                 [] 2016-12-04   \n",
       " 338                                                 [] 2016-12-05   \n",
       " 339                                                 [] 2016-12-06   \n",
       " 340                                                 [] 2016-12-07   \n",
       " 341  [former bayer exec sues the drug maker over al... 2016-12-08   \n",
       " 342                                                 [] 2016-12-09   \n",
       " 343                                                 [] 2016-12-10   \n",
       " 344                                                 [] 2016-12-11   \n",
       " 345                                                 [] 2016-12-12   \n",
       " 346  [top picks for the new year url ancx bayry boa... 2016-12-13   \n",
       " 347                                                 [] 2016-12-14   \n",
       " 348                                                 [] 2016-12-15   \n",
       " 349  [precision aml trial adds pharma candidates to... 2016-12-16   \n",
       " 350                                                 [] 2016-12-17   \n",
       " 351                                                 [] 2016-12-18   \n",
       " 352                                                 [] 2016-12-19   \n",
       " 353                                                 [] 2016-12-20   \n",
       " 354                                                 [] 2016-12-21   \n",
       " 355  [genetically modified seeds seem ineffective a... 2016-12-22   \n",
       " 356                                                 [] 2016-12-23   \n",
       " 357                                                 [] 2016-12-24   \n",
       " 358                                                 [] 2016-12-25   \n",
       " 359                                                 [] 2016-12-26   \n",
       " 360                                                 [] 2016-12-27   \n",
       " 361  [ftc requires divestitures as condition to pro... 2016-12-28   \n",
       " 362  [bayry is up 1 in a straight line since einhor... 2016-12-29   \n",
       " 363                                                 [] 2016-12-30   \n",
       " 364                                                 [] 2016-12-31   \n",
       " \n",
       "                emot      mean  \n",
       " 0               NaN       NaN  \n",
       " 1               NaN       NaN  \n",
       " 2               NaN       NaN  \n",
       " 3    [[0, 0, 0, 0]]  0.000000  \n",
       " 4    [[0, 1, 0, 1]]  0.500000  \n",
       " 5               NaN       NaN  \n",
       " 6               NaN       NaN  \n",
       " 7               NaN       NaN  \n",
       " 8               NaN       NaN  \n",
       " 9               NaN       NaN  \n",
       " 10              NaN       NaN  \n",
       " 11              NaN       NaN  \n",
       " 12              NaN       NaN  \n",
       " 13              NaN       NaN  \n",
       " 14              NaN       NaN  \n",
       " 15              NaN       NaN  \n",
       " 16              NaN       NaN  \n",
       " 17              NaN       NaN  \n",
       " 18              NaN       NaN  \n",
       " 19              NaN       NaN  \n",
       " 20              NaN       NaN  \n",
       " 21              NaN       NaN  \n",
       " 22              NaN       NaN  \n",
       " 23              NaN       NaN  \n",
       " 24              NaN       NaN  \n",
       " 25              NaN       NaN  \n",
       " 26              NaN       NaN  \n",
       " 27              NaN       NaN  \n",
       " 28              NaN       NaN  \n",
       " 29              NaN       NaN  \n",
       " ..              ...       ...  \n",
       " 335             NaN       NaN  \n",
       " 336             NaN       NaN  \n",
       " 337             NaN       NaN  \n",
       " 338             NaN       NaN  \n",
       " 339             NaN       NaN  \n",
       " 340             NaN       NaN  \n",
       " 341           [[0]]  0.000000  \n",
       " 342             NaN       NaN  \n",
       " 343             NaN       NaN  \n",
       " 344             NaN       NaN  \n",
       " 345             NaN       NaN  \n",
       " 346           [[0]]  0.000000  \n",
       " 347             NaN       NaN  \n",
       " 348             NaN       NaN  \n",
       " 349           [[0]]  0.000000  \n",
       " 350             NaN       NaN  \n",
       " 351             NaN       NaN  \n",
       " 352             NaN       NaN  \n",
       " 353             NaN       NaN  \n",
       " 354             NaN       NaN  \n",
       " 355           [[0]]  0.000000  \n",
       " 356             NaN       NaN  \n",
       " 357             NaN       NaN  \n",
       " 358             NaN       NaN  \n",
       " 359             NaN       NaN  \n",
       " 360             NaN       NaN  \n",
       " 361           [[1]]  1.000000  \n",
       " 362     [[0, 0, 1]]  0.333333  \n",
       " 363             NaN       NaN  \n",
       " 364             NaN       NaN  \n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'BAYRY_2017':                                                  Tweet       Date  \\\n",
       " 0                                                   [] 2017-01-01   \n",
       " 1    [monsanto mon q1 earnings what s in store for ... 2017-01-02   \n",
       " 2    [don t worry it s about to get worse url abt a... 2017-01-03   \n",
       " 3    [2 star analyst richard vosser from j p morgan... 2017-01-04   \n",
       " 4    [most active otcpink stocks vol m nsrgy 35 2 g... 2017-01-05   \n",
       " 5    [last chart on scans may be fav find bayry bay... 2017-01-06   \n",
       " 6                                                   [] 2017-01-07   \n",
       " 7                                                   [] 2017-01-08   \n",
       " 8                                                   [] 2017-01-09   \n",
       " 9                                                   [] 2017-01-10   \n",
       " 10                                                  [] 2017-01-11   \n",
       " 11                                                  [] 2017-01-12   \n",
       " 12                                                  [] 2017-01-13   \n",
       " 13                                                  [] 2017-01-14   \n",
       " 14                                                  [] 2017-01-15   \n",
       " 15                                                  [] 2017-01-16   \n",
       " 16   [confirming our report user says after ceo of ... 2017-01-17   \n",
       " 17                                                  [] 2017-01-18   \n",
       " 18                                                  [] 2017-01-19   \n",
       " 19                                                  [] 2017-01-20   \n",
       " 20                                                  [] 2017-01-21   \n",
       " 21                                                  [] 2017-01-22   \n",
       " 22                                                  [] 2017-01-23   \n",
       " 23                                                  [] 2017-01-24   \n",
       " 24                                                  [] 2017-01-25   \n",
       " 25   [bayry out of downtrend and large base into vo... 2017-01-26   \n",
       " 26                                                  [] 2017-01-27   \n",
       " 27                                                  [] 2017-01-28   \n",
       " 28                                                  [] 2017-01-29   \n",
       " 29                                                  [] 2017-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 335                                                 [] 2017-12-02   \n",
       " 336                                                 [] 2017-12-03   \n",
       " 337                                                 [] 2017-12-04   \n",
       " 338  [bayer s big ideas for star loxo assets url ba... 2017-12-05   \n",
       " 339                                                 [] 2017-12-06   \n",
       " 340                                                 [] 2017-12-07   \n",
       " 341                                                 [] 2017-12-08   \n",
       " 342                                                 [] 2017-12-09   \n",
       " 343                                                 [] 2017-12-10   \n",
       " 344  [must read how to make money investing in bitc... 2017-12-11   \n",
       " 345  [round up on monsanto for 2 18 14 billion is n... 2017-12-12   \n",
       " 346                                                 [] 2017-12-13   \n",
       " 347                                                 [] 2017-12-14   \n",
       " 348  [most active otcpink stocks vol m tcehy 151 3 ... 2017-12-15   \n",
       " 349                                                 [] 2017-12-16   \n",
       " 350  [why exelixis rebound is just getting started ... 2017-12-17   \n",
       " 351                                                 [] 2017-12-18   \n",
       " 352                                                 [] 2017-12-19   \n",
       " 353  [heading into 2 18 with cautious optimism url ... 2017-12-20   \n",
       " 354                                                 [] 2017-12-21   \n",
       " 355                                                 [] 2017-12-22   \n",
       " 356                                                 [] 2017-12-23   \n",
       " 357                                                 [] 2017-12-24   \n",
       " 358                                                 [] 2017-12-25   \n",
       " 359                                                 [] 2017-12-26   \n",
       " 360                                                 [] 2017-12-27   \n",
       " 361                                                 [] 2017-12-28   \n",
       " 362                                                 [] 2017-12-29   \n",
       " 363                                                 [] 2017-12-30   \n",
       " 364                                                 [] 2017-12-31   \n",
       " \n",
       "                      emot      mean  \n",
       " 0                     NaN       NaN  \n",
       " 1                [[0, 0]]  0.000000  \n",
       " 2             [[1, 0, 0]]  0.333333  \n",
       " 3    [[0, 0, 0, 0, 1, 0]]  0.166667  \n",
       " 4                   [[0]]  0.000000  \n",
       " 5                   [[1]]  1.000000  \n",
       " 6                     NaN       NaN  \n",
       " 7                     NaN       NaN  \n",
       " 8                     NaN       NaN  \n",
       " 9                     NaN       NaN  \n",
       " 10                    NaN       NaN  \n",
       " 11                    NaN       NaN  \n",
       " 12                    NaN       NaN  \n",
       " 13                    NaN       NaN  \n",
       " 14                    NaN       NaN  \n",
       " 15                    NaN       NaN  \n",
       " 16                  [[0]]  0.000000  \n",
       " 17                    NaN       NaN  \n",
       " 18                    NaN       NaN  \n",
       " 19                    NaN       NaN  \n",
       " 20                    NaN       NaN  \n",
       " 21                    NaN       NaN  \n",
       " 22                    NaN       NaN  \n",
       " 23                    NaN       NaN  \n",
       " 24                    NaN       NaN  \n",
       " 25                  [[1]]  1.000000  \n",
       " 26                    NaN       NaN  \n",
       " 27                    NaN       NaN  \n",
       " 28                    NaN       NaN  \n",
       " 29                    NaN       NaN  \n",
       " ..                    ...       ...  \n",
       " 335                   NaN       NaN  \n",
       " 336                   NaN       NaN  \n",
       " 337                   NaN       NaN  \n",
       " 338                 [[0]]  0.000000  \n",
       " 339                   NaN       NaN  \n",
       " 340                   NaN       NaN  \n",
       " 341                   NaN       NaN  \n",
       " 342                   NaN       NaN  \n",
       " 343                   NaN       NaN  \n",
       " 344                 [[0]]  0.000000  \n",
       " 345                 [[0]]  0.000000  \n",
       " 346                   NaN       NaN  \n",
       " 347                   NaN       NaN  \n",
       " 348              [[0, 0]]  0.000000  \n",
       " 349                   NaN       NaN  \n",
       " 350                 [[0]]  0.000000  \n",
       " 351                   NaN       NaN  \n",
       " 352                   NaN       NaN  \n",
       " 353              [[0, 0]]  0.000000  \n",
       " 354                   NaN       NaN  \n",
       " 355                   NaN       NaN  \n",
       " 356                   NaN       NaN  \n",
       " 357                   NaN       NaN  \n",
       " 358                   NaN       NaN  \n",
       " 359                   NaN       NaN  \n",
       " 360                   NaN       NaN  \n",
       " 361                   NaN       NaN  \n",
       " 362                   NaN       NaN  \n",
       " 363                   NaN       NaN  \n",
       " 364                   NaN       NaN  \n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'BAYRY_2018':                                                  Tweet       Date  \\\n",
       " 0    [must read this stock ended 2 17 at a 52 week ... 2018-01-01   \n",
       " 1    [must read our top pick ended 2 17 at a 52 wee... 2018-01-02   \n",
       " 2    [slowly but steadily big pharma is chipping aw... 2018-01-03   \n",
       " 3                                                   [] 2018-01-04   \n",
       " 4                                                   [] 2018-01-05   \n",
       " 5                                                   [] 2018-01-06   \n",
       " 6                                                   [] 2018-01-07   \n",
       " 7                                                   [] 2018-01-08   \n",
       " 8                                                   [] 2018-01-09   \n",
       " 9                                                   [] 2018-01-10   \n",
       " 10                                                  [] 2018-01-11   \n",
       " 11                                                  [] 2018-01-12   \n",
       " 12                                                  [] 2018-01-13   \n",
       " 13                                                  [] 2018-01-14   \n",
       " 14                                                  [] 2018-01-15   \n",
       " 15                                                  [] 2018-01-16   \n",
       " 16                                                  [] 2018-01-17   \n",
       " 17                                                  [] 2018-01-18   \n",
       " 18                                                  [] 2018-01-19   \n",
       " 19                                                  [] 2018-01-20   \n",
       " 20                                                  [] 2018-01-21   \n",
       " 21                                                  [] 2018-01-22   \n",
       " 22                                                  [] 2018-01-23   \n",
       " 23                                                  [] 2018-01-24   \n",
       " 24                                                  [] 2018-01-25   \n",
       " 25   [at last night s davos dinner trump said user ... 2018-01-26   \n",
       " 26                                                  [] 2018-01-27   \n",
       " 27                                                  [] 2018-01-28   \n",
       " 28                                                  [] 2018-01-29   \n",
       " 29                                                  [] 2018-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 335  [bayry asml mon cost cutting, bayer suffering ... 2018-12-02   \n",
       " 336  [peo industry revenue is expected to grow an a... 2018-12-03   \n",
       " 337                                                 [] 2018-12-04   \n",
       " 338  [time accumulate share trep long tevvf tcehy t... 2018-12-05   \n",
       " 339  [bayry 2 14b block 17 9 don t like the sound o... 2018-12-06   \n",
       " 340  [spy qqq iwm vix dia cron mo avgo bayry mrna a... 2018-12-07   \n",
       " 341  [updated china based tencent tcehy has an undi... 2018-12-08   \n",
       " 342                                                 [] 2018-12-09   \n",
       " 343  [report bayer ceo says has not had communicati... 2018-12-10   \n",
       " 344  [bayry forecast sales for bayer ag adr are now... 2018-12-11   \n",
       " 345  [feel like jnj or bayry ate more likely to buy... 2018-12-12   \n",
       " 346                                                 [] 2018-12-13   \n",
       " 347  [user is forecast to net the most new sales in... 2018-12-14   \n",
       " 348                                                 [] 2018-12-15   \n",
       " 349  [update radiogel the new gold merck mrk to up ... 2018-12-16   \n",
       " 350                                                 [] 2018-12-17   \n",
       " 351  [bayer ag bayry given consensus rating of hold... 2018-12-18   \n",
       " 352  [glaxo stock pops on pfizer deal to merge cons... 2018-12-19   \n",
       " 353  [bayry bayer ag receives approval in china for... 2018-12-20   \n",
       " 354                                                 [] 2018-12-21   \n",
       " 355  [week in review how trump s policies moved sto... 2018-12-22   \n",
       " 356  [breaking radiogel the new gold merck mrk to u... 2018-12-23   \n",
       " 357  [most active otcpink stocks vol m tcehy 9 9 ns... 2018-12-24   \n",
       " 358                                                 [] 2018-12-25   \n",
       " 359                                                 [] 2018-12-26   \n",
       " 360  [holler at bp for some corexit spaceangel core... 2018-12-27   \n",
       " 361  [was just readin bout joel osteen wife not tak... 2018-12-28   \n",
       " 362                            [long bayry short jazz] 2018-12-29   \n",
       " 363                                                 [] 2018-12-30   \n",
       " 364                                                 [] 2018-12-31   \n",
       " \n",
       "             emot      mean  \n",
       " 0          [[1]]  1.000000  \n",
       " 1          [[0]]  0.000000  \n",
       " 2    [[0, 0, 0]]  0.000000  \n",
       " 3            NaN       NaN  \n",
       " 4            NaN       NaN  \n",
       " 5            NaN       NaN  \n",
       " 6            NaN       NaN  \n",
       " 7            NaN       NaN  \n",
       " 8            NaN       NaN  \n",
       " 9            NaN       NaN  \n",
       " 10           NaN       NaN  \n",
       " 11           NaN       NaN  \n",
       " 12           NaN       NaN  \n",
       " 13           NaN       NaN  \n",
       " 14           NaN       NaN  \n",
       " 15           NaN       NaN  \n",
       " 16           NaN       NaN  \n",
       " 17           NaN       NaN  \n",
       " 18           NaN       NaN  \n",
       " 19           NaN       NaN  \n",
       " 20           NaN       NaN  \n",
       " 21           NaN       NaN  \n",
       " 22           NaN       NaN  \n",
       " 23           NaN       NaN  \n",
       " 24           NaN       NaN  \n",
       " 25         [[0]]  0.000000  \n",
       " 26           NaN       NaN  \n",
       " 27           NaN       NaN  \n",
       " 28           NaN       NaN  \n",
       " 29           NaN       NaN  \n",
       " ..           ...       ...  \n",
       " 335     [[1, 1]]  1.000000  \n",
       " 336     [[1, 1]]  1.000000  \n",
       " 337          NaN       NaN  \n",
       " 338        [[0]]  0.000000  \n",
       " 339  [[1, 0, 0]]  0.333333  \n",
       " 340     [[0, 0]]  0.000000  \n",
       " 341        [[0]]  0.000000  \n",
       " 342          NaN       NaN  \n",
       " 343  [[1, 0, 1]]  0.666667  \n",
       " 344  [[1, 0, 0]]  0.333333  \n",
       " 345     [[1, 0]]  0.500000  \n",
       " 346          NaN       NaN  \n",
       " 347        [[0]]  0.000000  \n",
       " 348          NaN       NaN  \n",
       " 349        [[0]]  0.000000  \n",
       " 350          NaN       NaN  \n",
       " 351     [[0, 0]]  0.000000  \n",
       " 352        [[1]]  1.000000  \n",
       " 353  [[1, 0, 0]]  0.333333  \n",
       " 354          NaN       NaN  \n",
       " 355        [[0]]  0.000000  \n",
       " 356        [[0]]  0.000000  \n",
       " 357     [[0, 0]]  0.000000  \n",
       " 358          NaN       NaN  \n",
       " 359          NaN       NaN  \n",
       " 360     [[0, 1]]  0.500000  \n",
       " 361        [[0]]  0.000000  \n",
       " 362        [[1]]  1.000000  \n",
       " 363          NaN       NaN  \n",
       " 364          NaN       NaN  \n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'HON_2009':                                                  Tweet       Date  \\\n",
       " 0                                                   [] 2009-01-01   \n",
       " 1                                                   [] 2009-01-02   \n",
       " 2                                                   [] 2009-01-03   \n",
       " 3                                                   [] 2009-01-04   \n",
       " 4                                                   [] 2009-01-05   \n",
       " 5                                                   [] 2009-01-06   \n",
       " 6                                                   [] 2009-01-07   \n",
       " 7                                                   [] 2009-01-08   \n",
       " 8                                                   [] 2009-01-09   \n",
       " 9                                                   [] 2009-01-10   \n",
       " 10                                                  [] 2009-01-11   \n",
       " 11                                                  [] 2009-01-12   \n",
       " 12                                                  [] 2009-01-13   \n",
       " 13                                                  [] 2009-01-14   \n",
       " 14                                                  [] 2009-01-15   \n",
       " 15                                                  [] 2009-01-16   \n",
       " 16                                                  [] 2009-01-17   \n",
       " 17                                                  [] 2009-01-18   \n",
       " 18                                                  [] 2009-01-19   \n",
       " 19                                                  [] 2009-01-20   \n",
       " 20                                                  [] 2009-01-21   \n",
       " 21                                                  [] 2009-01-22   \n",
       " 22                                                  [] 2009-01-23   \n",
       " 23                                                  [] 2009-01-24   \n",
       " 24                                                  [] 2009-01-25   \n",
       " 25                                                  [] 2009-01-26   \n",
       " 26   [hon looking constructive here, gf s work hone... 2009-01-27   \n",
       " 27                                                  [] 2009-01-28   \n",
       " 28                                                  [] 2009-01-29   \n",
       " 29                                                  [] 2009-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 335  [my trading robot has detected a short signal ... 2009-12-02   \n",
       " 336                                                 [] 2009-12-03   \n",
       " 337  [url hon w a pattern worked out nicely right u... 2009-12-04   \n",
       " 338                                                 [] 2009-12-05   \n",
       " 339                                                 [] 2009-12-06   \n",
       " 340                                                 [] 2009-12-07   \n",
       " 341                                                 [] 2009-12-08   \n",
       " 342                                                 [] 2009-12-09   \n",
       " 343  [talking ba hon and clmt listen in on lunch th... 2009-12-10   \n",
       " 344  [invest in america week nucor url dis f hon m ... 2009-12-11   \n",
       " 345  [cramer next url aapl adbe ba bby bucy dfs dri... 2009-12-12   \n",
       " 346                                                 [] 2009-12-13   \n",
       " 347  [minneapolis public housing authority honeywel... 2009-12-14   \n",
       " 348  [url cover hon 4 83 macd vr anomaly closed out... 2009-12-15   \n",
       " 349  [user hon trader bought 1 dec 42 calls y day d... 2009-12-16   \n",
       " 350                                                 [] 2009-12-17   \n",
       " 351                                                 [] 2009-12-18   \n",
       " 352                                                 [] 2009-12-19   \n",
       " 353  [url hon took out trend line and 2 ma on incre... 2009-12-20   \n",
       " 354                                                 [] 2009-12-21   \n",
       " 355                                                 [] 2009-12-22   \n",
       " 356                                                 [] 2009-12-23   \n",
       " 357                                                 [] 2009-12-24   \n",
       " 358                                                 [] 2009-12-25   \n",
       " 359                                                 [] 2009-12-26   \n",
       " 360                                                 [] 2009-12-27   \n",
       " 361                                                 [] 2009-12-28   \n",
       " 362                                                 [] 2009-12-29   \n",
       " 363  [cramer 1 stocks to buy your kids in 2 1 url a... 2009-12-30   \n",
       " 364                                                 [] 2009-12-31   \n",
       " \n",
       "                                     emot      mean  \n",
       " 0                                    NaN       NaN  \n",
       " 1                                    NaN       NaN  \n",
       " 2                                    NaN       NaN  \n",
       " 3                                    NaN       NaN  \n",
       " 4                                    NaN       NaN  \n",
       " 5                                    NaN       NaN  \n",
       " 6                                    NaN       NaN  \n",
       " 7                                    NaN       NaN  \n",
       " 8                                    NaN       NaN  \n",
       " 9                                    NaN       NaN  \n",
       " 10                                   NaN       NaN  \n",
       " 11                                   NaN       NaN  \n",
       " 12                                   NaN       NaN  \n",
       " 13                                   NaN       NaN  \n",
       " 14                                   NaN       NaN  \n",
       " 15                                   NaN       NaN  \n",
       " 16                                   NaN       NaN  \n",
       " 17                                   NaN       NaN  \n",
       " 18                                   NaN       NaN  \n",
       " 19                                   NaN       NaN  \n",
       " 20                                   NaN       NaN  \n",
       " 21                                   NaN       NaN  \n",
       " 22                                   NaN       NaN  \n",
       " 23                                   NaN       NaN  \n",
       " 24                                   NaN       NaN  \n",
       " 25                                   NaN       NaN  \n",
       " 26                              [[0, 1]]  0.500000  \n",
       " 27                                   NaN       NaN  \n",
       " 28                                   NaN       NaN  \n",
       " 29                                   NaN       NaN  \n",
       " ..                                   ...       ...  \n",
       " 335                                [[1]]  1.000000  \n",
       " 336                                  NaN       NaN  \n",
       " 337                                [[0]]  0.000000  \n",
       " 338                                  NaN       NaN  \n",
       " 339                                  NaN       NaN  \n",
       " 340                                  NaN       NaN  \n",
       " 341                                  NaN       NaN  \n",
       " 342                                  NaN       NaN  \n",
       " 343                                [[0]]  0.000000  \n",
       " 344                                [[1]]  1.000000  \n",
       " 345                                [[0]]  0.000000  \n",
       " 346                                  NaN       NaN  \n",
       " 347                                [[0]]  0.000000  \n",
       " 348                       [[1, 1, 0, 1]]  0.750000  \n",
       " 349  [[0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0]]  0.545455  \n",
       " 350                                  NaN       NaN  \n",
       " 351                                  NaN       NaN  \n",
       " 352                                  NaN       NaN  \n",
       " 353                             [[0, 0]]  0.000000  \n",
       " 354                                  NaN       NaN  \n",
       " 355                                  NaN       NaN  \n",
       " 356                                  NaN       NaN  \n",
       " 357                                  NaN       NaN  \n",
       " 358                                  NaN       NaN  \n",
       " 359                                  NaN       NaN  \n",
       " 360                                  NaN       NaN  \n",
       " 361                                  NaN       NaN  \n",
       " 362                                  NaN       NaN  \n",
       " 363                                [[0]]  0.000000  \n",
       " 364                                  NaN       NaN  \n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'HON_2010':                                                  Tweet       Date  \\\n",
       " 0                                                   [] 2010-01-01   \n",
       " 1                                                   [] 2010-01-02   \n",
       " 2                                                   [] 2010-01-03   \n",
       " 3    [check out aerospace mention and more with ba ... 2010-01-04   \n",
       " 4                                                   [] 2010-01-05   \n",
       " 5                                                   [] 2010-01-06   \n",
       " 6    [hon, hon files form 8 k events or changes bet... 2010-01-07   \n",
       " 7    [sold czz for a small loss in order to invest ... 2010-01-08   \n",
       " 8                                                   [] 2010-01-09   \n",
       " 9                                                   [] 2010-01-10   \n",
       " 10                                                  [] 2010-01-11   \n",
       " 11                                                  [] 2010-01-12   \n",
       " 12                                                  [] 2010-01-13   \n",
       " 13   [company pleads guilty to negligent endangerme... 2010-01-14   \n",
       " 14                                                  [] 2010-01-15   \n",
       " 15                                                  [] 2010-01-16   \n",
       " 16                                                  [] 2010-01-17   \n",
       " 17                                                  [] 2010-01-18   \n",
       " 18                                                  [] 2010-01-19   \n",
       " 19                                                  [] 2010-01-20   \n",
       " 20                                                  [] 2010-01-21   \n",
       " 21                                                  [] 2010-01-22   \n",
       " 22                                                  [] 2010-01-23   \n",
       " 23                                                  [] 2010-01-24   \n",
       " 24                                                  [] 2010-01-25   \n",
       " 25                                                  [] 2010-01-26   \n",
       " 26                                                  [] 2010-01-27   \n",
       " 27                                                  [] 2010-01-28   \n",
       " 28   [hon reported earnings that beat by 1 and reaf... 2010-01-29   \n",
       " 29                                                  [] 2010-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 335                                                 [] 2010-12-02   \n",
       " 336                                                 [] 2010-12-03   \n",
       " 337                                                 [] 2010-12-04   \n",
       " 338                                                 [] 2010-12-05   \n",
       " 339                                                 [] 2010-12-06   \n",
       " 340                                                 [] 2010-12-07   \n",
       " 341                                                 [] 2010-12-08   \n",
       " 342                                                 [] 2010-12-09   \n",
       " 343  [honeywell hon volatility low at 25 into 1 div... 2010-12-10   \n",
       " 344                                                 [] 2010-12-11   \n",
       " 345                                                 [] 2010-12-12   \n",
       " 346  [user 1 top dividend stocks increasing payouts... 2010-12-13   \n",
       " 347  [wednesday s street sheet url hon nflx ngc1 fa... 2010-12-14   \n",
       " 348  [obama says progress was made in meeting on ec... 2010-12-15   \n",
       " 349                                                 [] 2010-12-16   \n",
       " 350  [jamie dimon eyes the door jpm bac wfc c ge go... 2010-12-17   \n",
       " 351                                                 [] 2010-12-18   \n",
       " 352                                                 [] 2010-12-19   \n",
       " 353                                                 [] 2010-12-20   \n",
       " 354                                                 [] 2010-12-21   \n",
       " 355  [exclusive sued chemie sale said to draw inter... 2010-12-22   \n",
       " 356                                                 [] 2010-12-23   \n",
       " 357                                                 [] 2010-12-24   \n",
       " 358                                                 [] 2010-12-25   \n",
       " 359                                                 [] 2010-12-26   \n",
       " 360                                                 [] 2010-12-27   \n",
       " 361                                                 [] 2010-12-28   \n",
       " 362                                                 [] 2010-12-29   \n",
       " 363                                                 [] 2010-12-30   \n",
       " 364                                                 [] 2010-12-31   \n",
       " \n",
       "                         emot      mean  \n",
       " 0                        NaN       NaN  \n",
       " 1                        NaN       NaN  \n",
       " 2                        NaN       NaN  \n",
       " 3                   [[0, 0]]  0.000000  \n",
       " 4                        NaN       NaN  \n",
       " 5                        NaN       NaN  \n",
       " 6                   [[0, 0]]  0.000000  \n",
       " 7    [[0, 0, 1, 0, 0, 1, 0]]  0.285714  \n",
       " 8                        NaN       NaN  \n",
       " 9                        NaN       NaN  \n",
       " 10                       NaN       NaN  \n",
       " 11                       NaN       NaN  \n",
       " 12                       NaN       NaN  \n",
       " 13                     [[1]]  1.000000  \n",
       " 14                       NaN       NaN  \n",
       " 15                       NaN       NaN  \n",
       " 16                       NaN       NaN  \n",
       " 17                       NaN       NaN  \n",
       " 18                       NaN       NaN  \n",
       " 19                       NaN       NaN  \n",
       " 20                       NaN       NaN  \n",
       " 21                       NaN       NaN  \n",
       " 22                       NaN       NaN  \n",
       " 23                       NaN       NaN  \n",
       " 24                       NaN       NaN  \n",
       " 25                       NaN       NaN  \n",
       " 26                       NaN       NaN  \n",
       " 27                       NaN       NaN  \n",
       " 28                     [[1]]  1.000000  \n",
       " 29                       NaN       NaN  \n",
       " ..                       ...       ...  \n",
       " 335                      NaN       NaN  \n",
       " 336                      NaN       NaN  \n",
       " 337                      NaN       NaN  \n",
       " 338                      NaN       NaN  \n",
       " 339                      NaN       NaN  \n",
       " 340                      NaN       NaN  \n",
       " 341                      NaN       NaN  \n",
       " 342                      NaN       NaN  \n",
       " 343              [[1, 0, 0]]  0.333333  \n",
       " 344                      NaN       NaN  \n",
       " 345                      NaN       NaN  \n",
       " 346                    [[0]]  0.000000  \n",
       " 347                    [[0]]  0.000000  \n",
       " 348           [[0, 0, 0, 0]]  0.000000  \n",
       " 349                      NaN       NaN  \n",
       " 350              [[0, 0, 0]]  0.000000  \n",
       " 351                      NaN       NaN  \n",
       " 352                      NaN       NaN  \n",
       " 353                      NaN       NaN  \n",
       " 354                      NaN       NaN  \n",
       " 355                    [[0]]  0.000000  \n",
       " 356                      NaN       NaN  \n",
       " 357                      NaN       NaN  \n",
       " 358                      NaN       NaN  \n",
       " 359                      NaN       NaN  \n",
       " 360                      NaN       NaN  \n",
       " 361                      NaN       NaN  \n",
       " 362                      NaN       NaN  \n",
       " 363                      NaN       NaN  \n",
       " 364                      NaN       NaN  \n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'HON_2011':                                                  Tweet       Date  \\\n",
       " 0                                                   [] 2011-01-01   \n",
       " 1                                                   [] 2011-01-02   \n",
       " 2                                                   [] 2011-01-03   \n",
       " 3    [hon honeywell international stock analysis ho... 2011-01-04   \n",
       " 4    [my investment game plan for 2 11 url cat itw ... 2011-01-05   \n",
       " 5    [pentagon s brinkley plays afghan matchmaker f... 2011-01-06   \n",
       " 6    [don t sell this gold stock url aem etn fnfg g... 2011-01-07   \n",
       " 7                                                   [] 2011-01-08   \n",
       " 8                                                   [] 2011-01-09   \n",
       " 9    [cramer s mad money son of lear 1 7 11 url aa ... 2011-01-10   \n",
       " 10                                                  [] 2011-01-11   \n",
       " 11                                                  [] 2011-01-12   \n",
       " 12                                                  [] 2011-01-13   \n",
       " 13   [new post mad men or happy days market url spy... 2011-01-14   \n",
       " 14                                                  [] 2011-01-15   \n",
       " 15                                                  [] 2011-01-16   \n",
       " 16   [hon url upward chan continues w macd closing ... 2011-01-17   \n",
       " 17                                                  [] 2011-01-18   \n",
       " 18   [obama hu focus on commercial ties in white ho... 2011-01-19   \n",
       " 19   [rt user ge investors look past profit to gaug... 2011-01-20   \n",
       " 20   [global inflation nips at url aapl amgn amzn a... 2011-01-21   \n",
       " 21                                                  [] 2011-01-22   \n",
       " 22   [honeywell haier group 2 heavy hitters bring e... 2011-01-23   \n",
       " 23                                                  [] 2011-01-24   \n",
       " 24   [obama embraces business agenda on exports roa... 2011-01-25   \n",
       " 25                                                  [] 2011-01-26   \n",
       " 26   [earnings a h amzn colm gmcr msft mww mygn rvb... 2011-01-27   \n",
       " 27   [morning news january 28 2 11 url via user lmt... 2011-01-28   \n",
       " 28                                                  [] 2011-01-29   \n",
       " 29                                                  [] 2011-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 335                                                 [] 2011-12-02   \n",
       " 336                                                 [] 2011-12-03   \n",
       " 337  [hon url double shooting star rsi flat macd ki... 2011-12-04   \n",
       " 338  [hon honeywell purchases wax technology expand... 2011-12-05   \n",
       " 339  [tuesday s watchlist amzn arba hon nflx soda u... 2011-12-06   \n",
       " 340                                                 [] 2011-12-07   \n",
       " 341                                                 [] 2011-12-08   \n",
       " 342  [week ahead markets turn their focus to the u ... 2011-12-09   \n",
       " 343                                                 [] 2011-12-10   \n",
       " 344                                                 [] 2011-12-11   \n",
       " 345  [pensions costs to pressure honeywell lockheed... 2011-12-12   \n",
       " 346                                                 [] 2011-12-13   \n",
       " 347                                                 [] 2011-12-14   \n",
       " 348  [pir m hon really busting butt now in show, 2 ... 2011-12-15   \n",
       " 349  [steve cohen and insiders are bullish on these... 2011-12-16   \n",
       " 350                                                 [] 2011-12-17   \n",
       " 351                                                 [] 2011-12-18   \n",
       " 352                                                 [] 2011-12-19   \n",
       " 353                                                 [] 2011-12-20   \n",
       " 354  [honeywell international inc hon president and... 2011-12-21   \n",
       " 355  [the five year plans to trust url crm dd deck ... 2011-12-22   \n",
       " 356  [2 stocks insiders and hedge funds love the mo... 2011-12-23   \n",
       " 357                                                 [] 2011-12-24   \n",
       " 358                                                 [] 2011-12-25   \n",
       " 359                                                 [] 2011-12-26   \n",
       " 360  [top trade ideas for the week of december 27 2... 2011-12-27   \n",
       " 361  [cramer what it takes to pick winning stocks u... 2011-12-28   \n",
       " 362                                                 [] 2011-12-29   \n",
       " 363  [whirlpool asks government to enforce trade la... 2011-12-30   \n",
       " 364                                                 [] 2011-12-31   \n",
       " \n",
       "                            emot   mean  \n",
       " 0                           NaN    NaN  \n",
       " 1                           NaN    NaN  \n",
       " 2                           NaN    NaN  \n",
       " 3                         [[0]]  0.000  \n",
       " 4                   [[0, 0, 0]]  0.000  \n",
       " 5                         [[0]]  0.000  \n",
       " 6                         [[1]]  1.000  \n",
       " 7                           NaN    NaN  \n",
       " 8                           NaN    NaN  \n",
       " 9                         [[1]]  1.000  \n",
       " 10                          NaN    NaN  \n",
       " 11                          NaN    NaN  \n",
       " 12                          NaN    NaN  \n",
       " 13                     [[0, 0]]  0.000  \n",
       " 14                          NaN    NaN  \n",
       " 15                          NaN    NaN  \n",
       " 16                        [[0]]  0.000  \n",
       " 17                          NaN    NaN  \n",
       " 18                        [[0]]  0.000  \n",
       " 19               [[0, 0, 0, 0]]  0.000  \n",
       " 20                        [[0]]  0.000  \n",
       " 21                          NaN    NaN  \n",
       " 22                        [[0]]  0.000  \n",
       " 23                          NaN    NaN  \n",
       " 24                        [[0]]  0.000  \n",
       " 25                          NaN    NaN  \n",
       " 26                        [[0]]  0.000  \n",
       " 27   [[0, 0, 0, 1, 0, 0, 0, 0]]  0.125  \n",
       " 28                          NaN    NaN  \n",
       " 29                          NaN    NaN  \n",
       " ..                          ...    ...  \n",
       " 335                         NaN    NaN  \n",
       " 336                         NaN    NaN  \n",
       " 337                       [[0]]  0.000  \n",
       " 338                       [[0]]  0.000  \n",
       " 339                    [[0, 0]]  0.000  \n",
       " 340                         NaN    NaN  \n",
       " 341                         NaN    NaN  \n",
       " 342                       [[1]]  1.000  \n",
       " 343                         NaN    NaN  \n",
       " 344                         NaN    NaN  \n",
       " 345                       [[0]]  0.000  \n",
       " 346                         NaN    NaN  \n",
       " 347                         NaN    NaN  \n",
       " 348              [[1, 1, 0, 0]]  0.500  \n",
       " 349                       [[0]]  0.000  \n",
       " 350                         NaN    NaN  \n",
       " 351                         NaN    NaN  \n",
       " 352                         NaN    NaN  \n",
       " 353                         NaN    NaN  \n",
       " 354                    [[0, 0]]  0.000  \n",
       " 355                       [[0]]  0.000  \n",
       " 356                       [[0]]  0.000  \n",
       " 357                         NaN    NaN  \n",
       " 358                         NaN    NaN  \n",
       " 359                         NaN    NaN  \n",
       " 360                       [[0]]  0.000  \n",
       " 361                    [[0, 0]]  0.000  \n",
       " 362                         NaN    NaN  \n",
       " 363                       [[1]]  1.000  \n",
       " 364                         NaN    NaN  \n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'HON_2012':                                                  Tweet       Date  \\\n",
       " 0    [ok no more sad shit time to be an ass again h... 2012-01-01   \n",
       " 1    [honeywell expanding turbocharger portfolio in... 2012-01-02   \n",
       " 2    [url stocks shaping the right side of their ba... 2012-01-03   \n",
       " 3    [my short term watchlist clf fcx bhp brgyy rds... 2012-01-04   \n",
       " 4    [a year end lookback on my 2 11 investment gam... 2012-01-05   \n",
       " 5    [can someone tell me why the hell is aa still ... 2012-01-06   \n",
       " 6                                                   [] 2012-01-07   \n",
       " 7    [some nice golden cross setups ame cmc dia exx... 2012-01-08   \n",
       " 8    [user user ok am i still include in this convo... 2012-01-09   \n",
       " 9                                                   [] 2012-01-10   \n",
       " 10                                                  [] 2012-01-11   \n",
       " 11                                                  [] 2012-01-12   \n",
       " 12                                                  [] 2012-01-13   \n",
       " 13                                                  [] 2012-01-14   \n",
       " 14                                                  [] 2012-01-15   \n",
       " 15                                                  [] 2012-01-16   \n",
       " 16                                                  [] 2012-01-17   \n",
       " 17                                                  [] 2012-01-18   \n",
       " 18                                                  [] 2012-01-19   \n",
       " 19                                                  [] 2012-01-20   \n",
       " 20                                                  [] 2012-01-21   \n",
       " 21                                                  [] 2012-01-22   \n",
       " 22                                                  [] 2012-01-23   \n",
       " 23                                                  [] 2012-01-24   \n",
       " 24                                                  [] 2012-01-25   \n",
       " 25                                                  [] 2012-01-26   \n",
       " 26   [resilient market yesterday people despised et... 2012-01-27   \n",
       " 27                                                  [] 2012-01-28   \n",
       " 28                                                  [] 2012-01-29   \n",
       " 29                                                  [] 2012-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 335                                                 [] 2012-12-02   \n",
       " 336                                                 [] 2012-12-03   \n",
       " 337  [the hidden impact of solar why you should car... 2012-12-04   \n",
       " 338  [new stock coverage victoria s secret owner li... 2012-12-05   \n",
       " 339                                                 [] 2012-12-06   \n",
       " 340                                                 [] 2012-12-07   \n",
       " 341                                                 [] 2012-12-08   \n",
       " 342                                                 [] 2012-12-09   \n",
       " 343  [wopr from user said to buy intermec in on 12 ... 2012-12-10   \n",
       " 344                                                 [] 2012-12-11   \n",
       " 345  [complete thursday game plan url aph wor tel a... 2012-12-12   \n",
       " 346  [for the first time in a long time mmm guide o... 2012-12-13   \n",
       " 347  [rt user how honeywell ceo david cote found hi... 2012-12-14   \n",
       " 348                                                 [] 2012-12-15   \n",
       " 349                                                 [] 2012-12-16   \n",
       " 350  [i first read this as rectal scanner rt user h... 2012-12-17   \n",
       " 351                                                 [] 2012-12-18   \n",
       " 352  [hollysys looks like a name to watch in emergi... 2012-12-19   \n",
       " 353                                                 [] 2012-12-20   \n",
       " 354  [does the hon board of directors condone the c... 2012-12-21   \n",
       " 355                                                 [] 2012-12-22   \n",
       " 356                                                 [] 2012-12-23   \n",
       " 357                                                 [] 2012-12-24   \n",
       " 358                                                 [] 2012-12-25   \n",
       " 359                                                 [] 2012-12-26   \n",
       " 360                                                 [] 2012-12-27   \n",
       " 361  [so obama thinks a 5 3 conference call with ho... 2012-12-28   \n",
       " 362                                                 [] 2012-12-29   \n",
       " 363                                                 [] 2012-12-30   \n",
       " 364                                                 [] 2012-12-31   \n",
       " \n",
       "                         emot      mean  \n",
       " 0                      [[1]]  1.000000  \n",
       " 1                [[0, 0, 0]]  0.000000  \n",
       " 2                [[0, 0, 0]]  0.000000  \n",
       " 3                [[1, 0, 0]]  0.333333  \n",
       " 4    [[0, 0, 0, 0, 0, 0, 0]]  0.000000  \n",
       " 5                      [[1]]  1.000000  \n",
       " 6                        NaN       NaN  \n",
       " 7                [[0, 0, 0]]  0.000000  \n",
       " 8                      [[0]]  0.000000  \n",
       " 9                        NaN       NaN  \n",
       " 10                       NaN       NaN  \n",
       " 11                       NaN       NaN  \n",
       " 12                       NaN       NaN  \n",
       " 13                       NaN       NaN  \n",
       " 14                       NaN       NaN  \n",
       " 15                       NaN       NaN  \n",
       " 16                       NaN       NaN  \n",
       " 17                       NaN       NaN  \n",
       " 18                       NaN       NaN  \n",
       " 19                       NaN       NaN  \n",
       " 20                       NaN       NaN  \n",
       " 21                       NaN       NaN  \n",
       " 22                       NaN       NaN  \n",
       " 23                       NaN       NaN  \n",
       " 24                       NaN       NaN  \n",
       " 25                       NaN       NaN  \n",
       " 26                     [[0]]  0.000000  \n",
       " 27                       NaN       NaN  \n",
       " 28                       NaN       NaN  \n",
       " 29                       NaN       NaN  \n",
       " ..                       ...       ...  \n",
       " 335                      NaN       NaN  \n",
       " 336                      NaN       NaN  \n",
       " 337                 [[0, 0]]  0.000000  \n",
       " 338                 [[0, 0]]  0.000000  \n",
       " 339                      NaN       NaN  \n",
       " 340                      NaN       NaN  \n",
       " 341                      NaN       NaN  \n",
       " 342                      NaN       NaN  \n",
       " 343        [[0, 1, 1, 0, 0]]  0.400000  \n",
       " 344                      NaN       NaN  \n",
       " 345                    [[0]]  0.000000  \n",
       " 346              [[0, 1, 1]]  0.666667  \n",
       " 347              [[0, 0, 0]]  0.000000  \n",
       " 348                      NaN       NaN  \n",
       " 349                      NaN       NaN  \n",
       " 350                    [[0]]  0.000000  \n",
       " 351                      NaN       NaN  \n",
       " 352                    [[0]]  0.000000  \n",
       " 353                      NaN       NaN  \n",
       " 354                    [[0]]  0.000000  \n",
       " 355                      NaN       NaN  \n",
       " 356                      NaN       NaN  \n",
       " 357                      NaN       NaN  \n",
       " 358                      NaN       NaN  \n",
       " 359                      NaN       NaN  \n",
       " 360                      NaN       NaN  \n",
       " 361                 [[0, 0]]  0.000000  \n",
       " 362                      NaN       NaN  \n",
       " 363                      NaN       NaN  \n",
       " 364                      NaN       NaN  \n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'HON_2013':                                                  Tweet       Date  \\\n",
       " 0    [did the president deliberately snub wall stre... 2013-01-01   \n",
       " 1    [my 2 12 investment game plan year end lookbac... 2013-01-02   \n",
       " 2    [hon honeywell international given buy rating ... 2013-01-03   \n",
       " 3    [which industrial giant is a winning bet for n... 2013-01-04   \n",
       " 4    [honeywell continues to supply gm and 3 stocks... 2013-01-05   \n",
       " 5                                                   [] 2013-01-06   \n",
       " 6    [hon honeywell international given new 74 pric... 2013-01-07   \n",
       " 7    [honeywell to release fourth quarter financial... 2013-01-08   \n",
       " 8    [top s p1 stocks performance ba hpq ma wy unh ... 2013-01-09   \n",
       " 9    [cramer sell kior and buy joy global honeywell... 2013-01-10   \n",
       " 10                                                  [] 2013-01-11   \n",
       " 11   [honeywell will trade at new all time highs by... 2013-01-12   \n",
       " 12                                                  [] 2013-01-13   \n",
       " 13   [honeywell international hon showing bullish t... 2013-01-14   \n",
       " 14   [52 week highs ford office depot walgreens hon... 2013-01-15   \n",
       " 15                                                  [] 2013-01-16   \n",
       " 16                                                  [] 2013-01-17   \n",
       " 17   [next week s key earnings reports url jnj isrg... 2013-01-18   \n",
       " 18                                                  [] 2013-01-19   \n",
       " 19                                                  [] 2013-01-20   \n",
       " 20                                                  [] 2013-01-21   \n",
       " 21                                                  [] 2013-01-22   \n",
       " 22                                                  [] 2013-01-23   \n",
       " 23   [fund manager tom forester three old friends s... 2013-01-24   \n",
       " 24   [what ceos of ge apple starbucks other u s gia... 2013-01-25   \n",
       " 25                                                  [] 2013-01-26   \n",
       " 26                                                  [] 2013-01-27   \n",
       " 27   [wsj from power tools to carpets signs of hous... 2013-01-28   \n",
       " 28                                                  [] 2013-01-29   \n",
       " 29                                                  [] 2013-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 335                                                 [] 2013-12-02   \n",
       " 336                                                 [] 2013-12-03   \n",
       " 337  [had limit orders trigger today selling an iro... 2013-12-04   \n",
       " 338                                                 [] 2013-12-05   \n",
       " 339                                                 [] 2013-12-06   \n",
       " 340                                                 [] 2013-12-07   \n",
       " 341  [ceo jeff immelt has delivered ge shareholders... 2013-12-08   \n",
       " 342                                                 [] 2013-12-09   \n",
       " 343                                                 [] 2013-12-10   \n",
       " 344                                                 [] 2013-12-11   \n",
       " 345                                                 [] 2013-12-12   \n",
       " 346  [honeywell announces 5b stock buyback hon, ge ... 2013-12-13   \n",
       " 347                                                 [] 2013-12-14   \n",
       " 348  [honeywell hon is green lighted by its board f... 2013-12-15   \n",
       " 349                                                 [] 2013-12-16   \n",
       " 350  [miss the market day here are today s top stoc... 2013-12-17   \n",
       " 351  [i still really like hon and ba of course thos... 2013-12-18   \n",
       " 352                                                 [] 2013-12-19   \n",
       " 353  [jim cramer s charitable trust portfolio url c... 2013-12-20   \n",
       " 354                                                 [] 2013-12-21   \n",
       " 355  [obamacare doctor shortage to spur 2 billion t... 2013-12-22   \n",
       " 356                                                 [] 2013-12-23   \n",
       " 357                                                 [] 2013-12-24   \n",
       " 358                                                 [] 2013-12-25   \n",
       " 359                                                 [] 2013-12-26   \n",
       " 360  [ups fedex fly cost cutting technology not dro... 2013-12-27   \n",
       " 361                                                 [] 2013-12-28   \n",
       " 362  [iata nov air traffic 4 8 ytd 5 3 ahead of the... 2013-12-29   \n",
       " 363  [is diversified industrial a safe place for co... 2013-12-30   \n",
       " 364                                                 [] 2013-12-31   \n",
       " \n",
       "                            emot  mean  \n",
       " 0                      [[0, 0]]  0.00  \n",
       " 1    [[1, 0, 0, 1, 1, 0, 1, 0]]  0.50  \n",
       " 2                      [[0, 0]]  0.00  \n",
       " 3                      [[0, 1]]  0.50  \n",
       " 4                         [[0]]  0.00  \n",
       " 5                           NaN   NaN  \n",
       " 6                         [[0]]  0.00  \n",
       " 7                      [[1, 0]]  0.50  \n",
       " 8                      [[0, 1]]  0.50  \n",
       " 9                      [[0, 0]]  0.00  \n",
       " 10                          NaN   NaN  \n",
       " 11                        [[0]]  0.00  \n",
       " 12                          NaN   NaN  \n",
       " 13                        [[0]]  0.00  \n",
       " 14                     [[1, 0]]  0.50  \n",
       " 15                          NaN   NaN  \n",
       " 16                          NaN   NaN  \n",
       " 17                        [[0]]  0.00  \n",
       " 18                          NaN   NaN  \n",
       " 19                          NaN   NaN  \n",
       " 20                          NaN   NaN  \n",
       " 21                          NaN   NaN  \n",
       " 22                          NaN   NaN  \n",
       " 23                        [[0]]  0.00  \n",
       " 24               [[0, 0, 0, 1]]  0.25  \n",
       " 25                          NaN   NaN  \n",
       " 26                          NaN   NaN  \n",
       " 27                        [[0]]  0.00  \n",
       " 28                          NaN   NaN  \n",
       " 29                          NaN   NaN  \n",
       " ..                          ...   ...  \n",
       " 335                         NaN   NaN  \n",
       " 336                         NaN   NaN  \n",
       " 337                       [[0]]  0.00  \n",
       " 338                         NaN   NaN  \n",
       " 339                         NaN   NaN  \n",
       " 340                         NaN   NaN  \n",
       " 341                       [[0]]  0.00  \n",
       " 342                         NaN   NaN  \n",
       " 343                         NaN   NaN  \n",
       " 344                         NaN   NaN  \n",
       " 345                         NaN   NaN  \n",
       " 346  [[1, 0, 0, 1, 0, 0, 0, 0]]  0.25  \n",
       " 347                         NaN   NaN  \n",
       " 348                       [[0]]  0.00  \n",
       " 349                         NaN   NaN  \n",
       " 350           [[0, 1, 0, 0, 0]]  0.20  \n",
       " 351                    [[0, 0]]  0.00  \n",
       " 352                         NaN   NaN  \n",
       " 353                       [[0]]  0.00  \n",
       " 354                         NaN   NaN  \n",
       " 355                       [[0]]  0.00  \n",
       " 356                         NaN   NaN  \n",
       " 357                         NaN   NaN  \n",
       " 358                         NaN   NaN  \n",
       " 359                         NaN   NaN  \n",
       " 360                       [[0]]  0.00  \n",
       " 361                         NaN   NaN  \n",
       " 362                       [[0]]  0.00  \n",
       " 363                       [[0]]  0.00  \n",
       " 364                         NaN   NaN  \n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'HON_2014':                                                  Tweet       Date  \\\n",
       " 0                                                   [] 2014-01-01   \n",
       " 1                                                   [] 2014-01-02   \n",
       " 2                                                   [] 2014-01-03   \n",
       " 3                                                   [] 2014-01-04   \n",
       " 4                                                   [] 2014-01-05   \n",
       " 5                                                   [] 2014-01-06   \n",
       " 6                                                   [] 2014-01-07   \n",
       " 7                                                   [] 2014-01-08   \n",
       " 8                                                   [] 2014-01-09   \n",
       " 9                                                   [] 2014-01-10   \n",
       " 10                                                  [] 2014-01-11   \n",
       " 11                                                  [] 2014-01-12   \n",
       " 12                                                  [] 2014-01-13   \n",
       " 13                                                  [] 2014-01-14   \n",
       " 14                                                  [] 2014-01-15   \n",
       " 15                                                  [] 2014-01-16   \n",
       " 16                                                  [] 2014-01-17   \n",
       " 17                                                  [] 2014-01-18   \n",
       " 18                                                  [] 2014-01-19   \n",
       " 19                                                  [] 2014-01-20   \n",
       " 20                                                  [] 2014-01-21   \n",
       " 21                                                  [] 2014-01-22   \n",
       " 22                                                  [] 2014-01-23   \n",
       " 23                                                  [] 2014-01-24   \n",
       " 24                                                  [] 2014-01-25   \n",
       " 25                                                  [] 2014-01-26   \n",
       " 26                                                  [] 2014-01-27   \n",
       " 27                                                  [] 2014-01-28   \n",
       " 28                                                  [] 2014-01-29   \n",
       " 29                                                  [] 2014-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 335  [did you lose money on wfm hon bk abx personal... 2014-12-02   \n",
       " 336  [3 stocks up 1 since appointing new ceos tim c... 2014-12-03   \n",
       " 337  [hon hon stock gapped up 28 hon last price 99 ... 2014-12-04   \n",
       " 338  [are you putting your money in hon i kss do in... 2014-12-05   \n",
       " 339  [stocks you might want to hold onto hma hon da... 2014-12-06   \n",
       " 340  [todays movers to research key mdrx hon slw in... 2014-12-07   \n",
       " 341  [hon hon 99 49 honeywell international inc hon... 2014-12-08   \n",
       " 342  [looking for more info on lmca aap hon cbm mon... 2014-12-09   \n",
       " 343                                                 [] 2014-12-10   \n",
       " 344  [smarthome devices gaining traction aapl googl... 2014-12-11   \n",
       " 345  [hon s town hon s town black tape poems url vi... 2014-12-12   \n",
       " 346                                                 [] 2014-12-13   \n",
       " 347  [earnings dec 15 19 bbry play dri pay fdx fcel... 2014-12-14   \n",
       " 348  [why hon s ceo dave cote is loving low oil pri... 2014-12-15   \n",
       " 349  [regretting your investment in xray hon abbv f... 2014-12-16   \n",
       " 350  [honeywell chairman ceo dave cote on user re h... 2014-12-17   \n",
       " 351  [tidings of comfort joy lot of well known larg... 2014-12-18   \n",
       " 352  [don t take it from user these c suite perspec... 2014-12-19   \n",
       " 353  [this weeks stocks you should watch mfa hon wc... 2014-12-20   \n",
       " 354  [listen to lean feat deno ghost prod by taysou... 2014-12-21   \n",
       " 355  [52 week high fb dis cmcsa csco hd cvs mmm ups... 2014-12-22   \n",
       " 356  [52 week high wfc pg orcl bac v dis cmcsa csco... 2014-12-23   \n",
       " 357  [conglomerate stocks year in review and where ... 2014-12-24   \n",
       " 358                                                 [] 2014-12-25   \n",
       " 359                                                 [] 2014-12-26   \n",
       " 360                                                 [] 2014-12-27   \n",
       " 361                                                 [] 2014-12-28   \n",
       " 362                                                 [] 2014-12-29   \n",
       " 363                                                 [] 2014-12-30   \n",
       " 364                                                 [] 2014-12-31   \n",
       " \n",
       "                                                   emot      mean  \n",
       " 0                                                  NaN       NaN  \n",
       " 1                                                  NaN       NaN  \n",
       " 2                                                  NaN       NaN  \n",
       " 3                                                  NaN       NaN  \n",
       " 4                                                  NaN       NaN  \n",
       " 5                                                  NaN       NaN  \n",
       " 6                                                  NaN       NaN  \n",
       " 7                                                  NaN       NaN  \n",
       " 8                                                  NaN       NaN  \n",
       " 9                                                  NaN       NaN  \n",
       " 10                                                 NaN       NaN  \n",
       " 11                                                 NaN       NaN  \n",
       " 12                                                 NaN       NaN  \n",
       " 13                                                 NaN       NaN  \n",
       " 14                                                 NaN       NaN  \n",
       " 15                                                 NaN       NaN  \n",
       " 16                                                 NaN       NaN  \n",
       " 17                                                 NaN       NaN  \n",
       " 18                                                 NaN       NaN  \n",
       " 19                                                 NaN       NaN  \n",
       " 20                                                 NaN       NaN  \n",
       " 21                                                 NaN       NaN  \n",
       " 22                                                 NaN       NaN  \n",
       " 23                                                 NaN       NaN  \n",
       " 24                                                 NaN       NaN  \n",
       " 25                                                 NaN       NaN  \n",
       " 26                                                 NaN       NaN  \n",
       " 27                                                 NaN       NaN  \n",
       " 28                                                 NaN       NaN  \n",
       " 29                                                 NaN       NaN  \n",
       " ..                                                 ...       ...  \n",
       " 335                                        [[1, 0, 0]]  0.333333  \n",
       " 336                                        [[0, 0, 0]]  0.000000  \n",
       " 337                                           [[1, 0]]  0.500000  \n",
       " 338                                           [[0, 1]]  0.500000  \n",
       " 339                                        [[0, 0, 1]]  0.333333  \n",
       " 340                                              [[0]]  0.000000  \n",
       " 341                                              [[0]]  0.000000  \n",
       " 342                                           [[0, 1]]  0.500000  \n",
       " 343                                                NaN       NaN  \n",
       " 344                                        [[1, 0, 0]]  0.333333  \n",
       " 345                                        [[0, 0, 0]]  0.000000  \n",
       " 346                                                NaN       NaN  \n",
       " 347                                           [[0, 1]]  0.500000  \n",
       " 348  [[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,...  0.187500  \n",
       " 349                                        [[0, 0, 0]]  0.000000  \n",
       " 350                                     [[0, 0, 0, 0]]  0.000000  \n",
       " 351                                        [[0, 0, 0]]  0.000000  \n",
       " 352                                     [[0, 1, 1, 0]]  0.500000  \n",
       " 353                                           [[0, 0]]  0.000000  \n",
       " 354                                              [[0]]  0.000000  \n",
       " 355                                           [[1, 0]]  0.500000  \n",
       " 356                                              [[1]]  1.000000  \n",
       " 357                                              [[0]]  0.000000  \n",
       " 358                                                NaN       NaN  \n",
       " 359                                                NaN       NaN  \n",
       " 360                                                NaN       NaN  \n",
       " 361                                                NaN       NaN  \n",
       " 362                                                NaN       NaN  \n",
       " 363                                                NaN       NaN  \n",
       " 364                                                NaN       NaN  \n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'HON_2015':                                                  Tweet       Date  \\\n",
       " 0    [morningstar gives a credit rating to honeywel... 2015-01-01   \n",
       " 1    [hon traders sell shares of honeywell internat... 2015-01-02   \n",
       " 2    [hon on the snap new nip kingriquo url, honeyw... 2015-01-03   \n",
       " 3                                                   [] 2015-01-04   \n",
       " 4    [the winning team fear this user x user x hon ... 2015-01-05   \n",
       " 5    [hon honeywell announces that it has started f... 2015-01-06   \n",
       " 6    [hon my bruvvvaaa how ya been i got your name ... 2015-01-07   \n",
       " 7    [honeywell to report q4 results on jan 23 url ... 2015-01-08   \n",
       " 8    [hon still looks overpriced rated 1 9 as price... 2015-01-09   \n",
       " 9    [user my list of stevejobs like ceos ua tsla r... 2015-01-10   \n",
       " 10                                                  [] 2015-01-11   \n",
       " 11   [user aa guide in aerospace sounds good for ho... 2015-01-12   \n",
       " 12   [s p1 stocks performance bmy twx cmcsa amzn ap... 2015-01-13   \n",
       " 13   [what some bellwethers are saying about 2 15 u... 2015-01-14   \n",
       " 14   [buy signal detected for 22 stocks sna dfs hon... 2015-01-15   \n",
       " 15   [mad money recap next week s game plan gd ptct... 2015-01-16   \n",
       " 16   [cramer s mad money cramer s game plan 1 16 15... 2015-01-17   \n",
       " 17                                                  [] 2015-01-18   \n",
       " 18                                                  [] 2015-01-19   \n",
       " 19   [etf securities branches out from commodities ... 2015-01-20   \n",
       " 20                                                  [] 2015-01-21   \n",
       " 21   [draghi delivers and market likes it mcd hon s... 2015-01-22   \n",
       " 22   [bingo rt user user is this a thing of beauty ... 2015-01-23   \n",
       " 23                                                  [] 2015-01-24   \n",
       " 24                                                  [] 2015-01-25   \n",
       " 25                                                  [] 2015-01-26   \n",
       " 26                                                  [] 2015-01-27   \n",
       " 27                                                  [] 2015-01-28   \n",
       " 28                                                  [] 2015-01-29   \n",
       " 29                                                  [] 2015-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 335  [short seller alert hon honeywell intl shortse... 2015-12-02   \n",
       " 336  [now playing ab tract mixtape by hon via user ... 2015-12-03   \n",
       " 337  [hon honeywell intl financials url hon avgo wm... 2015-12-04   \n",
       " 338  [hon is set up for a strong run higher chart p... 2015-12-05   \n",
       " 339                                                 [] 2015-12-06   \n",
       " 340                                                 [] 2015-12-07   \n",
       " 341  [watchlist additions going into 2 16 hon lmt b... 2015-12-08   \n",
       " 342                                                 [] 2015-12-09   \n",
       " 343  [call put highest volume changes calls mchp hz... 2015-12-10   \n",
       " 344                [hon breaking down into some space] 2015-12-11   \n",
       " 345                                                 [] 2015-12-12   \n",
       " 346                                                 [] 2015-12-13   \n",
       " 347  [buy signal detected for 21 stocks dhi len rht... 2015-12-14   \n",
       " 348  [hon dow ge, merck pfizer ceos among those vis... 2015-12-15   \n",
       " 349  [top sentiment for sp15 stocks at midday cvs m... 2015-12-16   \n",
       " 350  [industrial stocks talk 2 16 thoughts on the t... 2015-12-17   \n",
       " 351  [honeywell international inc valuation october... 2015-12-18   \n",
       " 352                                                 [] 2015-12-19   \n",
       " 353                                                 [] 2015-12-20   \n",
       " 354  [stocks is there life after fang url via user ... 2015-12-21   \n",
       " 355  [mro 1 to 2 89p ec clearance of elster sale re... 2015-12-22   \n",
       " 356  [sell signal detected for 24 stocks unh hon ce... 2015-12-23   \n",
       " 357  [markets react to the rate hike what can inves... 2015-12-24   \n",
       " 358                                                 [] 2015-12-25   \n",
       " 359  [aria ariad pharmaceuticals inc change url ari... 2015-12-26   \n",
       " 360                                                 [] 2015-12-27   \n",
       " 361                                                 [] 2015-12-28   \n",
       " 362  [hon read this maybe honeywell could do this a... 2015-12-29   \n",
       " 363  [regulators consider satellite tracking for de... 2015-12-30   \n",
       " 364                                                 [] 2015-12-31   \n",
       " \n",
       "                                           emot      mean  \n",
       " 0                                     [[0, 0]]  0.000000  \n",
       " 1    [[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]]  0.153846  \n",
       " 2                                     [[0, 0]]  0.000000  \n",
       " 3                                          NaN       NaN  \n",
       " 4                                     [[0, 0]]  0.000000  \n",
       " 5                                     [[0, 0]]  0.000000  \n",
       " 6                                     [[0, 0]]  0.000000  \n",
       " 7                                        [[1]]  1.000000  \n",
       " 8                                        [[1]]  1.000000  \n",
       " 9                                        [[0]]  0.000000  \n",
       " 10                                         NaN       NaN  \n",
       " 11                                       [[0]]  0.000000  \n",
       " 12                                       [[0]]  0.000000  \n",
       " 13                                       [[0]]  0.000000  \n",
       " 14                                       [[0]]  0.000000  \n",
       " 15                                    [[0, 0]]  0.000000  \n",
       " 16                                       [[0]]  0.000000  \n",
       " 17                                         NaN       NaN  \n",
       " 18                                         NaN       NaN  \n",
       " 19                                    [[0, 0]]  0.000000  \n",
       " 20                                         NaN       NaN  \n",
       " 21                                    [[0, 0]]  0.000000  \n",
       " 22                              [[0, 0, 1, 0]]  0.250000  \n",
       " 23                                         NaN       NaN  \n",
       " 24                                         NaN       NaN  \n",
       " 25                                         NaN       NaN  \n",
       " 26                                         NaN       NaN  \n",
       " 27                                         NaN       NaN  \n",
       " 28                                         NaN       NaN  \n",
       " 29                                         NaN       NaN  \n",
       " ..                                         ...       ...  \n",
       " 335                                      [[0]]  0.000000  \n",
       " 336                                      [[0]]  0.000000  \n",
       " 337                                      [[0]]  0.000000  \n",
       " 338                                      [[0]]  0.000000  \n",
       " 339                                        NaN       NaN  \n",
       " 340                                        NaN       NaN  \n",
       " 341                                   [[0, 0]]  0.000000  \n",
       " 342                                        NaN       NaN  \n",
       " 343                                   [[0, 1]]  0.500000  \n",
       " 344                                      [[1]]  1.000000  \n",
       " 345                                        NaN       NaN  \n",
       " 346                                        NaN       NaN  \n",
       " 347                                      [[1]]  1.000000  \n",
       " 348                          [[0, 0, 0, 0, 0]]  0.000000  \n",
       " 349     [[0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0]]  0.333333  \n",
       " 350                             [[0, 0, 1, 0]]  0.250000  \n",
       " 351                                      [[0]]  0.000000  \n",
       " 352                                        NaN       NaN  \n",
       " 353                                        NaN       NaN  \n",
       " 354                                      [[0]]  0.000000  \n",
       " 355                                [[0, 0, 0]]  0.000000  \n",
       " 356                                   [[1, 0]]  0.500000  \n",
       " 357                                      [[0]]  0.000000  \n",
       " 358                                        NaN       NaN  \n",
       " 359                                   [[0, 0]]  0.000000  \n",
       " 360                                        NaN       NaN  \n",
       " 361                                        NaN       NaN  \n",
       " 362                       [[1, 0, 0, 0, 0, 0]]  0.166667  \n",
       " 363                                      [[0]]  0.000000  \n",
       " 364                                        NaN       NaN  \n",
       " \n",
       " [365 rows x 4 columns],\n",
       " 'HON_2016':                                                  Tweet       Date  \\\n",
       " 0    [info hon sentiment indicator as seen by inves... 2016-01-01   \n",
       " 1                                                   [] 2016-01-02   \n",
       " 2                                                   [] 2016-01-03   \n",
       " 3    [user ge or hon either one, swing buy candidat... 2016-01-04   \n",
       " 4    [renaissance investment group takes position i... 2016-01-05   \n",
       " 5    [technical longs i like owning here fb googl a... 2016-01-06   \n",
       " 6                                                   [] 2016-01-07   \n",
       " 7                                                   [] 2016-01-08   \n",
       " 8    [hon honeywell international inc hon shares so... 2016-01-09   \n",
       " 9                                                   [] 2016-01-10   \n",
       " 10   [hon nasdaq stocks click here url trending hon... 2016-01-11   \n",
       " 11   [dmrc hon honeywell s xenon 19 handheld barcod... 2016-01-12   \n",
       " 12                                                  [] 2016-01-13   \n",
       " 13                                                  [] 2016-01-14   \n",
       " 14                                                  [] 2016-01-15   \n",
       " 15                                                  [] 2016-01-16   \n",
       " 16   [market meltdown what s the fed got to do with... 2016-01-17   \n",
       " 17   [like companies with solid fundamentals have a... 2016-01-18   \n",
       " 18                                                  [] 2016-01-19   \n",
       " 19                                                  [] 2016-01-20   \n",
       " 20   [new whitepaper published on home automation a... 2016-01-21   \n",
       " 21   [um labenthal thinks hon and ge have too much ... 2016-01-22   \n",
       " 22                                                  [] 2016-01-23   \n",
       " 23   [big earnings week coming up mon mcd kmb hal t... 2016-01-24   \n",
       " 24                                                  [] 2016-01-25   \n",
       " 25                                                  [] 2016-01-26   \n",
       " 26                                                  [] 2016-01-27   \n",
       " 27                                                  [] 2016-01-28   \n",
       " 28                                                  [] 2016-01-29   \n",
       " 29                                                  [] 2016-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 336                                                 [] 2016-12-03   \n",
       " 337                                                 [] 2016-12-04   \n",
       " 338  [viasat vsat has real wifi that can stream any... 2016-12-05   \n",
       " 339  [honeywell confirms it will locate 19 million ... 2016-12-06   \n",
       " 340  [hon case study with bullflow and something to... 2016-12-07   \n",
       " 341  [hon update dec 8 2 16 top oi changes 1 159 ad... 2016-12-08   \n",
       " 342  [user plays industrial activist with nelson pe... 2016-12-09   \n",
       " 343  [blue bell private wealth management llc maint... 2016-12-10   \n",
       " 344                                                 [] 2016-12-11   \n",
       " 345  [pretty uneventful open early flow flow dia ar... 2016-12-12   \n",
       " 346  [hon case study on calls flagged user at 93 1 ... 2016-12-13   \n",
       " 347  [hon is a pseudo pe firm focused on mfg tech a... 2016-12-14   \n",
       " 348  [gpro gopro takeover targets include honeywell... 2016-12-15   \n",
       " 349  [downside pre mkt agio fnsr gpor ubsi jdst jwn... 2016-12-16   \n",
       " 350  [january 2 17 earnings is going to be a bloodb... 2016-12-17   \n",
       " 351                                                 [] 2016-12-18   \n",
       " 352  [here s oppenheimer s list of the 29 best stoc... 2016-12-19   \n",
       " 353  [video live day trading making 25 in opening 3... 2016-12-20   \n",
       " 354  [report honeywell among bidders for munters ur... 2016-12-21   \n",
       " 355  [top indicators flat in november cat de hon it... 2016-12-22   \n",
       " 356                  [asix spinoff from hon chem fert] 2016-12-23   \n",
       " 357  [my new sounds enemies que hon ft swift taylor... 2016-12-24   \n",
       " 358                                                 [] 2016-12-25   \n",
       " 359                                                 [] 2016-12-26   \n",
       " 360  [whos your right hand man hon url, themotleyfo... 2016-12-27   \n",
       " 361                                                 [] 2016-12-28   \n",
       " 362  [still up year over year and likely to weigh o... 2016-12-29   \n",
       " 363                                                 [] 2016-12-30   \n",
       " 364                                                 [] 2016-12-31   \n",
       " 365  [hon analyst price target on honeywell interna... 2015-12-31   \n",
       " \n",
       "                                                   emot      mean  \n",
       " 0                                          [[0, 0, 0]]  0.000000  \n",
       " 1                                                  NaN       NaN  \n",
       " 2                                                  NaN       NaN  \n",
       " 3                                             [[1, 0]]  0.500000  \n",
       " 4                                                [[0]]  0.000000  \n",
       " 5                                             [[0, 0]]  0.000000  \n",
       " 6                                                  NaN       NaN  \n",
       " 7                                                  NaN       NaN  \n",
       " 8                                                [[0]]  0.000000  \n",
       " 9                                                  NaN       NaN  \n",
       " 10                                               [[0]]  0.000000  \n",
       " 11                                               [[1]]  1.000000  \n",
       " 12                                                 NaN       NaN  \n",
       " 13                                                 NaN       NaN  \n",
       " 14                                                 NaN       NaN  \n",
       " 15                                                 NaN       NaN  \n",
       " 16                                               [[1]]  1.000000  \n",
       " 17                                               [[0]]  0.000000  \n",
       " 18                                                 NaN       NaN  \n",
       " 19                                                 NaN       NaN  \n",
       " 20                                   [[0, 0, 0, 0, 0]]  0.000000  \n",
       " 21                                               [[1]]  1.000000  \n",
       " 22                                                 NaN       NaN  \n",
       " 23                                            [[0, 0]]  0.000000  \n",
       " 24                                                 NaN       NaN  \n",
       " 25                                                 NaN       NaN  \n",
       " 26                                                 NaN       NaN  \n",
       " 27                                                 NaN       NaN  \n",
       " 28                                                 NaN       NaN  \n",
       " 29                                                 NaN       NaN  \n",
       " ..                                                 ...       ...  \n",
       " 336                                                NaN       NaN  \n",
       " 337                                                NaN       NaN  \n",
       " 338                                              [[1]]  1.000000  \n",
       " 339                                              [[0]]  0.000000  \n",
       " 340                                              [[1]]  1.000000  \n",
       " 341                                              [[0]]  0.000000  \n",
       " 342                                              [[0]]  0.000000  \n",
       " 343                                              [[0]]  0.000000  \n",
       " 344                                                NaN       NaN  \n",
       " 345                                  [[0, 0, 0, 1, 0]]  0.200000  \n",
       " 346                                              [[1]]  1.000000  \n",
       " 347                                              [[1]]  1.000000  \n",
       " 348                                              [[0]]  0.000000  \n",
       " 349  [[0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,...  0.333333  \n",
       " 350                                           [[0, 0]]  0.000000  \n",
       " 351                                                NaN       NaN  \n",
       " 352                                              [[0]]  0.000000  \n",
       " 353                                           [[0, 0]]  0.000000  \n",
       " 354                                              [[0]]  0.000000  \n",
       " 355                                        [[0, 0, 0]]  0.000000  \n",
       " 356                                              [[1]]  1.000000  \n",
       " 357                                              [[0]]  0.000000  \n",
       " 358                                                NaN       NaN  \n",
       " 359                                                NaN       NaN  \n",
       " 360                                           [[0, 0]]  0.000000  \n",
       " 361                                                NaN       NaN  \n",
       " 362                                              [[1]]  1.000000  \n",
       " 363                                                NaN       NaN  \n",
       " 364                                                NaN       NaN  \n",
       " 365                                           [[0, 1]]  0.500000  \n",
       " \n",
       " [366 rows x 4 columns],\n",
       " 'HON_2017':                                                  Tweet       Date  \\\n",
       " 0    [train babcock advisors llc maintains stake in... 2017-01-01   \n",
       " 1    [expect big changes for nasa under trump url a... 2017-01-02   \n",
       " 2    [teflon hon arshon the don just a few names i ... 2017-01-03   \n",
       " 3    [norinchukin bank the buys 587 shares of honey... 2017-01-04   \n",
       " 4    [hon honeywell international inc long term ros... 2017-01-05   \n",
       " 5    [co s that have issued guidance in the last 3 ... 2017-01-06   \n",
       " 6    [our recorded webinar session from earlier url... 2017-01-07   \n",
       " 7                                                   [] 2017-01-08   \n",
       " 8                                                   [] 2017-01-09   \n",
       " 9                                                   [] 2017-01-10   \n",
       " 10                                                  [] 2017-01-11   \n",
       " 11                                                  [] 2017-01-12   \n",
       " 12                                                  [] 2017-01-13   \n",
       " 13                                                  [] 2017-01-14   \n",
       " 14   [possible breakout plays to watch next week ur... 2017-01-15   \n",
       " 15   [keep an eye on hon may want more highs, join ... 2017-01-16   \n",
       " 16                                                  [] 2017-01-17   \n",
       " 17   [long setups bdx hum cmi hd hon cme mchp adbe ... 2017-01-18   \n",
       " 18   [david cote reveals what it takes to be ceo of... 2017-01-19   \n",
       " 19                                                  [] 2017-01-20   \n",
       " 20                                                  [] 2017-01-21   \n",
       " 21                                                  [] 2017-01-22   \n",
       " 22   [upcoming earnings tue baba vz jnj mmm wed t q... 2017-01-23   \n",
       " 23                                                  [] 2017-01-24   \n",
       " 24                                                  [] 2017-01-25   \n",
       " 25   [friday s earnings releases abbv aal cvx gd ho... 2017-01-26   \n",
       " 26   [on the fly top stock stories for friday googl... 2017-01-27   \n",
       " 27   [boy that hon call was better than expected re... 2017-01-28   \n",
       " 28                                                  [] 2017-01-29   \n",
       " 29   [upcoming earnings tue xom s amd aapl pfe uaa ... 2017-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 336  [true it was a shot from the hip on my end the... 2017-12-03   \n",
       " 337                                                 [] 2017-12-04   \n",
       " 338  [aapl dis gntx has hon laz msft sbux txn ups v... 2017-12-05   \n",
       " 339                                                 [] 2017-12-06   \n",
       " 340  [hon to acquire a 25 percent ownership interes... 2017-12-07   \n",
       " 341  [yuge yule stock buyback ceo bonus season cont... 2017-12-08   \n",
       " 342                                                 [] 2017-12-09   \n",
       " 343  [dragonfly capital 5 trade ideas for monday co... 2017-12-10   \n",
       " 344  [5 trade ideas for monday cop ggg hon pvh unh ... 2017-12-11   \n",
       " 345  [earnings w implied move pm arwr 9 civi 7 mdb ... 2017-12-12   \n",
       " 346  [see what ge does and do the opposite url via ... 2017-12-13   \n",
       " 347  [hon receives buy rating from oppenheimer new ... 2017-12-14   \n",
       " 348  [want automatic email alerts for tntr hon cpah... 2017-12-15   \n",
       " 349  [what stockmarket bubble 2 recent stock buybac... 2017-12-16   \n",
       " 350                                                 [] 2017-12-17   \n",
       " 351  [oppenheimer lowers honeywell international fy... 2017-12-18   \n",
       " 352  [3 low volatility etfs for your portfolio splv... 2017-12-19   \n",
       " 353  [i highly recommend trying this out for findin... 2017-12-20   \n",
       " 354  [money flows buying on weakness among stocks t... 2017-12-21   \n",
       " 355  [new post on hon url user, hon url 275 profits... 2017-12-22   \n",
       " 356  [via ihs markit in 2 1 1 of homes had connecte... 2017-12-23   \n",
       " 357  [honeywell hon reiterated buy and price target... 2017-12-24   \n",
       " 358                                                 [] 2017-12-25   \n",
       " 359  [honeywell international inc hon receives cons... 2017-12-26   \n",
       " 360  [if corporations don t take advantage of the n... 2017-12-27   \n",
       " 361                                                 [] 2017-12-28   \n",
       " 362  [big call buyers url pypl ampe mar chk fit gs ... 2017-12-29   \n",
       " 363  [industrial select sector spdr fund experience... 2017-12-30   \n",
       " 364                                                 [] 2017-12-31   \n",
       " 365  [papp l roy associates reduces stake in honeyw... 2016-12-31   \n",
       " \n",
       "                                        emot      mean  \n",
       " 0                               [[0, 0, 0]]  0.000000  \n",
       " 1                               [[0, 0, 0]]  0.000000  \n",
       " 2                               [[0, 0, 0]]  0.000000  \n",
       " 3                               [[0, 1, 0]]  0.333333  \n",
       " 4                                  [[0, 0]]  0.000000  \n",
       " 5                         [[1, 1, 0, 0, 0]]  0.400000  \n",
       " 6                                     [[0]]  0.000000  \n",
       " 7                                       NaN       NaN  \n",
       " 8                                       NaN       NaN  \n",
       " 9                                       NaN       NaN  \n",
       " 10                                      NaN       NaN  \n",
       " 11                                      NaN       NaN  \n",
       " 12                                      NaN       NaN  \n",
       " 13                                      NaN       NaN  \n",
       " 14                                    [[0]]  0.000000  \n",
       " 15                                 [[0, 0]]  0.000000  \n",
       " 16                                      NaN       NaN  \n",
       " 17                                    [[1]]  1.000000  \n",
       " 18                                 [[0, 0]]  0.000000  \n",
       " 19                                      NaN       NaN  \n",
       " 20                                      NaN       NaN  \n",
       " 21                                      NaN       NaN  \n",
       " 22                              [[0, 0, 0]]  0.000000  \n",
       " 23                                      NaN       NaN  \n",
       " 24                                      NaN       NaN  \n",
       " 25                                 [[0, 0]]  0.000000  \n",
       " 26                        [[0, 0, 0, 0, 0]]  0.000000  \n",
       " 27                                    [[0]]  0.000000  \n",
       " 28                                      NaN       NaN  \n",
       " 29                                 [[1, 0]]  0.500000  \n",
       " ..                                      ...       ...  \n",
       " 336                                   [[1]]  1.000000  \n",
       " 337                                     NaN       NaN  \n",
       " 338                                   [[0]]  0.000000  \n",
       " 339                                     NaN       NaN  \n",
       " 340                                   [[0]]  0.000000  \n",
       " 341                 [[0, 0, 1, 1, 0, 0, 0]]  0.285714  \n",
       " 342                                     NaN       NaN  \n",
       " 343                                [[0, 1]]  0.500000  \n",
       " 344                          [[0, 0, 0, 0]]  0.000000  \n",
       " 345                                   [[0]]  0.000000  \n",
       " 346  [[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0]]  0.166667  \n",
       " 347                       [[0, 0, 0, 0, 0]]  0.000000  \n",
       " 348                                [[0, 0]]  0.000000  \n",
       " 349                                   [[0]]  0.000000  \n",
       " 350                                     NaN       NaN  \n",
       " 351                                [[0, 0]]  0.000000  \n",
       " 352                                [[0, 0]]  0.000000  \n",
       " 353                                [[0, 0]]  0.000000  \n",
       " 354                             [[0, 0, 0]]  0.000000  \n",
       " 355                                [[0, 0]]  0.000000  \n",
       " 356                                [[0, 0]]  0.000000  \n",
       " 357                                   [[0]]  0.000000  \n",
       " 358                                     NaN       NaN  \n",
       " 359                                [[0, 1]]  0.500000  \n",
       " 360                                [[0, 0]]  0.000000  \n",
       " 361                                     NaN       NaN  \n",
       " 362                                   [[0]]  0.000000  \n",
       " 363                                   [[0]]  0.000000  \n",
       " 364                                     NaN       NaN  \n",
       " 365                                   [[0]]  0.000000  \n",
       " \n",
       " [366 rows x 4 columns],\n",
       " 'HON_2018':                                                  Tweet       Date  \\\n",
       " 0                                                   [] 2018-01-01   \n",
       " 1                                                   [] 2018-01-02   \n",
       " 2    [covered quite a bit in today s user daily run... 2018-01-03   \n",
       " 3    [top 1 position 7 32 vlo 6 65 t 6 62 sna 6 4 a... 2018-01-04   \n",
       " 4    [the latest update to the r i p portfolio bac ... 2018-01-05   \n",
       " 5                                                   [] 2018-01-06   \n",
       " 6    [one hell of a nice breakout setup in hon rt i... 2018-01-07   \n",
       " 7    [honeywell hon settled 5 at 154 74 much like a... 2018-01-08   \n",
       " 8    [hon daily chart could be one to watch for a p... 2018-01-09   \n",
       " 9    [doing scans just to keep my skills sharp here... 2018-01-10   \n",
       " 10   [wall street breakfast ceos warn of daca hit t... 2018-01-11   \n",
       " 11   [user all star trader labu atvi hon nvda mscc ... 2018-01-12   \n",
       " 12                                                  [] 2018-01-13   \n",
       " 13                                                  [] 2018-01-14   \n",
       " 14                                                  [] 2018-01-15   \n",
       " 15                                                  [] 2018-01-16   \n",
       " 16                                                  [] 2018-01-17   \n",
       " 17                                                  [] 2018-01-18   \n",
       " 18                                                  [] 2018-01-19   \n",
       " 19                                                  [] 2018-01-20   \n",
       " 20                                                  [] 2018-01-21   \n",
       " 21                                                  [] 2018-01-22   \n",
       " 22                                                  [] 2018-01-23   \n",
       " 23                                                  [] 2018-01-24   \n",
       " 24                                                  [] 2018-01-25   \n",
       " 25                                                  [] 2018-01-26   \n",
       " 26                                                  [] 2018-01-27   \n",
       " 27                                                  [] 2018-01-28   \n",
       " 28                                                  [] 2018-01-29   \n",
       " 29                                                  [] 2018-01-30   \n",
       " ..                                                 ...        ...   \n",
       " 336  [top 1 practice positions by 9 13 cof p 8 55 a... 2018-12-03   \n",
       " 337  [any thoughts on buying back some adbe and hon... 2018-12-04   \n",
       " 338              [hon honeywell excellent entry point] 2018-12-05   \n",
       " 339                                                 [] 2018-12-06   \n",
       " 340                                                 [] 2018-12-07   \n",
       " 341  [the investing secrets of the richest man the ... 2018-12-08   \n",
       " 342                                                 [] 2018-12-09   \n",
       " 343  [some head and shoulder set ups url sail hon f... 2018-12-10   \n",
       " 344  [v hd duk brk b hon yelp t jnj hig jpm xpo luv... 2018-12-11   \n",
       " 345  [user hi y n hon i am ulysses nice to meet you... 2018-12-12   \n",
       " 346  [ball is in your court pepsico pep executives ... 2018-12-13   \n",
       " 347                                                 [] 2018-12-14   \n",
       " 348                                                 [] 2018-12-15   \n",
       " 349                                                 [] 2018-12-16   \n",
       " 350  [mas lowered to 37 at nomura mnk lowered to 22... 2018-12-17   \n",
       " 351                                                 [] 2018-12-18   \n",
       " 352  [made 25k today thanks to the option trades at... 2018-12-19   \n",
       " 353  [these 11 stocks are set to rocket higher in 2... 2018-12-20   \n",
       " 354  [flight delayed honeywell is working on a fix ... 2018-12-21   \n",
       " 355  [long short bitcoin volatility with up to 1 x ... 2018-12-22   \n",
       " 356                                                 [] 2018-12-23   \n",
       " 357  [hon honeywell traded at 166 share in 1 2 18 d... 2018-12-24   \n",
       " 358  [hard to top hon gotti out here dropping class... 2018-12-25   \n",
       " 359  [christmas rally googl fb jpm c bac wfc aapl p... 2018-12-26   \n",
       " 360  [call put highest volume changes calls col avg... 2018-12-27   \n",
       " 361                                                 [] 2018-12-28   \n",
       " 362  [copy expert traders systematically using cryp... 2018-12-29   \n",
       " 363  [forget 3m ge is a better value stock user sto... 2018-12-30   \n",
       " 364                                                 [] 2018-12-31   \n",
       " 365  [hon honeywell international inc honeywell int... 2017-12-31   \n",
       " \n",
       "                emot      mean  \n",
       " 0               NaN       NaN  \n",
       " 1               NaN       NaN  \n",
       " 2          [[0, 0]]  0.000000  \n",
       " 3             [[1]]  1.000000  \n",
       " 4             [[0]]  0.000000  \n",
       " 5               NaN       NaN  \n",
       " 6             [[0]]  0.000000  \n",
       " 7             [[1]]  1.000000  \n",
       " 8    [[0, 0, 1, 1]]  0.500000  \n",
       " 9    [[0, 0, 1, 0]]  0.250000  \n",
       " 10         [[0, 0]]  0.000000  \n",
       " 11      [[0, 0, 0]]  0.000000  \n",
       " 12              NaN       NaN  \n",
       " 13              NaN       NaN  \n",
       " 14              NaN       NaN  \n",
       " 15              NaN       NaN  \n",
       " 16              NaN       NaN  \n",
       " 17              NaN       NaN  \n",
       " 18              NaN       NaN  \n",
       " 19              NaN       NaN  \n",
       " 20              NaN       NaN  \n",
       " 21              NaN       NaN  \n",
       " 22              NaN       NaN  \n",
       " 23              NaN       NaN  \n",
       " 24              NaN       NaN  \n",
       " 25              NaN       NaN  \n",
       " 26              NaN       NaN  \n",
       " 27              NaN       NaN  \n",
       " 28              NaN       NaN  \n",
       " 29              NaN       NaN  \n",
       " ..              ...       ...  \n",
       " 336        [[1, 0]]  0.500000  \n",
       " 337  [[0, 0, 0, 0]]  0.000000  \n",
       " 338           [[0]]  0.000000  \n",
       " 339             NaN       NaN  \n",
       " 340             NaN       NaN  \n",
       " 341           [[0]]  0.000000  \n",
       " 342             NaN       NaN  \n",
       " 343           [[1]]  1.000000  \n",
       " 344           [[0]]  0.000000  \n",
       " 345           [[0]]  0.000000  \n",
       " 346           [[0]]  0.000000  \n",
       " 347             NaN       NaN  \n",
       " 348             NaN       NaN  \n",
       " 349             NaN       NaN  \n",
       " 350        [[1, 1]]  1.000000  \n",
       " 351             NaN       NaN  \n",
       " 352           [[0]]  0.000000  \n",
       " 353        [[0, 0]]  0.000000  \n",
       " 354     [[1, 0, 0]]  0.333333  \n",
       " 355  [[1, 0, 0, 0]]  0.250000  \n",
       " 356             NaN       NaN  \n",
       " 357        [[0, 0]]  0.000000  \n",
       " 358           [[0]]  0.000000  \n",
       " 359     [[0, 0, 0]]  0.000000  \n",
       " 360     [[0, 0, 0]]  0.000000  \n",
       " 361             NaN       NaN  \n",
       " 362        [[0, 0]]  0.000000  \n",
       " 363        [[0, 0]]  0.000000  \n",
       " 364             NaN       NaN  \n",
       " 365           [[0]]  0.000000  \n",
       " \n",
       " [366 rows x 4 columns]}"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_stack = {}\n",
    "for name, val in final_proc.items():\n",
    "    frames = []\n",
    "    for key in data_names:\n",
    "        frames.append(key)\n",
    "#     frames.sort()\n",
    "# #     print(name, frames)\n",
    "    stacked = [val.groupby(final_proc[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[1])) for t in frames]\n",
    "    company_stack[name.split(\"_\")[0]] = pd.concat(stacked)\n",
    "    company_stack[name.split(\"_\")[0]].reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in final_proc[\"HON_2016\"].iterrows():\n",
    "    if(len(v) > 364):\n",
    "        v.drop([364])\n",
    "for i, v in final_proc[\"HON_2017\"].iterrows():\n",
    "    if(len(v) > 364):\n",
    "        v.drop([364])\n",
    "for i, v in final_proc[\"HON_2018\"].iterrows():\n",
    "    if(len(v) > 364):\n",
    "        v.drop([364])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from fancyimpute import KNN\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "for key, value in copy.items():\n",
      "    completed[key] = KNN(k=10).complete(value)\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "data_names = []\n",
      "file_names = []\n",
      "companies = [\"MMM\", \"SYF\", \"BAYRY\", \"HON\"]\n",
      "for w in companies:\n",
      "    for i in np.arange(2009,2019):\n",
      "        file_names.append(w+\"_\"+str(i)+\".csv\")\n",
      "        data_names.append(w+\"_\"+str(i))\n",
      "mmm = pd.read_pickle('3m_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "hon = pd.read_pickle('honeywell_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "syf = pd.read_pickle('synchrony_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "bayry = pd.read_pickle('bayer_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "mmm[\"Date\"] = fix_date(mmm)\n",
      "hon[\"Date\"] = fix_date(hon)\n",
      "syf[\"Date\"] = fix_date(syf)\n",
      "bayry[\"Date\"] = fix_date(bayry)\n",
      "\n",
      "\n",
      "def process_pickles(company_name, frame, dict_to_add):\n",
      "    temp_dict = {}\n",
      "    frame[\"Year\"] = frame[\"Date\"].apply(lambda x: x.split(\" \")[2])\n",
      "    years = frame[\"Year\"].unique()\n",
      "    for i in years:\n",
      "        name = company_name+\"_\"+i\n",
      "        temp = frame.groupby(['Year']).get_group(str(i))\n",
      "        if int(i) != 2019:\n",
      "            dict_to_add[name].append(temp[[\"Date\", \"Tweet\"]])\n",
      "#         else: \n",
      "#             dict_to_add[name] = (temp[[\"Date\", \"Tweet\"]])\n",
      "    return dict_to_add\n",
      "\n",
      "\n",
      "file_dict = {}\n",
      "for file, name in zip(file_names, data_names):\n",
      "    file_dict[name] = pd.read_csv(file, names = [\"Date\", \"Tweet\"])\n",
      "    \n",
      "file_dict = process_pickles(\"MMM\", mmm, file_dict)\n",
      "file_dict = process_pickles(\"HON\", hon, file_dict)\n",
      "file_dict = process_pickles(\"SYF\", syn, file_dict)\n",
      "file_dict = process_pickles(\"BAYRY\", bayry, file_dict)\n",
      "\n",
      "\n",
      "\n",
      "def fix_date(df): \n",
      "    dx = df[\"Date\"].apply(lambda x: x.split(\" \"))\n",
      "    dx = dx.apply(lambda x: \"{} {} {}\".format(x[1][:-3], x[0], x[2])) \n",
      "    return dx\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def fix_date(df): \n",
      "    dx = df[\"Date\"].apply(lambda x: x.split(\" \"))\n",
      "    dx = dx.apply(lambda x: \"{} {} {}\".format(x[1][:-3], x[0], x[2])) \n",
      "    return dx \n",
      "\n",
      "data_names = []\n",
      "file_names = []\n",
      "companies = [\"MMM\", \"SYF\", \"BAYRY\", \"HON\"]\n",
      "for w in companies:\n",
      "    for i in np.arange(2009,2019):\n",
      "        file_names.append(w+\"_\"+str(i)+\".csv\")\n",
      "        data_names.append(w+\"_\"+str(i))\n",
      "mmm = pd.read_pickle('3m_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "hon = pd.read_pickle('honeywell_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "syf = pd.read_pickle('synchrony_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "bayry = pd.read_pickle('bayer_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "mmm[\"Date\"] = fix_date(mmm)\n",
      "hon[\"Date\"] = fix_date(hon)\n",
      "syf[\"Date\"] = fix_date(syf)\n",
      "bayry[\"Date\"] = fix_date(bayry)\n",
      "\n",
      "\n",
      "def process_pickles(company_name, frame, dict_to_add):\n",
      "    temp_dict = {}\n",
      "    frame[\"Year\"] = frame[\"Date\"].apply(lambda x: x.split(\" \")[2])\n",
      "    years = frame[\"Year\"].unique()\n",
      "    for i in years:\n",
      "        name = company_name+\"_\"+i\n",
      "        temp = frame.groupby(['Year']).get_group(str(i))\n",
      "        if int(i) != 2019:\n",
      "            dict_to_add[name].append(temp[[\"Date\", \"Tweet\"]])\n",
      "#         else: \n",
      "#             dict_to_add[name] = (temp[[\"Date\", \"Tweet\"]])\n",
      "    return dict_to_add\n",
      "\n",
      "\n",
      "file_dict = {}\n",
      "for file, name in zip(file_names, data_names):\n",
      "    file_dict[name] = pd.read_csv(file, names = [\"Date\", \"Tweet\"])\n",
      "    \n",
      "file_dict = process_pickles(\"MMM\", mmm, file_dict)\n",
      "file_dict = process_pickles(\"HON\", hon, file_dict)\n",
      "file_dict = process_pickles(\"SYF\", syn, file_dict)\n",
      "file_dict = process_pickles(\"BAYRY\", bayry, file_dict)\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def fix_date(df): \n",
      "    dx = df[\"Date\"].apply(lambda x: x.split(\" \"))\n",
      "    dx = dx.apply(lambda x: \"{} {} {}\".format(x[1][:-3], x[0], x[2])) \n",
      "    return dx \n",
      "\n",
      "data_names = []\n",
      "file_names = []\n",
      "companies = [\"MMM\", \"SYF\", \"BAYRY\", \"HON\"]\n",
      "for w in companies:\n",
      "    for i in np.arange(2009,2019):\n",
      "        file_names.append(w+\"_\"+str(i)+\".csv\")\n",
      "        data_names.append(w+\"_\"+str(i))\n",
      "mmm = pd.read_pickle('3m_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "hon = pd.read_pickle('honeywell_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "syf = pd.read_pickle('synchrony_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "bayry = pd.read_pickle('bayer_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "mmm[\"Date\"] = fix_date(mmm)\n",
      "hon[\"Date\"] = fix_date(hon)\n",
      "syf[\"Date\"] = fix_date(syf)\n",
      "bayry[\"Date\"] = fix_date(bayry)\n",
      "\n",
      "\n",
      "def process_pickles(company_name, frame, dict_to_add):\n",
      "    temp_dict = {}\n",
      "    frame[\"Year\"] = frame[\"Date\"].apply(lambda x: x.split(\" \")[2])\n",
      "    years = frame[\"Year\"].unique()\n",
      "    for i in years:\n",
      "        name = company_name+\"_\"+i\n",
      "        temp = frame.groupby(['Year']).get_group(str(i))\n",
      "        if int(i) != 2019:\n",
      "            dict_to_add[name].append(temp[[\"Date\", \"Tweet\"]])\n",
      "#         else: \n",
      "#             dict_to_add[name] = (temp[[\"Date\", \"Tweet\"]])\n",
      "    return dict_to_add\n",
      "\n",
      "\n",
      "file_dict = {}\n",
      "for file, name in zip(file_names, data_names):\n",
      "    file_dict[name] = pd.read_csv(file, names = [\"Date\", \"Tweet\"])\n",
      "    \n",
      "file_dict = process_pickles(\"MMM\", mmm, file_dict)\n",
      "file_dict = process_pickles(\"HON\", hon, file_dict)\n",
      "file_dict = process_pickles(\"SYF\", syn, file_dict)\n",
      "file_dict = process_pickles(\"BAYRY\", bayry, file_dict)\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def fix_date(df): \n",
      "    dx = df[\"Date\"].apply(lambda x: x.split(\" \"))\n",
      "    dx = dx.apply(lambda x: \"{} {} {}\".format(x[1][:-3], x[0], x[2])) \n",
      "    return dx \n",
      "\n",
      "data_names = []\n",
      "file_names = []\n",
      "companies = [\"MMM\", \"SYF\", \"BAYRY\", \"HON\"]\n",
      "for w in companies:\n",
      "    for i in np.arange(2009,2019):\n",
      "        file_names.append(w+\"_\"+str(i)+\".csv\")\n",
      "        data_names.append(w+\"_\"+str(i))\n",
      "mmm = pd.read_pickle('3m_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "hon = pd.read_pickle('honeywell_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "syf = pd.read_pickle('synchrony_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "bayry = pd.read_pickle('bayer_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "mmm[\"Date\"] = fix_date(mmm)\n",
      "hon[\"Date\"] = fix_date(hon)\n",
      "syf[\"Date\"] = fix_date(syf)\n",
      "bayry[\"Date\"] = fix_date(bayry)\n",
      "\n",
      "\n",
      "def process_pickles(company_name, frame, dict_to_add):\n",
      "    temp_dict = {}\n",
      "    frame[\"Year\"] = frame[\"Date\"].apply(lambda x: x.split(\" \")[2])\n",
      "    years = frame[\"Year\"].unique()\n",
      "    for i in years:\n",
      "        name = company_name+\"_\"+i\n",
      "        temp = frame.groupby(['Year']).get_group(str(i))\n",
      "        if int(i) != 2019:\n",
      "            dict_to_add[name].append(temp[[\"Date\", \"Tweet\"]])\n",
      "#         else: \n",
      "#             dict_to_add[name] = (temp[[\"Date\", \"Tweet\"]])\n",
      "    return dict_to_add\n",
      "\n",
      "\n",
      "file_dict = {}\n",
      "for file, name in zip(file_names, data_names):\n",
      "    file_dict[name] = pd.read_csv(file, names = [\"Date\", \"Tweet\"])\n",
      "    \n",
      "file_dict = process_pickles(\"MMM\", mmm, file_dict)\n",
      "file_dict = process_pickles(\"HON\", hon, file_dict)\n",
      "file_dict = process_pickles(\"SYF\", syf, file_dict)\n",
      "file_dict = process_pickles(\"BAYRY\", bayry, file_dict)\n",
      "pre_proc = {}\n",
      "\n",
      "count = 1;\n",
      "for key, value in file_dict.items():\n",
      "    temp_dict = {}\n",
      "    temp_frame = pd.DataFrame(columns = [\"Tweet\"])\n",
      "    for index, row in value.iterrows():\n",
      "        temp_dict.setdefault(row['Date'], [])\n",
      "        temp_dict[row['Date']].append(row['Tweet'])\n",
      "                              \n",
      "    for keys, tweet in temp_dict.items():\n",
      "        temp_frame.loc[keys] = [tweet]\n",
      "    temp_frame = temp_frame.reset_index()\n",
      "    temp_frame.rename(columns = {\"index\": \"Date\", \"Tweet\":\"Tweet\"}, inplace = True)\n",
      "    temp_frame.sort_values(by=['Date'])\n",
      "    pre_proc[key] = temp_frame\n",
      "dates = np.arange(1,32)\n",
      "feb = np.arange(1, 29)\n",
      "months = {\"Jan\": 31, \"Feb\":28, \"Mar\":31, \"Apr\":30, \"May\":31, \"Jun\":30, \"Jul\":31, \"Aug\":31, \"Sep\":30, \"Oct\": 31, \"Nov\":30, \"Dec\":31}\n",
      "index_dict = {}\n",
      "for k, v in months.items():\n",
      "    for j in range(1,v+1):\n",
      "            key = str(j) + \" \" + k\n",
      "            index_dict[key] = len(index_dict.keys())+1\n",
      "\n",
      "def make_df(stack, key):\n",
      "    temp = {}\n",
      "    year = key.split(\"_\")[-1]\n",
      "    data = [(t+\" \"+str(year)) for t in index_dict.keys()]\n",
      "    for i in data: \n",
      "        temp[i] = \"\"\n",
      "    return temp\n",
      "\n",
      "final_proc = {}\n",
      "for k, v in pre_proc.items():\n",
      "    temp = make_df(v, k)\n",
      "    for indx, val in v.iterrows():\n",
      "        temp[val[\"Date\"]] = val[\"Tweet\"]\n",
      "    out = pd.DataFrame(columns = {\"Date\", \"Tweet\"})\n",
      "    out[\"Date\"] = temp.keys()\n",
      "    out[\"Tweet\"] = list(temp.values())\n",
      "    final_proc[k] = out\n",
      "def convert_dates(stack):\n",
      "    for i, v in stack.items():\n",
      "        v['Date'] = pd.to_datetime(v['Date'], dayfirst = True)\n",
      "    return stack\n",
      "import re\n",
      "def preprocess_text(text):\n",
      "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', text)\n",
      "    text = re.sub('@[^\\s]+','USER', text)\n",
      "    text = text.lower().replace(\"ё\", \"е\")\n",
      "    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
      "    text = re.sub(' +',' ', text)\n",
      "    return text.strip()\n",
      "\n",
      "for key, value in final_proc.items():\n",
      "    for idx, txt in value.iterrows():\n",
      "        txt[\"Tweet\"] = [preprocess_text(t) for t in txt[\"Tweet\"]]\n",
      "\n",
      "final_proc = convert_dates(final_proc)\n",
      "# Read the data from CSV files\n",
      "n = ['ItemID', 'Sentiment', 'SentimentSource', 'SentimentText']\n",
      "raw_data = pd.read_csv('Sentiment Analysis Dataset 2.csv', names=n, header = 0, usecols=['Sentiment', 'SentimentText'])\n",
      "neg, pos = raw_data.groupby('Sentiment')\n",
      "neg_c = raw_data.Sentiment.value_counts()\n",
      "sample_size = int(min(neg_c[0], neg_c[1]))\n",
      "raw_data = np.concatenate((neg[1].values[:sample_size], pos[1].values[:sample_size]), axis=0)\n",
      "labels = [1]*sample_size + [0]*sample_size\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
      "from sklearn.model_selection import train_test_split, GridSearchCV\n",
      "\n",
      "text_clf = Pipeline([('vect', CountVectorizer()),\n",
      "                     ('tfidf', TfidfTransformer()),\n",
      "                     ('clf', MultinomialNB())])\n",
      "tuned_parameters = {\n",
      "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
      "    'tfidf__use_idf': (True, False),\n",
      "    'tfidf__norm': ('l1', 'l2'),\n",
      "    'clf__alpha': [1, 1e-1, 1e-2]\n",
      "}\n",
      "data = [preprocess_text(t[1]) for t in raw_data]\n",
      "from sklearn.metrics import classification_report\n",
      "clf = GridSearchCV(text_clf, tuned_parameters, cv=3, verbose = 100)\n",
      "clf.fit(x_train, y_train)\n",
      "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.33, random_state=42)\n",
      "print(len(x_test))\n",
      "from sklearn.metrics import classification_report\n",
      "clf = GridSearchCV(text_clf, tuned_parameters, cv=3, verbose = 100)\n",
      "clf.fit(x_train, y_train)\n",
      "print(classification_report(y_test, clf.predict(x_test), digits=4))\n",
      "import pickle\n",
      "model = \"grid.sav\"\n",
      "pickle.dump(clf, open(modle, \"wb\"))\n",
      "import pickle\n",
      "model = \"grid.sav\"\n",
      "pickle.dump(clf, open(model, \"wb\"))\n",
      "for i, v in complex_stack.items:\n",
      "    if(v.iloc[364]):\n",
      "        v.drop([364])\n",
      "        print(len(v), i)\n",
      "from scipy import stats\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "for i, v in final_proc.items():\n",
      "    emot = []\n",
      "    for index, val in v.iterrows():\n",
      "        if len(val[\"Tweet\"]) != 0:\n",
      "            emot.append([clf.predict(t) for t in [val[\"Tweet\"]]])\n",
      "        else:\n",
      "            emot.append(np.NaN)\n",
      "    v[\"emot\"] = emot\n",
      "    v[\"mean\"] = [np.mean(t) for t in emot]\n",
      "company_stack = {}\n",
      "for name in [\"HON\"]:\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        if(name == (key.split(\"_\")[0])):\n",
      "#             print(key)\n",
      "            frames.append(key)\n",
      "    frames.sort()\n",
      "    print(name, frames)\n",
      "    stacked = [final_proc[t].groupby(final_proc[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[-1])) for t in frames]\n",
      "    company_stack[name] = pd.concat(stacked)\n",
      "    company_stack[name].reset_index(inplace = True)\n",
      "for i, v in complex_stack.items:\n",
      "    if(v.iloc[364]):\n",
      "        v.drop([364])\n",
      "        print(len(v), i)\n",
      "for i, v in company_stack.items:\n",
      "    if(v.iloc[364]):\n",
      "        v.drop([364])\n",
      "        print(len(v), i)\n",
      "for i, v in company_stack.items():\n",
      "    if(v.iloc[364]):\n",
      "        v.drop([364])\n",
      "        print(len(v), i)\n",
      "for i, v in company_stack.items():\n",
      "#     if(v.iloc[364]):\n",
      "#         v.drop([364])\n",
      "    print(len(v), i)\n",
      "for i, v in final_proc.items():\n",
      "#     if(v.iloc[364]):\n",
      "#         v.drop([364])\n",
      "    print(len(v), i)\n",
      "for i, v in final_proc.items():\n",
      "    if(v.iloc[364]):\n",
      "        v.drop([364])\n",
      "    print(len(v), i)\n",
      "for i, v in final_proc.items():\n",
      "    if(len(v) >365):\n",
      "        v.drop([365])\n",
      "    print(len(v), i)\n",
      "for i, v in final_proc.items():\n",
      "    if(len(v) >= 365):\n",
      "        v.drop([365])\n",
      "    print(len(v), i)\n",
      "for i, v in final_proc.items():\n",
      "    if(len(v) >= 365):\n",
      "        v.drop(365)\n",
      "    print(len(v), i)\n",
      "for i, v in final_proc.items():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "    print(len(v), i)\n",
      "for i, v in final_proc.items():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "    print(len(v), i)\n",
      "for i, v in final_proc.items():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([364])\n",
      "    print(len(v), i)\n",
      "for i, v in final_proc[\"HON_2016\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([364])\n",
      "    print(len(v), i)\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def fix_date(df): \n",
      "    dx = df[\"Date\"].apply(lambda x: x.split(\" \"))\n",
      "    dx = dx.apply(lambda x: \"{} {} {}\".format(x[1][:-3], x[0], x[2])) \n",
      "    return dx \n",
      "\n",
      "data_names = []\n",
      "file_names = []\n",
      "companies = [\"MMM\", \"SYF\", \"BAYRY\", \"HON\"]\n",
      "for w in companies:\n",
      "    for i in np.arange(2009,2019):\n",
      "        file_names.append(w+\"_\"+str(i)+\".csv\")\n",
      "        data_names.append(w+\"_\"+str(i))\n",
      "mmm = pd.read_pickle('3m_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "hon = pd.read_pickle('honeywell_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "syf = pd.read_pickle('synchrony_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "bayry = pd.read_pickle('bayer_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "mmm[\"Date\"] = fix_date(mmm)\n",
      "hon[\"Date\"] = fix_date(hon)\n",
      "syf[\"Date\"] = fix_date(syf)\n",
      "bayry[\"Date\"] = fix_date(bayry)\n",
      "\n",
      "\n",
      "def process_pickles(company_name, frame, dict_to_add):\n",
      "    temp_dict = {}\n",
      "    frame[\"Year\"] = frame[\"Date\"].apply(lambda x: x.split(\" \")[2])\n",
      "    years = frame[\"Year\"].unique()\n",
      "    for i in years:\n",
      "        name = company_name+\"_\"+i\n",
      "        temp = frame.groupby(['Year']).get_group(str(i))\n",
      "        if int(i) != 2019:\n",
      "            dict_to_add[name].append(temp[[\"Date\", \"Tweet\"]])\n",
      "#         else: \n",
      "#             dict_to_add[name] = (temp[[\"Date\", \"Tweet\"]])\n",
      "    return dict_to_add\n",
      "\n",
      "\n",
      "file_dict = {}\n",
      "for file, name in zip(file_names, data_names):\n",
      "    file_dict[name] = pd.read_csv(file, names = [\"Date\", \"Tweet\"])\n",
      "    \n",
      "file_dict = process_pickles(\"MMM\", mmm, file_dict)\n",
      "file_dict = process_pickles(\"HON\", hon, file_dict)\n",
      "file_dict = process_pickles(\"SYF\", syf, file_dict)\n",
      "file_dict = process_pickles(\"BAYRY\", bayry, file_dict)\n",
      "pre_proc = {}\n",
      "\n",
      "count = 1;\n",
      "for key, value in file_dict.items():\n",
      "    temp_dict = {}\n",
      "    temp_frame = pd.DataFrame(columns = [\"Tweet\"])\n",
      "    for index, row in value.iterrows():\n",
      "        temp_dict.setdefault(row['Date'], [])\n",
      "        temp_dict[row['Date']].append(row['Tweet'])\n",
      "                              \n",
      "    for keys, tweet in temp_dict.items():\n",
      "        temp_frame.loc[keys] = [tweet]\n",
      "    temp_frame = temp_frame.reset_index()\n",
      "    temp_frame.rename(columns = {\"index\": \"Date\", \"Tweet\":\"Tweet\"}, inplace = True)\n",
      "    temp_frame.sort_values(by=['Date'])\n",
      "    pre_proc[key] = temp_frame\n",
      "dates = np.arange(1,32)\n",
      "feb = np.arange(1, 29)\n",
      "months = {\"Jan\": 31, \"Feb\":28, \"Mar\":31, \"Apr\":30, \"May\":31, \"Jun\":30, \"Jul\":31, \"Aug\":31, \"Sep\":30, \"Oct\": 31, \"Nov\":30, \"Dec\":31}\n",
      "index_dict = {}\n",
      "for k, v in months.items():\n",
      "    for j in range(1,v+1):\n",
      "            key = str(j) + \" \" + k\n",
      "            index_dict[key] = len(index_dict.keys())+1\n",
      "\n",
      "def make_df(stack, key):\n",
      "    temp = {}\n",
      "    year = key.split(\"_\")[-1]\n",
      "    data = [(t+\" \"+str(year)) for t in index_dict.keys()]\n",
      "    for i in data: \n",
      "        temp[i] = \"\"\n",
      "    return temp\n",
      "\n",
      "final_proc = {}\n",
      "for k, v in pre_proc.items():\n",
      "    temp = make_df(v, k)\n",
      "    for indx, val in v.iterrows():\n",
      "        temp[val[\"Date\"]] = val[\"Tweet\"]\n",
      "    out = pd.DataFrame(columns = {\"Date\", \"Tweet\"})\n",
      "    out[\"Date\"] = temp.keys()\n",
      "    out[\"Tweet\"] = list(temp.values())\n",
      "    final_proc[k] = out\n",
      "import re\n",
      "def preprocess_text(text):\n",
      "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', text)\n",
      "    text = re.sub('@[^\\s]+','USER', text)\n",
      "    text = text.lower().replace(\"ё\", \"е\")\n",
      "    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
      "    text = re.sub(' +',' ', text)\n",
      "    return text.strip()\n",
      "\n",
      "for key, value in final_proc.items():\n",
      "    for idx, txt in value.iterrows():\n",
      "        txt[\"Tweet\"] = [preprocess_text(t) for t in txt[\"Tweet\"]]\n",
      "\n",
      "final_proc = convert_dates(final_proc)\n",
      "from scipy import stats\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "for i, v in final_proc.items():\n",
      "    emot = []\n",
      "    for index, val in v.iterrows():\n",
      "        if len(val[\"Tweet\"]) != 0:\n",
      "            emot.append([clf.predict(t) for t in [val[\"Tweet\"]]])\n",
      "        else:\n",
      "            emot.append(np.NaN)\n",
      "    v[\"emot\"] = emot\n",
      "    v[\"mean\"] = [np.mean(t) for t in emot]\n",
      "company_stack = {}\n",
      "for name in [\"HON\"]:\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        if(name == (key.split(\"_\")[0])):\n",
      "#             print(key)\n",
      "            frames.append(key)\n",
      "    frames.sort()\n",
      "    print(name, frames)\n",
      "    stacked = [final_proc[t].groupby(final_proc[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[-1])) for t in frames]\n",
      "    company_stack[name] = pd.concat(stacked)\n",
      "    company_stack[name].reset_index(inplace = True)\n",
      "for i, v in final_proc[\"HON_2016\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "for i, v in final_proc[\"HON_2017\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "for i, v in final_proc[\"HON_2018\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "from fancyimpute import KNN\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "for key, value in copy.items():\n",
      "    completed[key] = KNN(k=10).complete(value)\n",
      "from fancyimpute import KNN\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "for key, value in copy.items():\n",
      "    completed[key] = KNN(k=10).fit_transform(value)\n",
      "from fancyimpute import KNN\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "for key, value in copy.items():\n",
      "    completed[key] = KNN(k=10).fit_transform(value[\"Date\"])\n",
      "from fancyimpute import KNN\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "for key, value in copy.items():\n",
      "    completed[key] = KNN(k=10).fit_transform(value)\n",
      "from fancyimpute import KNN\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "for key, value in copy.items():\n",
      "    completed[key] = KNN(k=10).fit_transform(value[:])\n",
      "from fancyimpute import KNN\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "for key, value in copy.items():\n",
      "    value\n",
      "    completed[key] = KNN(k=10).fit_transform(value[:])\n",
      "from fancyimpute import KNN\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "for key, value in copy.items():\n",
      "    value\n",
      "#     completed[key] = KNN(k=10).fit_transform(value[:])\n",
      "from fancyimpute import KNN\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "for key, value in copy.items():\n",
      "    print(value)\n",
      "#     completed[key] = KNN(k=10).fit_transform(value[:])\n",
      "from fancyimpute import KNN\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "for key, value in copy.items():\n",
      "    completed[key] = KNN(k=10).fit_transform(value[[\"mean\", \"Date\"]])\n",
      "from fancyimpute import KNN\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "for key, value in copy.items():\n",
      "    completed[key] = KNN(k=10).fit_transform(value[[\"mean\"]])\n",
      "completed\n",
      "completed[\"HON\"]\n",
      "copy\n",
      "copy[\"HON\"]\n",
      "copy[\"HON\"][\"imputed_mean\"] = completed[\"HON\"]\n",
      "copy[\"HON\"][\"imputed_mean\"] = completed[\"HON\"]\n",
      "copy\n",
      "copy[\"HON\"][\"imputed_mean\"] = completed[\"HON\"]\n",
      "copy[\"HON\"]\n",
      "copy[\"HON\"][\"imputed_mean\"] = completed[\"HON\"]\n",
      "copy[\"HON\"][\"Date\"].groupby(copy[\"HON\"][\"Date\"].dt.year).get_group(2018)\n",
      "copy[\"HON\"][\"imputed_mean\"] = completed[\"HON\"]\n",
      "copy[\"HON\"][\"Date\"].groupby(copy[\"HON\"][\"Date\"].dt.year).get_group(2018).index.values\n",
      "copy[\"HON\"][\"imputed_mean\"] = completed[\"HON\"]\n",
      "copy[\"HON\"].groupby(copy[\"HON\"][\"Date\"].dt.year).get_group(2018).index.values\n",
      "copy[\"HON\"][\"imputed_mean\"] = completed[\"HON\"]\n",
      "copy[\"HON\"].groupby(copy[\"HON\"][\"Date\"].dt.year).get_group(2018)\n",
      "from fancyimpute import KNN\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "for key, value in copy.items():\n",
      "    completed[key] = KNN(k=14).fit_transform(value[[\"mean\"]])\n",
      "copy[\"HON\"][\"imputed_mean\"] = completed[\"HON\"]\n",
      "copy[\"HON\"].groupby(copy[\"HON\"][\"Date\"].dt.year).get_group(2018)\n",
      "copy[\"HON\"][\"imputed_mean\"] = completed[\"HON\"]\n",
      "chon = copy[\"HON\"]\n",
      "chon.groupby(chon[\"Date\"].dt.year).get_group(2018)[chon[\"mean\"].isnull()]\n",
      "copy[\"HON\"][\"imputed_mean\"] = completed[\"HON\"]\n",
      "chon = copy[\"HON\"]\n",
      "nan_impute = chon.groupby(chon[\"Date\"].dt.year).get_group(2018)[chon[\"mean\"].isnull()][\"imputed_mean\"]\n",
      "copy[\"HON\"][\"imputed_mean\"] = completed[\"HON\"]\n",
      "chon = copy[\"HON\"]\n",
      "nan_impute = chon.groupby(chon[\"Date\"].dt.year).get_group(2018)[chon[\"mean\"].isnull()][\"imputed_mean\"]\n",
      "nan_impute\n",
      "copy[\"HON\"][\"imputed_mean\"] = completed[\"HON\"]\n",
      "chon = copy[\"HON\"]\n",
      "nan_impute = chon.groupby(chon[\"Date\"].dt.year).get_group(2018)[chon[\"mean\"].isnull()][\"imputed_mean\"]\n",
      "np.range(nan_impute)\n",
      "copy[\"HON\"][\"imputed_mean\"] = completed[\"HON\"]\n",
      "chon = copy[\"HON\"]\n",
      "nan_impute = chon.groupby(chon[\"Date\"].dt.year).get_group(2018)[chon[\"mean\"].isnull()][\"imputed_mean\"]\n",
      "np.arange(nan_impute)\n",
      "copy[\"HON\"][\"imputed_mean\"] = completed[\"HON\"]\n",
      "chon = copy[\"HON\"]\n",
      "nan_impute = chon.groupby(chon[\"Date\"].dt.year).get_group(2018)[chon[\"mean\"].isnull()][\"imputed_mean\"]\n",
      "max(nan_impute)\n",
      "from fancyimpute import KNN\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "for key, value in copy.items():\n",
      "    completed[key] = KNN(k=3).fit_transform(value[[\"mean\"]])\n",
      "copy[\"HON\"][\"imputed_mean\"] = completed[\"HON\"]\n",
      "chon = copy[\"HON\"]\n",
      "nan_impute = chon.groupby(chon[\"Date\"].dt.year).get_group(2018)[chon[\"mean\"].isnull()][\"imputed_mean\"]\n",
      "copy[\"HON\"][\"imputed_mean\"] = completed[\"HON\"]\n",
      "chon = copy[\"HON\"]\n",
      "nan_impute = chon.groupby(chon[\"Date\"].dt.year).get_group(2018)[chon[\"mean\"].isnull()][\"imputed_mean\"]\n",
      "nan_impute\n",
      "from fancyimpute import KNN\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "for key, value in copy.items():\n",
      "    completed[key] = IterativeImputer().fit_transform(value[[\"mean\"]])\n",
      "from fancyimpute import KNN, IterativeImputer\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "for key, value in copy.items():\n",
      "    completed[key] = IterativeImputer().fit_transform(value[[\"mean\"]])\n",
      "from fancyimpute import KNN, IterativeImputer\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "for key, value in copy.items():\n",
      "    completed[key] = IterativeImputer().fit_transform(value)\n",
      "from fancyimpute import KNN, IterativeImputer\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "for key, value in copy.items():\n",
      "    completed[key] = IterativeImputer().fit_transform(value[\"mean\"])\n",
      "from fancyimpute import KNN, IterativeImputer\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "for key, value in copy.items():\n",
      "    completed[key] = IterativeImputer().fit_transform(value[[\"mean\"]])\n",
      "from fancyimpute import KNN, SoftImpute\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "for key, value in copy.items():\n",
      "    completed[key] = SoftImpute().fit_transform(value[[\"mean\"]])\n",
      "copy[\"HON\"][\"imputed_mean\"] = completed[\"HON\"]\n",
      "chon = copy[\"HON\"]\n",
      "nan_impute = chon.groupby(chon[\"Date\"].dt.year).get_group(2018)[chon[\"mean\"].isnull()][\"imputed_mean\"]\n",
      "nan_impute\n",
      "completed[\"HON\"]\n",
      "# copy[\"HON\"][\"imputed_mean\"] = completed[\"HON\"]\n",
      "# chon = copy[\"HON\"]\n",
      "# nan_impute = chon.groupby(chon[\"Date\"].dt.year).get_group(2018)[chon[\"mean\"].isnull()][\"imputed_mean\"]\n",
      "# nan_impute\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"interp\"].rolling(window=30).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=30).mean()\n",
      "noimp = go.Scatter(\n",
      "    x=temp[\"Date\"], \n",
      "    y=temp[\"MA_no_imputation\"],\n",
      "#     mode = 'lines',\n",
      "#     name = 'w/o imputation'\n",
      ")\n",
      "wimp = go.Scatter(\n",
      "    x = temp[\"Date\"],\n",
      "    y = temp[\"MA_w_imputation\"],\n",
      "#     mode = 'lines',\n",
      "#     name = 'with imputation'\n",
      ")\n",
      "data = [noimp, wimp]\n",
      "fig = go.Figure(data = data)\n",
      "py.iplot(fig, filename = 'time-series-simple')\n",
      "# py.iplot(data, filename = 'time-series-simple')\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=30).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=30).mean()\n",
      "noimp = go.Scatter(\n",
      "    x=temp[\"Date\"], \n",
      "    y=temp[\"MA_no_imputation\"],\n",
      "#     mode = 'lines',\n",
      "#     name = 'w/o imputation'\n",
      ")\n",
      "wimp = go.Scatter(\n",
      "    x = temp[\"Date\"],\n",
      "    y = temp[\"MA_w_imputation\"],\n",
      "#     mode = 'lines',\n",
      "#     name = 'with imputation'\n",
      ")\n",
      "data = [noimp, wimp]\n",
      "fig = go.Figure(data = data)\n",
      "py.iplot(fig, filename = 'time-series-simple')\n",
      "# py.iplot(data, filename = 'time-series-simple')\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"]\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"]\n",
      "noimp = go.Scatter(\n",
      "    x=temp[\"Date\"], \n",
      "    y=temp[\"MA_no_imputation\"],\n",
      "#     mode = 'lines',\n",
      "#     name = 'w/o imputation'\n",
      ")\n",
      "wimp = go.Scatter(\n",
      "    x = temp[\"Date\"],\n",
      "    y = temp[\"MA_w_imputation\"],\n",
      "#     mode = 'lines',\n",
      "#     name = 'with imputation'\n",
      ")\n",
      "data = [noimp, wimp]\n",
      "fig = go.Figure(data = data)\n",
      "py.iplot(fig, filename = 'time-series-simple')\n",
      "# py.iplot(data, filename = 'time-series-simple')\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=30).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=30).mean()\n",
      "noimp = go.Scatter(\n",
      "    x=temp[\"Date\"], \n",
      "    y=temp[\"MA_no_imputation\"],\n",
      "#     mode = 'lines',\n",
      "#     name = 'w/o imputation'\n",
      ")\n",
      "wimp = go.Scatter(\n",
      "    x = temp[\"Date\"],\n",
      "    y = temp[\"MA_w_imputation\"],\n",
      "#     mode = 'lines',\n",
      "#     name = 'with imputation'\n",
      ")\n",
      "data = [noimp, wimp]\n",
      "fig = go.Figure(data = data)\n",
      "py.iplot(fig, filename = 'time-series-simple')\n",
      "# py.iplot(data, filename = 'time-series-simple')\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=30).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=30).mean()\n",
      "plt.scatter(temp[\"Date\"], temp[\"MA_no_imputation\"],temp[\"Date\"], temp[\"MA_w_imputation\"])\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=30).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=30).mean()\n",
      "plt.scatter(temp[\"Date\"], temp[\"MA_no_imputation\"],temp[\"Date\"], temp[\"MA_w_imputation\"])\n",
      "plot.show(0)\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=30).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=30).mean()\n",
      "plt.scatter(temp[\"Date\"], temp[\"MA_no_imputation\"],temp[\"Date\"], temp[\"MA_w_imputation\"])\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=30).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=30).mean()\n",
      "plt.scatter(temp[\"Date\"], temp[\"MA_no_imputation\"])\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=30).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=30).mean()\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"])\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=30).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=30).mean()\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], temp[\"Date\"], temp['MA_w_imputation'])\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=30).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=30).mean()\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"])\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=30).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=30).mean()\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"])\n",
      "plt.plog_date(temp[\"Date\"], temp['MA_w_imputation'])\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=30).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=30).mean()\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"])\n",
      "plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'])\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=3).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=3).mean()\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"])\n",
      "plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'])\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"])\n",
      "plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'])\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"])\n",
      "plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'])\n",
      "plt.show()\n",
      "plt.plot(temp[\"Date\"],temp[\"MA_no_imputation\"])- temp['MA_w_imputation'])\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"])\n",
      "plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'])\n",
      "plt.show()\n",
      "plt.plot(temp[\"Date\"],(temp[\"MA_no_imputation\"]- temp['MA_w_imputation']))\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "\n",
      "\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=0.5, color=\"black\", alpha=0.3)\n",
      "plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "plt.plot(temp[\"Date\"],(temp[\"MA_no_imputation\"]- temp['MA_w_imputation']))\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "\n",
      "\n",
      "# ax.get_xaxis().tick_bottom()    \n",
      "# ax.get_yaxis().tick_left()\n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=0.5, color=\"black\", alpha=0.3)\n",
      "plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "plt.plot(temp[\"Date\"],(temp[\"MA_no_imputation\"]- temp['MA_w_imputation']))\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "plt.figure(figsize=(12, 14))    \n",
      "ax = plt.subplot()\n",
      "\n",
      "# ax.get_xaxis().tick_bottom()    \n",
      "# ax.get_yaxis().tick_left()\n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=0.5, color=\"black\", alpha=0.3)\n",
      "plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "plt.plot(temp[\"Date\"],(temp[\"MA_no_imputation\"]- temp['MA_w_imputation']))\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "plt.figure(figsize=(12, 14))    \n",
      "ax = plt.subplot()\n",
      "\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=0.5, color=\"black\", alpha=0.3)\n",
      "plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "plt.plot(temp[\"Date\"],(temp[\"MA_no_imputation\"]- temp['MA_w_imputation']))\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "plt.figure(figsize=(12, 14))    \n",
      "ax = plt.subplot()\n",
      "\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=0.5, color=\"black\", alpha=0.3)\n",
      "plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "plt.plot(temp[\"Date\"],(temp[\"MA_no_imputation\"]- temp['MA_w_imputation']))\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "plt.figure(figsize=(2, 14))    \n",
      "ax = plt.subplot()\n",
      "\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=0.5, color=\"black\", alpha=0.3)\n",
      "plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "plt.plot(temp[\"Date\"],(temp[\"MA_no_imputation\"]- temp['MA_w_imputation']))\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "plt.figure(figsize=(30, 12))    \n",
      "ax = plt.subplot()\n",
      "\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=0.5, color=\"black\", alpha=0.3)\n",
      "plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "plt.plot(temp[\"Date\"],(temp[\"MA_no_imputation\"]- temp['MA_w_imputation']))\n",
      "plt.show()\n",
      "from fancyimpute import KNN, SoftImpute\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "knn = KNN(3)\n",
      "d = copy[\"HON\"].select_dtypes(include=[np.float]).as_matrix\n",
      "from fancyimpute import KNN, SoftImpute\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "knn = KNN(3)\n",
      "d = copy[\"HON\"].select_dtypes(include=[np.float]).as_matrix\n",
      "\n",
      "d_filled = knn.fit_transform(d)\n",
      "from fancyimpute import KNN, SoftImpute\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "knn = KNN(3)\n",
      "d = copy[\"HON\"].select_dtypes(include=[np.float]).as_matrix()\n",
      "\n",
      "d_filled = knn.fit_transform(d)\n",
      "from fancyimpute import KNN, SoftImpute\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "knn = KNN(3)\n",
      "d = copy[\"HON\"].select_dtypes(include=[np.float]).as_matrix()\n",
      "\n",
      "d_filled = knn.fit_transform(d)\n",
      "d_filled\n",
      "from fancyimpute import KNN, SoftImpute\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "knn = KNN(3)\n",
      "d = copy[\"HON\"].select_dtypes(include=[np.float]).as_matrix()\n",
      "d\n",
      "\n",
      "d_filled = knn.fit_transform(d)\n",
      "d_filled\n",
      "from fancyimpute import KNN, SoftImpute\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "knn = KNN(3)\n",
      "d = copy[\"HON\"].select_dtypes(include=[np.float]).as_matrix()\n",
      "\n",
      "d_filled = knn.fit_transform(d)\n",
      "dd = knn.transform(d_filled)\n",
      "from fancyimpute import KNN, SoftImpute\n",
      "\n",
      "copy = company_stack \n",
      "completed = {}\n",
      "\n",
      "knn = KNN(3)\n",
      "d = copy[\"HON\"].select_dtypes(include=[np.float]).as_matrix()\n",
      "\n",
      "d_filled = knn.fit_transform(d)\n",
      "dd = knn.fit_transform(d_filled)\n",
      "dd\n",
      "max(dd)\n",
      "dd[0]\n",
      "dd[,0]\n",
      "dd[:,0]\n",
      "max(dd[:,0])\n",
      "np.mean(dd[:,0])\n",
      "copy[\"HON\"][\"imputed_mean\"] = dd\n",
      "chon = copy[\"HON\"]\n",
      "nan_impute = chon.groupby(chon[\"Date\"].dt.year).get_group(2018)[chon[\"mean\"].isnull()][\"imputed_mean\"]\n",
      "nan_impute\n",
      "copy[\"HON\"][\"imputed_mean\"] = dd\n",
      "chon = copy[\"HON\"]\n",
      "nan_impute = chon.groupby(chon[\"Date\"].dt.year).get_group(2018)[chon[\"mean\"].isnull()][\"imputed_mean\"]\n",
      "chon\n",
      "copy[\"HON\"][\"imputed_mean\"] = dd\n",
      "chon = copy[\"HON\"]\n",
      "nan_impute = chon.groupby(chon[\"Date\"].dt.year).get_group(2018)[chon[\"mean\"].isnull()][\"imputed_mean\"]\n",
      "np.mean(nan_impute)\n",
      "copy[\"HON\"][\"imputed_mean\"] = dd\n",
      "chon = copy[\"HON\"]\n",
      "nan_impute = chon.groupby(chon[\"Date\"].dt.year).get_group(2018)[chon[\"mean\"].isnull()][\"imputed_mean\"]\n",
      "np.median(nan_impute)\n",
      "copy[\"HON\"][\"imputed_mean\"] = dd\n",
      "chon = copy[\"HON\"]\n",
      "nan_impute = chon.groupby(chon[\"Date\"].dt.year).get_group(2018)[chon[\"mean\"].isnull()][\"imputed_mean\"]\n",
      "max(nan_impute)\n",
      "from fancyimpute import KNN, SoftImpute\n",
      "from sklearn.impute import SimpleImputer \n",
      "\n",
      "copy = company_stack \n",
      "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
      "copy[\"HON\"]\n",
      "from fancyimpute import KNN, SoftImpute\n",
      "from sklearn import impute\n",
      "\n",
      "copy = company_stack \n",
      "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
      "copy[\"HON\"]\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropna()\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "plt.figure(figsize=(30, 12))    \n",
      "ax = plt.subplot()\n",
      "\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=0.5, color=\"black\", alpha=0.3)\n",
      "plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "plt.plot(temp[\"Date\"],(temp[\"MA_no_imputation\"]- temp['MA_w_imputation']))\n",
      "plt.show()\n",
      "from fancyimpute import KNN, SoftImpute\n",
      "from sklearn.preprocessing import Imputer\n",
      "\n",
      "copy = company_stack \n",
      "imp = Imputer(missing_values=np.nan, strategy='median')\n",
      "from fancyimpute import KNN, SoftImpute\n",
      "from sklearn.preprocessing import Imputer\n",
      "\n",
      "copy = company_stack \n",
      "imp = Imputer(missing_values=np.nan, strategy='median')\n",
      "imp.fit_transform(copy[\"HON\"])\n",
      "from fancyimpute import KNN, SoftImpute\n",
      "from sklearn.preprocessing import Imputer\n",
      "\n",
      "copy = company_stack \n",
      "imp = Imputer(missing_values=np.nan, strategy='median')\n",
      "d = copy[\"HON\"]\n",
      "imp.fit_transform(d[[\"mean\"]])\n",
      "from fancyimpute import KNN, SoftImpute\n",
      "from sklearn.preprocessing import Imputer\n",
      "\n",
      "copy = company_stack \n",
      "imp = Imputer(missing_values=np.nan, strategy='median')\n",
      "d = copy[\"HON\"]\n",
      "imp.fit(d[[\"mean\"]])\n",
      "from fancyimpute import KNN, SoftImpute\n",
      "from sklearn.preprocessing import Imputer\n",
      "\n",
      "copy = company_stack \n",
      "imp = Imputer(missing_values=np.nan, strategy='median')\n",
      "d = copy[\"HON\"]\n",
      "imp.fit(d[[\"mean\"]])\n",
      "print(imp.transform(d[[\"mean\"]]))\n",
      "from fancyimpute import KNN, SoftImpute\n",
      "from sklearn.preprocessing import Imputer\n",
      "\n",
      "copy = company_stack \n",
      "imp = Imputer(missing_values=np.nan, strategy='median')\n",
      "d = copy[\"HON\"]\n",
      "d\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "plt.figure(figsize=(30, 12))    \n",
      "ax = plt.subplot()\n",
      "\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=0.5, color=\"black\", alpha=0.3)\n",
      "plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "plt.plot(temp[\"Date\"],(temp[\"MA_no_imputation\"]- temp['MA_w_imputation']))\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "plt.figure(figsize=(30, 12))    \n",
      "ax = plt.subplot()\n",
      "\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=0.5, color=\"black\", alpha=1)\n",
      "plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "plt.plot(temp[\"Date\"],(temp[\"MA_no_imputation\"]- temp['MA_w_imputation']))\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "plt.figure(figsize=(30, 12))    \n",
      "ax = plt.subplot()\n",
      "\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "plt.plot(temp[\"Date\"],(temp[\"MA_no_imputation\"]- temp['MA_w_imputation']))\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "plt.figure(figsize=(30, 12))    \n",
      "ax = plt.subplot()\n",
      "\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "plt.plot(temp[\"Date\"],(temp[\"MA_no_imputation\"]- temp['MA_w_imputation']))\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "plt.figure(figsize=(30, 12))    \n",
      "ax = plt.subplot()\n",
      "\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_w_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "plt.plot(temp[\"Date\"],(temp[\"MA_no_imputation\"]- temp['MA_w_imputation']))\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "plt.figure(figsize=(30, 12))    \n",
      "ax = plt.subplot()\n",
      "print(temp['MA_w_imputation'])\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_w_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "plt.plot(temp[\"Date\"],(temp[\"MA_no_imputation\"]- temp['MA_w_imputation']))\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "plt.figure(figsize=(30, 12))    \n",
      "ax = plt.subplot()\n",
      "print(temp['MA_w_imputation'][\"mean\"].isna().sum()   )\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_w_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "plt.plot(temp[\"Date\"],(temp[\"MA_no_imputation\"]- temp['MA_w_imputation']))\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "plt.figure(figsize=(30, 12))    \n",
      "ax = plt.subplot()\n",
      "print(temp['MA_w_imputation'].isna().sum()   )\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_w_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "plt.plot(temp[\"Date\"],(temp[\"MA_no_imputation\"]- temp['MA_w_imputation']))\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "plt.figure(figsize=(30, 12))    \n",
      "ax = plt.subplot()\n",
      "print(temp['MA_w_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_w_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "plt.plot(temp[\"Date\"],(temp[\"MA_no_imputation\"]- temp['MA_w_imputation']))\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "ax = plt.subplot()\n",
      "print(temp['MA_w_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_w_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "plt.plot(temp[\"Date\"],(temp[\"MA_no_imputation\"]- temp['MA_w_imputation']))\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "ax = plt.subplot()\n",
      "print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_w_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "plt.plot(temp[\"Date\"],(temp[\"MA_no_imputation\"]- temp['MA_w_imputation']))\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "ax = plt.subplot()\n",
      "print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "temp\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def fix_date(df): \n",
      "    dx = df[\"Date\"].apply(lambda x: x.split(\" \"))\n",
      "    dx = dx.apply(lambda x: \"{} {} {}\".format(x[1][:-3], x[0], x[2])) \n",
      "    return dx \n",
      "\n",
      "data_names = []\n",
      "file_names = []\n",
      "companies = [\"MMM\", \"SYF\", \"BAYRY\", \"HON\"]\n",
      "for w in companies:\n",
      "    for i in np.arange(2009,2019):\n",
      "        file_names.append(w+\"_\"+str(i)+\".csv\")\n",
      "        data_names.append(w+\"_\"+str(i))\n",
      "mmm = pd.read_pickle('3m_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "hon = pd.read_pickle('honeywell_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "syf = pd.read_pickle('synchrony_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "bayry = pd.read_pickle('bayer_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "mmm[\"Date\"] = fix_date(mmm)\n",
      "hon[\"Date\"] = fix_date(hon)\n",
      "syf[\"Date\"] = fix_date(syf)\n",
      "bayry[\"Date\"] = fix_date(bayry)\n",
      "\n",
      "\n",
      "def process_pickles(company_name, frame, dict_to_add):\n",
      "    temp_dict = {}\n",
      "    frame[\"Year\"] = frame[\"Date\"].apply(lambda x: x.split(\" \")[2])\n",
      "    years = frame[\"Year\"].unique()\n",
      "    for i in years:\n",
      "        name = company_name+\"_\"+i\n",
      "        temp = frame.groupby(['Year']).get_group(str(i))\n",
      "        if int(i) != 2019:\n",
      "            dict_to_add[name].append(temp[[\"Date\", \"Tweet\"]])\n",
      "#         else: \n",
      "#             dict_to_add[name] = (temp[[\"Date\", \"Tweet\"]])\n",
      "    return dict_to_add\n",
      "\n",
      "\n",
      "file_dict = {}\n",
      "for file, name in zip(file_names, data_names):\n",
      "    file_dict[name] = pd.read_csv(file, names = [\"Date\", \"Tweet\"])\n",
      "    \n",
      "file_dict = process_pickles(\"MMM\", mmm, file_dict)\n",
      "file_dict = process_pickles(\"HON\", hon, file_dict)\n",
      "file_dict = process_pickles(\"SYF\", syf, file_dict)\n",
      "file_dict = process_pickles(\"BAYRY\", bayry, file_dict)\n",
      "pre_proc = {}\n",
      "\n",
      "count = 1;\n",
      "for key, value in file_dict.items():\n",
      "    temp_dict = {}\n",
      "    temp_frame = pd.DataFrame(columns = [\"Tweet\"])\n",
      "    for index, row in value.iterrows():\n",
      "        temp_dict.setdefault(row['Date'], [])\n",
      "        temp_dict[row['Date']].append(row['Tweet'])\n",
      "                              \n",
      "    for keys, tweet in temp_dict.items():\n",
      "        temp_frame.loc[keys] = [tweet]\n",
      "    temp_frame = temp_frame.reset_index()\n",
      "    temp_frame.rename(columns = {\"index\": \"Date\", \"Tweet\":\"Tweet\"}, inplace = True)\n",
      "    temp_frame.sort_values(by=['Date'])\n",
      "    pre_proc[key] = temp_frame\n",
      "dates = np.arange(1,32)\n",
      "feb = np.arange(1, 29)\n",
      "months = {\"Jan\": 31, \"Feb\":28, \"Mar\":31, \"Apr\":30, \"May\":31, \"Jun\":30, \"Jul\":31, \"Aug\":31, \"Sep\":30, \"Oct\": 31, \"Nov\":30, \"Dec\":31}\n",
      "index_dict = {}\n",
      "for k, v in months.items():\n",
      "    for j in range(1,v+1):\n",
      "            key = str(j) + \" \" + k\n",
      "            index_dict[key] = len(index_dict.keys())+1\n",
      "\n",
      "def make_df(stack, key):\n",
      "    temp = {}\n",
      "    year = key.split(\"_\")[-1]\n",
      "    data = [(t+\" \"+str(year)) for t in index_dict.keys()]\n",
      "    for i in data: \n",
      "        temp[i] = \"\"\n",
      "    return temp\n",
      "\n",
      "final_proc = {}\n",
      "for k, v in pre_proc.items():\n",
      "    temp = make_df(v, k)\n",
      "    for indx, val in v.iterrows():\n",
      "        temp[val[\"Date\"]] = val[\"Tweet\"]\n",
      "    out = pd.DataFrame(columns = {\"Date\", \"Tweet\"})\n",
      "    out[\"Date\"] = temp.keys()\n",
      "    out[\"Tweet\"] = list(temp.values())\n",
      "    final_proc[k] = out\n",
      "def convert_dates(stack):\n",
      "    for i, v in stack.items():\n",
      "        v['Date'] = pd.to_datetime(v['Date'], dayfirst = True)\n",
      "    return stack\n",
      "import re\n",
      "def preprocess_text(text):\n",
      "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', text)\n",
      "    text = re.sub('@[^\\s]+','USER', text)\n",
      "    text = text.lower().replace(\"ё\", \"е\")\n",
      "    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
      "    text = re.sub(' +',' ', text)\n",
      "    return text.strip()\n",
      "\n",
      "for key, value in final_proc.items():\n",
      "    for idx, txt in value.iterrows():\n",
      "        txt[\"Tweet\"] = [preprocess_text(t) for t in txt[\"Tweet\"]]\n",
      "\n",
      "final_proc = convert_dates(final_proc)\n",
      "from scipy import stats\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "for i, v in final_proc.items():\n",
      "    emot = []\n",
      "    for index, val in v.iterrows():\n",
      "        if len(val[\"Tweet\"]) != 0:\n",
      "            emot.append([clf.predict(t) for t in [val[\"Tweet\"]]])\n",
      "        else:\n",
      "            emot.append(np.NaN)\n",
      "    v[\"emot\"] = emot\n",
      "    v[\"mean\"] = [np.mean(t) for t in emot]\n",
      "company_stack = {}\n",
      "for name in [\"HON\"]:\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        if(name == (key.split(\"_\")[0])):\n",
      "#             print(key)\n",
      "            frames.append(key)\n",
      "    frames.sort()\n",
      "    print(name, frames)\n",
      "    stacked = [final_proc[t].groupby(final_proc[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[-1])) for t in frames]\n",
      "    company_stack[name] = pd.concat(stacked)\n",
      "    company_stack[name].reset_index(inplace = True)\n",
      "for i, v in final_proc[\"HON_2016\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "for i, v in final_proc[\"HON_2017\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "for i, v in final_proc[\"HON_2018\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"].apply(lambda x: abs(x-1) if x or np.nan)\n",
      "plt.figure(figsize=(30, 12))    \n",
      "ax = plt.subplot()\n",
      "print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "for i, v in final_proc[\"HON_2016\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "for i, v in final_proc[\"HON_2017\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "for i, v in final_proc[\"HON_2018\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"].apply(lambda x: abs(x-1) if x or np.nan)\n",
      "plt.figure(figsize=(30, 12))    \n",
      "ax = plt.subplot()\n",
      "print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"].apply(lambda x: abs(x-1) if x or np.nan)\n",
      "plt.figure(figsize=(30, 12))    \n",
      "ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"].apply(lambda x: abs(x-1) if x or np.nan())\n",
      "plt.figure(figsize=(30, 12))    \n",
      "ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"].apply(lambda x: abs(x-1) if x or np.nan\n",
      "plt.figure(figsize=(30, 12))    \n",
      "ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"].apply(lambda x: abs(x-1) if x or np.nan)\n",
      "plt.figure(figsize=(30, 12))    \n",
      "ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"].apply(lambda x: abs(x-1) if x or np.nan)\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"].apply(lambda x: abs(x-1) if x or np.NaN)\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=10).mean()\n",
      "temp[\"MA_no_imputation\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=5).mean()\n",
      "temp[\"MA_no_imputation\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"MMM\"]\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=5).mean()\n",
      "temp[\"MA_no_imputation\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"mmm\"]\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=5).mean()\n",
      "temp[\"MA_no_imputation\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "company_stack = {}\n",
      "for name in final_proc.items():\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        if(name == (key.split(\"_\")[0])):\n",
      "#             print(key)\n",
      "            frames.append(key)\n",
      "    frames.sort()\n",
      "    print(name, frames)\n",
      "    stacked = [final_proc[t].groupby(final_proc[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[-1])) for t in frames]\n",
      "    company_stack[name] = pd.concat(stacked)\n",
      "    company_stack[name].reset_index(inplace = True)\n",
      "company_stack = {}\n",
      "for name in final_proc.iterrows():\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        if(name == (key.split(\"_\")[0])):\n",
      "#             print(key)\n",
      "            frames.append(key)\n",
      "    frames.sort()\n",
      "    print(name, frames)\n",
      "    stacked = [final_proc[t].groupby(final_proc[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[-1])) for t in frames]\n",
      "    company_stack[name] = pd.concat(stacked)\n",
      "    company_stack[name].reset_index(inplace = True)\n",
      "company_stack = {}\n",
      "for name in final_proc.items():\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        if(name == (key.split(\"_\")[0])):\n",
      "#             print(key)\n",
      "            frames.append(key)\n",
      "    frames.sort()\n",
      "    print(name, frames)\n",
      "    stacked = [final_proc[t].groupby(final_proc[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[-1])) for t in frames]\n",
      "    company_stack[name] = pd.concat(stacked)\n",
      "    company_stack[name].reset_index(inplace = True)\n",
      "company_stack = {}\n",
      "for name in final_proc.items():\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        if(name == (key.split(\"_\")[0])):\n",
      "            frames.append(key)\n",
      "    frames.sort()\n",
      "    print(name, frames)\n",
      "    stacked = [final_proc[t].groupby(final_proc[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[-1])) for t in frames]\n",
      "    company_stack[name] = pd.concat(stacked)\n",
      "    company_stack[name].reset_index(inplace = True)\n",
      "company_stack = {}\n",
      "for name in final_proc.items():\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        if(name == (key.split(\"_\")[0])):\n",
      "            frames.append(key)\n",
      "    frames.sort()\n",
      "    print(name, frames)\n",
      "    stacked = [final_proc[t].groupby(final_proc[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[-1])) for t in frames]\n",
      "    company_stack[name.key] = pd.concat(stacked)\n",
      "    company_stack[name.key].reset_index(inplace = True)\n",
      "company_stack = {}\n",
      "for name, val in final_proc.items():\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        if(name == (key.split(\"_\")[0])):\n",
      "            frames.append(key)\n",
      "    frames.sort()\n",
      "    print(name, frames)\n",
      "    stacked = [val[t].groupby(val[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[-1])) for t in frames]\n",
      "    company_stack[name.key] = pd.concat(stacked)\n",
      "    company_stack[name.key].reset_index(inplace = True)\n",
      "company_stack = {}\n",
      "for name, val in final_proc.items():\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        if(name == (key.split(\"_\")[0])):\n",
      "            frames.append(key)\n",
      "    frames.sort()\n",
      "#     print(name, frames)\n",
      "    stacked = [val[t].groupby(val[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[-1])) for t in frames]\n",
      "    company_stack[name.key] = pd.concat(stacked)\n",
      "    company_stack[name.key].reset_index(inplace = True)\n",
      "company_stack = {}\n",
      "for name, val in final_proc.items():\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        name = (key.split(\"_\")[0])\n",
      "        frames.append(key)\n",
      "    frames.sort()\n",
      "#     print(name, frames)\n",
      "    stacked = [val[t].groupby(val[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[-1])) for t in frames]\n",
      "    company_stack[name.key] = pd.concat(stacked)\n",
      "    company_stack[name.key].reset_index(inplace = True)\n",
      "company_stack = {}\n",
      "for name, val in final_proc.items():\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        name = (key.split(\"_\")[0])\n",
      "        frames.append(key)\n",
      "    frames.sort()\n",
      "#     print(name, frames)\n",
      "    stacked = [val.groupby(val[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[-1])) for t in frames]\n",
      "    company_stack[name.key] = pd.concat(stacked)\n",
      "    company_stack[name.key].reset_index(inplace = True)\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def fix_date(df): \n",
      "    dx = df[\"Date\"].apply(lambda x: x.split(\" \"))\n",
      "    dx = dx.apply(lambda x: \"{} {} {}\".format(x[1][:-3], x[0], x[2])) \n",
      "    return dx \n",
      "\n",
      "data_names = []\n",
      "file_names = []\n",
      "companies = [\"MMM\", \"SYF\", \"BAYRY\", \"HON\"]\n",
      "for w in companies:\n",
      "    for i in np.arange(2009,2019):\n",
      "        file_names.append(w+\"_\"+str(i)+\".csv\")\n",
      "        data_names.append(w+\"_\"+str(i))\n",
      "mmm = pd.read_pickle('3m_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "hon = pd.read_pickle('honeywell_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "syf = pd.read_pickle('synchrony_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "bayry = pd.read_pickle('bayer_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "mmm[\"Date\"] = fix_date(mmm)\n",
      "hon[\"Date\"] = fix_date(hon)\n",
      "syf[\"Date\"] = fix_date(syf)\n",
      "bayry[\"Date\"] = fix_date(bayry)\n",
      "\n",
      "\n",
      "def process_pickles(company_name, frame, dict_to_add):\n",
      "    temp_dict = {}\n",
      "    frame[\"Year\"] = frame[\"Date\"].apply(lambda x: x.split(\" \")[2])\n",
      "    years = frame[\"Year\"].unique()\n",
      "    for i in years:\n",
      "        name = company_name+\"_\"+i\n",
      "        temp = frame.groupby(['Year']).get_group(str(i))\n",
      "        if int(i) != 2019:\n",
      "            dict_to_add[name].append(temp[[\"Date\", \"Tweet\"]])\n",
      "#         else: \n",
      "#             dict_to_add[name] = (temp[[\"Date\", \"Tweet\"]])\n",
      "    return dict_to_add\n",
      "\n",
      "\n",
      "file_dict = {}\n",
      "for file, name in zip(file_names, data_names):\n",
      "    file_dict[name] = pd.read_csv(file, names = [\"Date\", \"Tweet\"])\n",
      "    \n",
      "file_dict = process_pickles(\"MMM\", mmm, file_dict)\n",
      "file_dict = process_pickles(\"HON\", hon, file_dict)\n",
      "file_dict = process_pickles(\"SYF\", syf, file_dict)\n",
      "file_dict = process_pickles(\"BAYRY\", bayry, file_dict)\n",
      "company_stack = {}\n",
      "for name, val in final_proc.items():\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        name = (key.split(\"_\")[0])\n",
      "        frames.append(key)\n",
      "    frames.sort()\n",
      "#     print(name, frames)\n",
      "    stacked = [val.groupby(val[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[-1])) for t in frames]\n",
      "    company_stack[name.key] = pd.concat(stacked)\n",
      "    company_stack[name.key].reset_index(inplace = True)\n",
      "final_proc\n",
      "company_stack = {}\n",
      "for name, val in final_proc.items():\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        name = (key.split(\"_\")[0])\n",
      "        frames.append(key)\n",
      "    print(frames)\n",
      "#     frames.sort()\n",
      "# #     print(name, frames)\n",
      "#     stacked = [val.groupby(val[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[-1])) for t in frames]\n",
      "#     company_stack[name.key] = pd.concat(stacked)\n",
      "#     company_stack[name.key].reset_index(inplace = True)\n",
      "company_stack = {}\n",
      "for name, val in final_proc.items():\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        frames.append(key)\n",
      "#     frames.sort()\n",
      "# #     print(name, frames)\n",
      "    stacked = [val.groupby(val[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[-1])) for t in frames]\n",
      "#     company_stack[name.key] = pd.concat(stacked)\n",
      "#     company_stack[name.key].reset_index(inplace = True)\n",
      "company_stack = {}\n",
      "for name, val in final_proc.items():\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        frames.append(key)\n",
      "#     frames.sort()\n",
      "# #     print(name, frames)\n",
      "    stacked = [val.groupby(val[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[1])) for t in frames]\n",
      "#     company_stack[name.key] = pd.concat(stacked)\n",
      "#     company_stack[name.key].reset_index(inplace = True)\n",
      "company_stack = {}\n",
      "for name, val in final_proc.items():\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        frames.append(key)\n",
      "#     frames.sort()\n",
      "# #     print(name, frames)\n",
      "    stacked = [final_proc.groupby(final_proc[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[1])) for t in frames]\n",
      "#     company_stack[name.key] = pd.concat(stacked)\n",
      "#     company_stack[name.key].reset_index(inplace = True)\n",
      "company_stack = {}\n",
      "for name, val in final_proc.items():\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        frames.append(key)\n",
      "#     frames.sort()\n",
      "# #     print(name, frames)\n",
      "    stacked = [val.groupby(final_proc[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[1])) for t in frames]\n",
      "#     company_stack[name.key] = pd.concat(stacked)\n",
      "#     company_stack[name.key].reset_index(inplace = True)\n",
      "company_stack = {}\n",
      "for name, val in final_proc.items():\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        frames.append(key)\n",
      "#     frames.sort()\n",
      "# #     print(name, frames)\n",
      "    stacked = [val.groupby(final_proc[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[1])) for t in frames]\n",
      "    company_stack[name.key] = pd.concat(stacked)\n",
      "    company_stack[name.key].reset_index(inplace = True)\n",
      "company_stack = {}\n",
      "for name, val in final_proc.items():\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        frames.append(key)\n",
      "#     frames.sort()\n",
      "# #     print(name, frames)\n",
      "    stacked = [val.groupby(final_proc[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[1])) for t in frames]\n",
      "    company_stack[name] = pd.concat(stacked)\n",
      "    company_stack[name].reset_index(inplace = True)\n",
      "for i, v in final_proc[\"HON_2016\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "for i, v in final_proc[\"HON_2017\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "for i, v in final_proc[\"HON_2018\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"mmm\"]\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=5).mean()\n",
      "temp[\"MA_no_imputation\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"MMM\"]\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=5).mean()\n",
      "temp[\"MA_no_imputation\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "company_stack = {}\n",
      "for name, val in final_proc.items():\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        frames.append(key)\n",
      "#     frames.sort()\n",
      "# #     print(name, frames)\n",
      "    stacked = [val.groupby(final_proc[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[1])) for t in frames]\n",
      "    company_stack[name.split(\"_\")[-1]] = pd.concat(stacked)\n",
      "    company_stack[name.split(\"_\")[-1]].reset_index(inplace = True)\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"MMM\"]\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=5).mean()\n",
      "temp[\"MA_no_imputation\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "for i, v in final_proc[\"HON_2016\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "for i, v in final_proc[\"HON_2017\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "for i, v in final_proc[\"HON_2018\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "print(company_stack)\n",
      "company_stack = {}\n",
      "for name, val in final_proc.items():\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        frames.append(key)\n",
      "#     frames.sort()\n",
      "# #     print(name, frames)\n",
      "    stacked = [val.groupby(final_proc[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[1])) for t in frames]\n",
      "    company_stack[name.split(\"_\")[1]] = pd.concat(stacked)\n",
      "    company_stack[name.split(\"_\")[1]].reset_index(inplace = True)\n",
      "for i, v in final_proc[\"HON_2016\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "for i, v in final_proc[\"HON_2017\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "for i, v in final_proc[\"HON_2018\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "print(company_stack)\n",
      "company_stack = {}\n",
      "for name, val in final_proc.items():\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        frames.append(key)\n",
      "#     frames.sort()\n",
      "# #     print(name, frames)\n",
      "    stacked = [val.groupby(final_proc[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[1])) for t in frames]\n",
      "    print(name.split(\"_\")[-1])\n",
      "#     company_stack[name.split(\"_\")[-1]] = pd.concat(stacked)\n",
      "#     company_stack[name.split(\"_\")[-1]].reset_index(inplace = True)\n",
      "company_stack = {}\n",
      "for name, val in final_proc.items():\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        frames.append(key)\n",
      "#     frames.sort()\n",
      "# #     print(name, frames)\n",
      "    stacked = [val.groupby(final_proc[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[1])) for t in frames]\n",
      "    print(name)\n",
      "#     company_stack[name.split(\"_\")[-1]] = pd.concat(stacked)\n",
      "#     company_stack[name.split(\"_\")[-1]].reset_index(inplace = True)\n",
      "company_stack = {}\n",
      "for name, val in final_proc.items():\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        frames.append(key)\n",
      "#     frames.sort()\n",
      "# #     print(name, frames)\n",
      "    stacked = [val.groupby(final_proc[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[1])) for t in frames]\n",
      "    company_stack[name.split(\"_\")[0]] = pd.concat(stacked)\n",
      "    company_stack[name.split(\"_\")[0]].reset_index(inplace = True)\n",
      "for i, v in final_proc[\"HON_2016\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "for i, v in final_proc[\"HON_2017\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "for i, v in final_proc[\"HON_2018\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "print(company_stack)\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"MMM\"]\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=5).mean()\n",
      "temp[\"MA_no_imputation\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"MA_no_imputation\"] = temp[\"mean\"].rolling(window=5).mean()\n",
      "temp[\"MA_no_imputation\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA_no_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"mean\"] = temp[\"emot\"].rolling(window=5).mean()\n",
      "temp[\"mean\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"mean\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"MA\"] = temp[\"emot\"].rolling(window=5).mean()\n",
      "temp[\"MA\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "print(temp)\n",
      "temp[\"MA\"] = temp[\"emot\"].rolling(window=5).mean()\n",
      "temp[\"MA\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "print(temp)\n",
      "temp[\"MA\"] = temp[\"emot\"].rolling(window=5).mean()\n",
      "# temp[\"MA\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "print(temp)\n",
      "temp[\"MA\"] = temp[\"emot\"].rolling(window=5).sum()\n",
      "# temp[\"MA\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"MA\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "print(temp)\n",
      "temp[\"test\"] = temp[\"emot\"].rolling(window=5).sum()\n",
      "# temp[\"MA\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"test\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "print(temp)\n",
      "temp[\"test\"] = temp[\"emot\"].rolling(window=5).mean()\n",
      "# temp[\"MA\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"test\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "print(temp[\"emot\"])\n",
      "# temp[\"test\"] = temp[\"emot\"].rolling(window=5).mean()\n",
      "# temp[\"MA\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"test\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "print(temp[\"emot\"])\n",
      "# temp[\"test\"] = temp[\"emot\"].rolling(window=5).mean()\n",
      "# temp[\"MA\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "# plt.plot_date(temp[\"Date\"], temp[\"test\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "print(temp[\"emot\"])\n",
      "temp[\"test\"] = (temp[\"emot\"]).rolling(window=5).mean()\n",
      "# temp[\"MA\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "# plt.plot_date(temp[\"Date\"], temp[\"test\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "print(temp[\"emot\"])\n",
      "temp[\"emot\"].rolling(3, min_periods=1).mean()\n",
      "# temp[\"MA\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "# plt.plot_date(temp[\"Date\"], temp[\"test\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "print(temp[\"emot\"])\n",
      "temp[\"emot\"].rolling(3, min_periods=1).apply(np.nanmean)\n",
      "# temp[\"MA\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "# plt.plot_date(temp[\"Date\"], temp[\"test\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "print(temp[\"emot\"])\n",
      "temp[\"emot\"] = temp[\"emot\"].rolling(3, min_periods=1).apply(np.nanmean)\n",
      "# temp[\"MA\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "# plt.plot_date(temp[\"Date\"], temp[\"test\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "print(temp[\"emot\"])\n",
      "temp[\"test\"] = temp[\"emot\"].rolling(3).apply(np.nanmean)\n",
      "# temp[\"MA\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "# plt.plot_date(temp[\"Date\"], temp[\"test\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropnan()\n",
      "print(temp[\"emot\"])\n",
      "temp[\"test\"] = temp[\"emot\"].rolling(3).apply(np.nanmean)\n",
      "# temp[\"MA\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "# plt.plot_date(temp[\"Date\"], temp[\"test\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropnans()\n",
      "print(temp[\"emot\"])\n",
      "temp[\"test\"] = temp[\"emot\"].rolling(3).apply(np.nanmean)\n",
      "# temp[\"MA\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "# plt.plot_date(temp[\"Date\"], temp[\"test\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"].dropnans()\n",
      "print(temp[\"emot\"])\n",
      "temp[\"test\"] = temp[\"emot\"].rolling(3).apply(np.nanmean)\n",
      "# temp[\"MA\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "# plt.plot_date(temp[\"Date\"], temp[\"test\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "print(temp[\"emot\"])\n",
      "temp[\"test\"] = temp[\"emot\"].rolling(3).apply(np.nanmean)\n",
      "# temp[\"MA\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "# plt.plot_date(temp[\"Date\"], temp[\"test\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"test\"] = temp[\"emot\"].interpolate()\n",
      "print(temp[\"test\"])\n",
      "# temp[\"test\"] = temp[\"emot\"].rolling(3).apply(np.nanmean)\n",
      "# temp[\"MA\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "# plt.plot_date(temp[\"Date\"], temp[\"test\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"test\"] = temp[\"emot\"]\n",
      "print(temp[\"mean\"])\n",
      "temp[\"test\"] = temp[\"mean\"].rolling(3).apply(np.nanmean)\n",
      "# temp[\"MA\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "# plt.plot_date(temp[\"Date\"], temp[\"test\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"test\"] = temp[\"mean\"].interpolate()\n",
      "temp[\"test\"] = temp[\"test\"].rolling(3).apply(np.nanmean)\n",
      "# temp[\"MA\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "# plt.plot_date(temp[\"Date\"], temp[\"test\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"test\"] = temp[\"mean\"].interpolate()\n",
      "temp[\"test\"] = temp[\"test\"].rolling(3).apply(np.nanmean)\n",
      "print()\n",
      "# temp[\"MA\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "# plt.plot_date(temp[\"Date\"], temp[\"test\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"test\"] = temp[\"mean\"].interpolate()\n",
      "temp[\"test\"] = temp[\"test\"].rolling(3).apply(np.nanmean)\n",
      "print(temp[\"test\"])\n",
      "# temp[\"MA\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "# plt.plot_date(temp[\"Date\"], temp[\"test\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"test\"] = temp[\"mean\"].interpolate()\n",
      "temp[[\"test\", \"mean\"]]\n",
      "temp[\"test\"] = temp[\"test\"].rolling(3).apply(np.nanmean)\n",
      "# temp[\"MA\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "# plt.plot_date(temp[\"Date\"], temp[\"test\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"test\"] = temp[\"mean\"].interpolate()\n",
      "print(temp[[\"test\", \"mean\"]])\n",
      "temp[\"test\"] = temp[\"test\"].rolling(3).apply(np.nanmean)\n",
      "# temp[\"MA\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "# plt.plot_date(temp[\"Date\"], temp[\"test\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"interpolated_mean\"] = temp[\"mean\"].interpolate()\n",
      "temp[\"interpolated_mean\"] = temp[\"interpolated_mean\"].rolling(30).apply(np.nanmean)\n",
      "temp[\"interpolated_mean\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"interpolated_mean\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"interpolated_mean\"] = temp[\"mean\"].interpolate()\n",
      "temp[\"interpolated_mean\"] = temp[\"interpolated_mean\"].rolling(30).apply(np.nanmean)\n",
      "# temp[\"interpolated_mean\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"interpolated_mean\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"interpolated_mean\"] = temp[\"mean\"].interpolate()\n",
      "temp[\"interpolated_mean\"] = temp[\"interpolated_mean\"].rolling(30).apply(np.nanmean)\n",
      "# temp[\"interpolated_mean\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"mean\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"interpolated_mean\"] = temp[\"mean\"].interpolate()\n",
      "temp[\"interpolated_mean\"] = temp[\"interpolated_mean\"].rolling(30).mean() \n",
      "# temp[\"interpolated_mean\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"mean\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"interpolated_mean\"] = temp[\"mean\"].interpolate().dropnan()\n",
      "temp[\"interpolated_mean\"] = temp[\"interpolated_mean\"].rolling(30).mean() \n",
      "# temp[\"interpolated_mean\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"mean\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"interpolated_mean\"] = temp[\"mean\"].interpolate().dropna()\n",
      "temp[\"interpolated_mean\"] = temp[\"interpolated_mean\"].rolling(30).mean() \n",
      "# temp[\"interpolated_mean\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"mean\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp[\"interpolated_mean\"] = temp[\"mean\"].interpolate()\n",
      "temp[\"interpolated_mean\"] = temp[\"interpolated_mean\"].rolling(30).mean() \n",
      "# temp[\"interpolated_mean\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"mean\"].dropna(), \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp_2 = temp\n",
      "temp_2.dropna()\n",
      "temp[\"interpolated_mean\"] = temp[\"mean\"].interpolate()\n",
      "temp[\"interpolated_mean\"] = temp[\"interpolated_mean\"].rolling(30).mean() \n",
      "# temp[\"interpolated_mean\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp_2[\"Date\"], temp_2[\"mean\"] \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp_2 = temp\n",
      "temp_2.dropna()\n",
      "temp[\"interpolated_mean\"] = temp[\"mean\"].interpolate()\n",
      "temp[\"interpolated_mean\"] = temp[\"interpolated_mean\"].rolling(30).mean() \n",
      "# temp[\"interpolated_mean\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp_2[\"Date\"], temp_2[\"mean\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp_2 = temp\n",
      "temp_2.dropna\n",
      "temp[\"interpolated_mean\"] = temp[\"mean\"].interpolate()\n",
      "temp[\"interpolated_mean\"] = temp[\"interpolated_mean\"].rolling(30).mean() \n",
      "# temp[\"interpolated_mean\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp_2[\"Date\"], temp_2[\"mean\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def fix_date(df): \n",
      "    dx = df[\"Date\"].apply(lambda x: x.split(\" \"))\n",
      "    dx = dx.apply(lambda x: \"{} {} {}\".format(x[1][:-3], x[0], x[2])) \n",
      "    return dx \n",
      "\n",
      "data_names = []\n",
      "file_names = []\n",
      "companies = [\"MMM\", \"SYF\", \"BAYRY\", \"HON\"]\n",
      "for w in companies:\n",
      "    for i in np.arange(2009,2019):\n",
      "        file_names.append(w+\"_\"+str(i)+\".csv\")\n",
      "        data_names.append(w+\"_\"+str(i))\n",
      "mmm = pd.read_pickle('3m_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "hon = pd.read_pickle('honeywell_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "syf = pd.read_pickle('synchrony_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "bayry = pd.read_pickle('bayer_stocktwits.pickle', 'gzip').rename(columns = {'published_date':\"Date\", 'content':\"Tweet\"})\n",
      "mmm[\"Date\"] = fix_date(mmm)\n",
      "hon[\"Date\"] = fix_date(hon)\n",
      "syf[\"Date\"] = fix_date(syf)\n",
      "bayry[\"Date\"] = fix_date(bayry)\n",
      "\n",
      "\n",
      "def process_pickles(company_name, frame, dict_to_add):\n",
      "    temp_dict = {}\n",
      "    frame[\"Year\"] = frame[\"Date\"].apply(lambda x: x.split(\" \")[2])\n",
      "    years = frame[\"Year\"].unique()\n",
      "    for i in years:\n",
      "        name = company_name+\"_\"+i\n",
      "        temp = frame.groupby(['Year']).get_group(str(i))\n",
      "        if int(i) != 2019:\n",
      "            dict_to_add[name].append(temp[[\"Date\", \"Tweet\"]])\n",
      "#         else: \n",
      "#             dict_to_add[name] = (temp[[\"Date\", \"Tweet\"]])\n",
      "    return dict_to_add\n",
      "\n",
      "\n",
      "file_dict = {}\n",
      "for file, name in zip(file_names, data_names):\n",
      "    file_dict[name] = pd.read_csv(file, names = [\"Date\", \"Tweet\"])\n",
      "    \n",
      "file_dict = process_pickles(\"MMM\", mmm, file_dict)\n",
      "file_dict = process_pickles(\"HON\", hon, file_dict)\n",
      "file_dict = process_pickles(\"SYF\", syf, file_dict)\n",
      "file_dict = process_pickles(\"BAYRY\", bayry, file_dict)\n",
      "pre_proc = {}\n",
      "\n",
      "count = 1;\n",
      "for key, value in file_dict.items():\n",
      "    temp_dict = {}\n",
      "    temp_frame = pd.DataFrame(columns = [\"Tweet\"])\n",
      "    for index, row in value.iterrows():\n",
      "        temp_dict.setdefault(row['Date'], [])\n",
      "        temp_dict[row['Date']].append(row['Tweet'])\n",
      "                              \n",
      "    for keys, tweet in temp_dict.items():\n",
      "        temp_frame.loc[keys] = [tweet]\n",
      "    temp_frame = temp_frame.reset_index()\n",
      "    temp_frame.rename(columns = {\"index\": \"Date\", \"Tweet\":\"Tweet\"}, inplace = True)\n",
      "    temp_frame.sort_values(by=['Date'])\n",
      "    pre_proc[key] = temp_frame\n",
      "dates = np.arange(1,32)\n",
      "feb = np.arange(1, 29)\n",
      "months = {\"Jan\": 31, \"Feb\":28, \"Mar\":31, \"Apr\":30, \"May\":31, \"Jun\":30, \"Jul\":31, \"Aug\":31, \"Sep\":30, \"Oct\": 31, \"Nov\":30, \"Dec\":31}\n",
      "index_dict = {}\n",
      "for k, v in months.items():\n",
      "    for j in range(1,v+1):\n",
      "            key = str(j) + \" \" + k\n",
      "            index_dict[key] = len(index_dict.keys())+1\n",
      "\n",
      "def make_df(stack, key):\n",
      "    temp = {}\n",
      "    year = key.split(\"_\")[-1]\n",
      "    data = [(t+\" \"+str(year)) for t in index_dict.keys()]\n",
      "    for i in data: \n",
      "        temp[i] = \"\"\n",
      "    return temp\n",
      "\n",
      "final_proc = {}\n",
      "for k, v in pre_proc.items():\n",
      "    temp = make_df(v, k)\n",
      "    for indx, val in v.iterrows():\n",
      "        temp[val[\"Date\"]] = val[\"Tweet\"]\n",
      "    out = pd.DataFrame(columns = {\"Date\", \"Tweet\"})\n",
      "    out[\"Date\"] = temp.keys()\n",
      "    out[\"Tweet\"] = list(temp.values())\n",
      "    final_proc[k] = out\n",
      "import re\n",
      "def preprocess_text(text):\n",
      "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', text)\n",
      "    text = re.sub('@[^\\s]+','USER', text)\n",
      "    text = text.lower().replace(\"ё\", \"е\")\n",
      "    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
      "    text = re.sub(' +',' ', text)\n",
      "    return text.strip()\n",
      "\n",
      "for key, value in final_proc.items():\n",
      "    for idx, txt in value.iterrows():\n",
      "        txt[\"Tweet\"] = [preprocess_text(t) for t in txt[\"Tweet\"]]\n",
      "\n",
      "final_proc = convert_dates(final_proc)\n",
      "import re\n",
      "def preprocess_text(text):\n",
      "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', text)\n",
      "    text = re.sub('@[^\\s]+','USER', text)\n",
      "    text = text.lower().replace(\"ё\", \"е\")\n",
      "    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
      "    text = re.sub(' +',' ', text)\n",
      "    return text.strip()\n",
      "\n",
      "for key, value in final_proc.items():\n",
      "    for idx, txt in value.iterrows():\n",
      "        txt[\"Tweet\"] = [preprocess_text([t]) for t in txt[\"Tweet\"]]\n",
      "\n",
      "final_proc = convert_dates(final_proc)\n",
      "import re\n",
      "def preprocess_text(text):\n",
      "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', text)\n",
      "    text = re.sub('@[^\\s]+','USER', text)\n",
      "    text = text.lower().replace(\"ё\", \"е\")\n",
      "    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
      "    text = re.sub(' +',' ', text)\n",
      "    return text.strip()\n",
      "\n",
      "for key, value in final_proc.items():\n",
      "    for idx, txt in value.iterrows():\n",
      "        txt[\"Tweet\"] = [preprocess_text(t) for t in txt[\"Tweet\"]]\n",
      "\n",
      "final_proc = convert_dates(final_proc)\n",
      "import re\n",
      "def preprocess_text(text):\n",
      "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', text)\n",
      "    text = re.sub('@[^\\s]+','USER', text)\n",
      "    text = text.lower().replace(\"ё\", \"е\")\n",
      "    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
      "    text = re.sub(' +',' ', text)\n",
      "    return text.strip()\n",
      "\n",
      "for key, value in final_proc.items():\n",
      "    for idx, txt in value.iterrows():\n",
      "        txt[\"Tweet\"] = [preprocess_text(t) for t in txt[\"Tweet\"]]\n",
      "\n",
      "final_proc = convert_dates(final_proc)\n",
      "from scipy import stats\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "for i, v in final_proc.items():\n",
      "    emot = []\n",
      "    for index, val in v.iterrows():\n",
      "        if len(val[\"Tweet\"]) != 0:\n",
      "            emot.append([clf.predict(t) for t in [val[\"Tweet\"]]])\n",
      "        else:\n",
      "            emot.append(np.NaN)\n",
      "    v[\"emot\"] = emot\n",
      "    v[\"mean\"] = [np.mean(t) for t in emot]\n",
      "final_proc\n",
      "company_stack = {}\n",
      "for name, val in final_proc.items():\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        frames.append(key)\n",
      "#     frames.sort()\n",
      "# #     print(name, frames)\n",
      "    stacked = [val.groupby(final_proc[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[1])) for t in frames]\n",
      "    company_stack[name.split(\"_\")[0]] = pd.concat(stacked)\n",
      "    company_stack[name.split(\"_\")[0]].reset_index(inplace = True)\n",
      "for i, v in final_proc[\"HON_2016\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "for i, v in final_proc[\"HON_2017\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "for i, v in final_proc[\"HON_2018\"].iterrows():\n",
      "    if(len(v) > 365):\n",
      "        v.drop([365])\n",
      "print(company_stack)\n",
      "company_stack[\"inter\"] = company_stack[\"mean\"].interpolate()\n",
      "print(company_stack)\n",
      "\n",
      "company_stack[\"HON\"][\"inter\"] = company_stack[\"HON\"][\"mean\"].interpolate()\n",
      "print(company_stack[\"HON\"])\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp_2 = temp\n",
      "temp_2.dropna\n",
      "# temp[\"interpolated_mean\"] = temp[\"mean\"].interpolate()\n",
      "temp[\"interpolated_mean\"] = temp[\"inter\"].rolling(5).mean() \n",
      "# temp[\"interpolated_mean\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"interpolated_mean\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "\n",
      "# company_stack[\"HON\"][\"inter\"] = company_stack[\"HON\"][\"mean\"].interpolate()\n",
      "print(company_stack[\"HON\"])\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp_2 = temp\n",
      "temp_2.dropna\n",
      "# temp[\"interpolated_mean\"] = temp[\"mean\"].interpolate()\n",
      "temp[\"interpolated_mean\"] = temp[\"mean\"].rolling(5).mean() \n",
      "# temp[\"interpolated_mean\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"interpolated_mean\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp_2 = temp\n",
      "temp_2.dropna\n",
      "# temp[\"interpolated_mean\"] = temp[\"mean\"].interpolate()\n",
      "temp[\"interpolated_mean\"] = temp[\"inter\"].rolling(5).mean() \n",
      "# temp[\"interpolated_mean\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"interpolated_mean\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp_2 = temp\n",
      "temp_2.dropna\n",
      "temp[\"interpolated_mean\"] = temp[\"mean\"].dropna().interpolate()\n",
      "# temp[\"interpolated_mean\"] = temp[\"inter\"].rolling(5).mean() \n",
      "# temp[\"interpolated_mean\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"interpolated_mean\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp_2 = temp\n",
      "temp_2.dropna\n",
      "temp[\"interpolated_mean\"] = temp[\"mean\"].dropna()\n",
      "# temp[\"interpolated_mean\"] = temp[\"inter\"].rolling(5).mean() \n",
      "# temp[\"interpolated_mean\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"interpolated_mean\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "\n",
      "company_stack[\"HON\"][\"inter\"] = company_stack[\"HON\"][\"mean\"]\n",
      "print(company_stack[\"HON\"])\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp_2 = temp\n",
      "temp_2.dropna\n",
      "temp[\"interpolated_mean\"] = temp[\"mean\"].dropna()\n",
      "# temp[\"interpolated_mean\"] = temp[\"inter\"].rolling(5).mean() \n",
      "# temp[\"interpolated_mean\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"interpolated_mean\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "temp_2 = temp\n",
      "temp_2.dropna\n",
      "temp[\"interpolated_mean\"] = temp[\"mean\"].dropna()\n",
      "# temp[\"interpolated_mean\"] = temp[\"inter\"].rolling(5).mean() \n",
      "# temp[\"interpolated_mean\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"interpolated_mean\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "print(temp)\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "\n",
      "temp[\"interpolated_mean\"] = temp[\"mean\"].fillna(method='pad')\n",
      "# temp[\"interpolated_mean\"] = temp[\"inter\"].rolling(5).mean() \n",
      "# temp[\"interpolated_mean\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"interpolated_mean\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "print(temp)\n",
      "company_stack = {}\n",
      "for name, val in final_proc.items():\n",
      "    frames = []\n",
      "    for key in data_names:\n",
      "        frames.append(key)\n",
      "#     frames.sort()\n",
      "# #     print(name, frames)\n",
      "    stacked = [val.groupby(final_proc[t][\"Date\"].dt.year).get_group(int(t.split(\"_\")[1])) for t in frames]\n",
      "    company_stack[name.split(\"_\")[0]] = pd.concat(stacked)\n",
      "    company_stack[name.split(\"_\")[0]].reset_index(inplace = True)\n",
      "print(company_stack[\"HON\"])\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "\n",
      "temp[\"interpolated_mean\"] = temp[\"mean\"].fillna(method='pad')\n",
      "# temp[\"interpolated_mean\"] = temp[\"inter\"].rolling(5).mean() \n",
      "# temp[\"interpolated_mean\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"interpolated_mean\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "print(temp)\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "\n",
      "temp[\"interpolated_mean\"] = temp[\"mean\"].fillna(method='pad')\n",
      "temp[\"interpolated_mean\"] = temp[\"interpolated_mean\"].rolling(30).mean() \n",
      "# temp[\"interpolated_mean\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"interpolated_mean\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "print(temp)\n",
      "for i, v in final_proc[\"HON_2016\"].iterrows():\n",
      "    if(len(v) > 364):\n",
      "        v.drop([365])\n",
      "for i, v in final_proc[\"HON_2017\"].iterrows():\n",
      "    if(len(v) > 364):\n",
      "        v.drop([365])\n",
      "for i, v in final_proc[\"HON_2018\"].iterrows():\n",
      "    if(len(v) > 364):\n",
      "        v.drop([365])\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "\n",
      "temp[\"interpolated_mean\"] = temp[\"mean\"].fillna(method='pad')\n",
      "temp[\"interpolated_mean\"] = temp[\"interpolated_mean\"].rolling(30).mean() \n",
      "# temp[\"interpolated_mean\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"interpolated_mean\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "print(temp)\n",
      "for i, v in final_proc[\"HON_2016\"].iterrows():\n",
      "    if(len(v) > 364):\n",
      "        v.drop([364:])\n",
      "for i, v in final_proc[\"HON_2017\"].iterrows():\n",
      "    if(len(v) > 364):\n",
      "        v.drop([364:])\n",
      "for i, v in final_proc[\"HON_2018\"].iterrows():\n",
      "    if(len(v) > 364):\n",
      "        v.drop([364:])\n",
      "for i, v in final_proc[\"HON_2016\"].iterrows():\n",
      "    if(len(v) > 364):\n",
      "        v.drop([364])\n",
      "for i, v in final_proc[\"HON_2017\"].iterrows():\n",
      "    if(len(v) > 364):\n",
      "        v.drop([364])\n",
      "for i, v in final_proc[\"HON_2018\"].iterrows():\n",
      "    if(len(v) > 364):\n",
      "        v.drop([364])\n",
      "print(company_stack[\"HON\"])\n",
      "'''\n",
      "from tabulate import tabulate\n",
      "for i, v in company_stack.items():\n",
      "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
      "'''\n",
      "import plotly.plotly as py\n",
      "import plotly.graph_objs as go\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.pyplot import figure\n",
      "\n",
      "\n",
      "temp = company_stack[\"HON\"]\n",
      "\n",
      "temp[\"interpolated_mean\"] = temp[\"mean\"].fillna(method='pad')\n",
      "temp[\"interpolated_mean\"] = temp[\"interpolated_mean\"].rolling(30).mean() \n",
      "# temp[\"interpolated_mean\"].apply(lambda x: abs(x-1))\n",
      "plt.figure(figsize=(30, 12))    \n",
      "# ax = plt.subplot()\n",
      "# print(temp['MA_no_imputation'].isna().sum())\n",
      "ax.get_xaxis().tick_bottom()    \n",
      "ax.get_yaxis().tick_left()\n",
      "ax.spines[\"top\"].set_visible(False)    \n",
      "ax.spines[\"bottom\"].set_visible(False)    \n",
      "ax.spines[\"right\"].set_visible(False)    \n",
      "ax.spines[\"left\"].set_visible(False)    \n",
      "\n",
      "plt.plot_date(temp[\"Date\"], temp[\"interpolated_mean\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
      "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
      "plt.show()\n",
      "print(temp)\n",
      "%history\n"
     ]
    }
   ],
   "source": [
    "%history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       index                                              Tweet       Date  \\\n",
      "0          0                                                 [] 2018-01-01   \n",
      "1          1                                                 [] 2018-01-02   \n",
      "2          2  [covered quite a bit in today s user daily run... 2018-01-03   \n",
      "3          3  [top 1 position 7 32 vlo 6 65 t 6 62 sna 6 4 a... 2018-01-04   \n",
      "4          4  [the latest update to the r i p portfolio bac ... 2018-01-05   \n",
      "5          5                                                 [] 2018-01-06   \n",
      "6          6  [one hell of a nice breakout setup in hon rt i... 2018-01-07   \n",
      "7          7  [honeywell hon settled 5 at 154 74 much like a... 2018-01-08   \n",
      "8          8  [hon daily chart could be one to watch for a p... 2018-01-09   \n",
      "9          9  [doing scans just to keep my skills sharp here... 2018-01-10   \n",
      "10        10  [wall street breakfast ceos warn of daca hit t... 2018-01-11   \n",
      "11        11  [user all star trader labu atvi hon nvda mscc ... 2018-01-12   \n",
      "12        12                                                 [] 2018-01-13   \n",
      "13        13                                                 [] 2018-01-14   \n",
      "14        14                                                 [] 2018-01-15   \n",
      "15        15                                                 [] 2018-01-16   \n",
      "16        16                                                 [] 2018-01-17   \n",
      "17        17                                                 [] 2018-01-18   \n",
      "18        18                                                 [] 2018-01-19   \n",
      "19        19                                                 [] 2018-01-20   \n",
      "20        20                                                 [] 2018-01-21   \n",
      "21        21                                                 [] 2018-01-22   \n",
      "22        22                                                 [] 2018-01-23   \n",
      "23        23                                                 [] 2018-01-24   \n",
      "24        24                                                 [] 2018-01-25   \n",
      "25        25                                                 [] 2018-01-26   \n",
      "26        26                                                 [] 2018-01-27   \n",
      "27        27                                                 [] 2018-01-28   \n",
      "28        28                                                 [] 2018-01-29   \n",
      "29        29                                                 [] 2018-01-30   \n",
      "...      ...                                                ...        ...   \n",
      "14570    335                                                 [] 2018-12-02   \n",
      "14571    336  [top 1 practice positions by 9 13 cof p 8 55 a... 2018-12-03   \n",
      "14572    337  [any thoughts on buying back some adbe and hon... 2018-12-04   \n",
      "14573    338              [hon honeywell excellent entry point] 2018-12-05   \n",
      "14574    339                                                 [] 2018-12-06   \n",
      "14575    340                                                 [] 2018-12-07   \n",
      "14576    341  [the investing secrets of the richest man the ... 2018-12-08   \n",
      "14577    342                                                 [] 2018-12-09   \n",
      "14578    343  [some head and shoulder set ups url sail hon f... 2018-12-10   \n",
      "14579    344  [v hd duk brk b hon yelp t jnj hig jpm xpo luv... 2018-12-11   \n",
      "14580    345  [user hi y n hon i am ulysses nice to meet you... 2018-12-12   \n",
      "14581    346  [ball is in your court pepsico pep executives ... 2018-12-13   \n",
      "14582    347                                                 [] 2018-12-14   \n",
      "14583    348                                                 [] 2018-12-15   \n",
      "14584    349                                                 [] 2018-12-16   \n",
      "14585    350  [mas lowered to 37 at nomura mnk lowered to 22... 2018-12-17   \n",
      "14586    351                                                 [] 2018-12-18   \n",
      "14587    352  [made 25k today thanks to the option trades at... 2018-12-19   \n",
      "14588    353  [these 11 stocks are set to rocket higher in 2... 2018-12-20   \n",
      "14589    354  [flight delayed honeywell is working on a fix ... 2018-12-21   \n",
      "14590    355  [long short bitcoin volatility with up to 1 x ... 2018-12-22   \n",
      "14591    356                                                 [] 2018-12-23   \n",
      "14592    357  [hon honeywell traded at 166 share in 1 2 18 d... 2018-12-24   \n",
      "14593    358  [hard to top hon gotti out here dropping class... 2018-12-25   \n",
      "14594    359  [christmas rally googl fb jpm c bac wfc aapl p... 2018-12-26   \n",
      "14595    360  [call put highest volume changes calls col avg... 2018-12-27   \n",
      "14596    361                                                 [] 2018-12-28   \n",
      "14597    362  [copy expert traders systematically using cryp... 2018-12-29   \n",
      "14598    363  [forget 3m ge is a better value stock user sto... 2018-12-30   \n",
      "14599    364                                                 [] 2018-12-31   \n",
      "\n",
      "                 emot      mean  interpolated_mean  \n",
      "0                 NaN       NaN                NaN  \n",
      "1                 NaN       NaN                NaN  \n",
      "2            [[0, 0]]  0.000000                NaN  \n",
      "3               [[1]]  1.000000                NaN  \n",
      "4               [[0]]  0.000000                NaN  \n",
      "5                 NaN       NaN                NaN  \n",
      "6               [[0]]  0.000000                NaN  \n",
      "7               [[1]]  1.000000                NaN  \n",
      "8      [[0, 0, 1, 1]]  0.500000                NaN  \n",
      "9      [[0, 0, 1, 0]]  0.250000                NaN  \n",
      "10           [[0, 0]]  0.000000                NaN  \n",
      "11        [[0, 0, 0]]  0.000000                NaN  \n",
      "12                NaN       NaN                NaN  \n",
      "13                NaN       NaN                NaN  \n",
      "14                NaN       NaN                NaN  \n",
      "15                NaN       NaN                NaN  \n",
      "16                NaN       NaN                NaN  \n",
      "17                NaN       NaN                NaN  \n",
      "18                NaN       NaN                NaN  \n",
      "19                NaN       NaN                NaN  \n",
      "20                NaN       NaN                NaN  \n",
      "21                NaN       NaN                NaN  \n",
      "22                NaN       NaN                NaN  \n",
      "23                NaN       NaN                NaN  \n",
      "24                NaN       NaN                NaN  \n",
      "25                NaN       NaN                NaN  \n",
      "26                NaN       NaN                NaN  \n",
      "27                NaN       NaN                NaN  \n",
      "28                NaN       NaN                NaN  \n",
      "29                NaN       NaN                NaN  \n",
      "...               ...       ...                ...  \n",
      "14570             NaN       NaN           0.033333  \n",
      "14571        [[1, 0]]  0.500000           0.050000  \n",
      "14572  [[0, 0, 0, 0]]  0.000000           0.050000  \n",
      "14573           [[0]]  0.000000           0.050000  \n",
      "14574             NaN       NaN           0.050000  \n",
      "14575             NaN       NaN           0.050000  \n",
      "14576           [[0]]  0.000000           0.050000  \n",
      "14577             NaN       NaN           0.050000  \n",
      "14578           [[1]]  1.000000           0.083333  \n",
      "14579           [[0]]  0.000000           0.083333  \n",
      "14580           [[0]]  0.000000           0.083333  \n",
      "14581           [[0]]  0.000000           0.083333  \n",
      "14582             NaN       NaN           0.083333  \n",
      "14583             NaN       NaN           0.083333  \n",
      "14584             NaN       NaN           0.083333  \n",
      "14585        [[1, 1]]  1.000000           0.116667  \n",
      "14586             NaN       NaN           0.150000  \n",
      "14587           [[0]]  0.000000           0.150000  \n",
      "14588        [[0, 0]]  0.000000           0.150000  \n",
      "14589     [[1, 0, 0]]  0.333333           0.161111  \n",
      "14590  [[1, 0, 0, 0]]  0.250000           0.169444  \n",
      "14591             NaN       NaN           0.177778  \n",
      "14592        [[0, 0]]  0.000000           0.177778  \n",
      "14593           [[0]]  0.000000           0.177778  \n",
      "14594     [[0, 0, 0]]  0.000000           0.177778  \n",
      "14595     [[0, 0, 0]]  0.000000           0.177778  \n",
      "14596             NaN       NaN           0.177778  \n",
      "14597        [[0, 0]]  0.000000           0.177778  \n",
      "14598        [[0, 0]]  0.000000           0.144444  \n",
      "14599             NaN       NaN           0.144444  \n",
      "\n",
      "[14600 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(company_stack[\"HON\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'imputed_mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3077\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3078\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'imputed_mean'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-274-5dfcde067ff3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompany_stack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HON\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MA_w_imputation'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"imputed_mean\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrolling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m# temp[\"interpolated_mean\"].apply(lambda x: abs(x-1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2489\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3078\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'imputed_mean'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from tabulate import tabulate\n",
    "for i, v in company_stack.items():\n",
    "    print(tabulate(v, headers='keys', tablefmt='psql'))\n",
    "'''\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "\n",
    "temp = company_stack[\"HON\"]\n",
    "\n",
    "temp = company_stack[\"HON\"].dropna()\n",
    "temp['MA_w_imputation'] = temp[\"imputed_mean\"].rolling(window=30).mean()\n",
    "# temp[\"interpolated_mean\"].apply(lambda x: abs(x-1))\n",
    "plt.figure(figsize=(30, 12))    \n",
    "# ax = plt.subplot()\n",
    "# print(temp['MA_no_imputation'].isna().sum())\n",
    "ax.get_xaxis().tick_bottom()    \n",
    "ax.get_yaxis().tick_left()\n",
    "ax.spines[\"top\"].set_visible(False)    \n",
    "ax.spines[\"bottom\"].set_visible(False)    \n",
    "ax.spines[\"right\"].set_visible(False)    \n",
    "ax.spines[\"left\"].set_visible(False)    \n",
    "\n",
    "plt.plot_date(temp[\"Date\"], temp[\"MA_w_imputation\"], \"--\", lw=1, color=\"black\", alpha=1)\n",
    "# plt.plot_date(temp[\"Date\"], temp['MA_w_imputation'], \"--\", lw=0.5, color=\"pink\", alpha=0.3)\n",
    "plt.show()\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Date</th>\n",
       "      <th>emot</th>\n",
       "      <th>mean</th>\n",
       "      <th>imputed_mean</th>\n",
       "      <th>MA_w_imputation</th>\n",
       "      <th>MA_no_imputation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>[hon looking constructive here, gf s work hone...</td>\n",
       "      <td>2009-01-27</td>\n",
       "      <td>[[0, 1]]</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>[]</td>\n",
       "      <td>2009-01-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3620</th>\n",
       "      <td>335</td>\n",
       "      <td>[]</td>\n",
       "      <td>2018-12-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3621</th>\n",
       "      <td>336</td>\n",
       "      <td>[top 1 practice positions by 9 13 cof p 8 55 a...</td>\n",
       "      <td>2018-12-03</td>\n",
       "      <td>[[1, 0]]</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3622</th>\n",
       "      <td>337</td>\n",
       "      <td>[any thoughts on buying back some adbe and hon...</td>\n",
       "      <td>2018-12-04</td>\n",
       "      <td>[[0, 0, 0, 0]]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3623</th>\n",
       "      <td>338</td>\n",
       "      <td>[hon honeywell excellent entry point]</td>\n",
       "      <td>2018-12-05</td>\n",
       "      <td>[[0]]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3624</th>\n",
       "      <td>339</td>\n",
       "      <td>[]</td>\n",
       "      <td>2018-12-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3625</th>\n",
       "      <td>340</td>\n",
       "      <td>[]</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3626</th>\n",
       "      <td>341</td>\n",
       "      <td>[the investing secrets of the richest man the ...</td>\n",
       "      <td>2018-12-08</td>\n",
       "      <td>[[0]]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3627</th>\n",
       "      <td>342</td>\n",
       "      <td>[]</td>\n",
       "      <td>2018-12-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3628</th>\n",
       "      <td>343</td>\n",
       "      <td>[some head and shoulder set ups url sail hon f...</td>\n",
       "      <td>2018-12-10</td>\n",
       "      <td>[[1]]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3629</th>\n",
       "      <td>344</td>\n",
       "      <td>[v hd duk brk b hon yelp t jnj hig jpm xpo luv...</td>\n",
       "      <td>2018-12-11</td>\n",
       "      <td>[[0]]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3630</th>\n",
       "      <td>345</td>\n",
       "      <td>[user hi y n hon i am ulysses nice to meet you...</td>\n",
       "      <td>2018-12-12</td>\n",
       "      <td>[[0]]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3631</th>\n",
       "      <td>346</td>\n",
       "      <td>[ball is in your court pepsico pep executives ...</td>\n",
       "      <td>2018-12-13</td>\n",
       "      <td>[[0]]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3632</th>\n",
       "      <td>347</td>\n",
       "      <td>[]</td>\n",
       "      <td>2018-12-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3633</th>\n",
       "      <td>348</td>\n",
       "      <td>[]</td>\n",
       "      <td>2018-12-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3634</th>\n",
       "      <td>349</td>\n",
       "      <td>[]</td>\n",
       "      <td>2018-12-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3635</th>\n",
       "      <td>350</td>\n",
       "      <td>[mas lowered to 37 at nomura mnk lowered to 22...</td>\n",
       "      <td>2018-12-17</td>\n",
       "      <td>[[1, 1]]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3636</th>\n",
       "      <td>351</td>\n",
       "      <td>[]</td>\n",
       "      <td>2018-12-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3637</th>\n",
       "      <td>352</td>\n",
       "      <td>[made 25k today thanks to the option trades at...</td>\n",
       "      <td>2018-12-19</td>\n",
       "      <td>[[0]]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3638</th>\n",
       "      <td>353</td>\n",
       "      <td>[these 11 stocks are set to rocket higher in 2...</td>\n",
       "      <td>2018-12-20</td>\n",
       "      <td>[[0, 0]]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3639</th>\n",
       "      <td>354</td>\n",
       "      <td>[flight delayed honeywell is working on a fix ...</td>\n",
       "      <td>2018-12-21</td>\n",
       "      <td>[[1, 0, 0]]</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3640</th>\n",
       "      <td>355</td>\n",
       "      <td>[long short bitcoin volatility with up to 1 x ...</td>\n",
       "      <td>2018-12-22</td>\n",
       "      <td>[[1, 0, 0, 0]]</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3641</th>\n",
       "      <td>356</td>\n",
       "      <td>[]</td>\n",
       "      <td>2018-12-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3642</th>\n",
       "      <td>357</td>\n",
       "      <td>[hon honeywell traded at 166 share in 1 2 18 d...</td>\n",
       "      <td>2018-12-24</td>\n",
       "      <td>[[0, 0]]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3643</th>\n",
       "      <td>358</td>\n",
       "      <td>[hard to top hon gotti out here dropping class...</td>\n",
       "      <td>2018-12-25</td>\n",
       "      <td>[[0]]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3644</th>\n",
       "      <td>359</td>\n",
       "      <td>[christmas rally googl fb jpm c bac wfc aapl p...</td>\n",
       "      <td>2018-12-26</td>\n",
       "      <td>[[0, 0, 0]]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3645</th>\n",
       "      <td>360</td>\n",
       "      <td>[call put highest volume changes calls col avg...</td>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>[[0, 0, 0]]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058333</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3646</th>\n",
       "      <td>361</td>\n",
       "      <td>[]</td>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058333</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3647</th>\n",
       "      <td>362</td>\n",
       "      <td>[copy expert traders systematically using cryp...</td>\n",
       "      <td>2018-12-29</td>\n",
       "      <td>[[0, 0]]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058333</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3648</th>\n",
       "      <td>363</td>\n",
       "      <td>[forget 3m ge is a better value stock user sto...</td>\n",
       "      <td>2018-12-30</td>\n",
       "      <td>[[0, 0]]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058333</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3649</th>\n",
       "      <td>364</td>\n",
       "      <td>[]</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3650 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                              Tweet       Date  \\\n",
       "0         0                                                 [] 2009-01-01   \n",
       "1         1                                                 [] 2009-01-02   \n",
       "2         2                                                 [] 2009-01-03   \n",
       "3         3                                                 [] 2009-01-04   \n",
       "4         4                                                 [] 2009-01-05   \n",
       "5         5                                                 [] 2009-01-06   \n",
       "6         6                                                 [] 2009-01-07   \n",
       "7         7                                                 [] 2009-01-08   \n",
       "8         8                                                 [] 2009-01-09   \n",
       "9         9                                                 [] 2009-01-10   \n",
       "10       10                                                 [] 2009-01-11   \n",
       "11       11                                                 [] 2009-01-12   \n",
       "12       12                                                 [] 2009-01-13   \n",
       "13       13                                                 [] 2009-01-14   \n",
       "14       14                                                 [] 2009-01-15   \n",
       "15       15                                                 [] 2009-01-16   \n",
       "16       16                                                 [] 2009-01-17   \n",
       "17       17                                                 [] 2009-01-18   \n",
       "18       18                                                 [] 2009-01-19   \n",
       "19       19                                                 [] 2009-01-20   \n",
       "20       20                                                 [] 2009-01-21   \n",
       "21       21                                                 [] 2009-01-22   \n",
       "22       22                                                 [] 2009-01-23   \n",
       "23       23                                                 [] 2009-01-24   \n",
       "24       24                                                 [] 2009-01-25   \n",
       "25       25                                                 [] 2009-01-26   \n",
       "26       26  [hon looking constructive here, gf s work hone... 2009-01-27   \n",
       "27       27                                                 [] 2009-01-28   \n",
       "28       28                                                 [] 2009-01-29   \n",
       "29       29                                                 [] 2009-01-30   \n",
       "...     ...                                                ...        ...   \n",
       "3620    335                                                 [] 2018-12-02   \n",
       "3621    336  [top 1 practice positions by 9 13 cof p 8 55 a... 2018-12-03   \n",
       "3622    337  [any thoughts on buying back some adbe and hon... 2018-12-04   \n",
       "3623    338              [hon honeywell excellent entry point] 2018-12-05   \n",
       "3624    339                                                 [] 2018-12-06   \n",
       "3625    340                                                 [] 2018-12-07   \n",
       "3626    341  [the investing secrets of the richest man the ... 2018-12-08   \n",
       "3627    342                                                 [] 2018-12-09   \n",
       "3628    343  [some head and shoulder set ups url sail hon f... 2018-12-10   \n",
       "3629    344  [v hd duk brk b hon yelp t jnj hig jpm xpo luv... 2018-12-11   \n",
       "3630    345  [user hi y n hon i am ulysses nice to meet you... 2018-12-12   \n",
       "3631    346  [ball is in your court pepsico pep executives ... 2018-12-13   \n",
       "3632    347                                                 [] 2018-12-14   \n",
       "3633    348                                                 [] 2018-12-15   \n",
       "3634    349                                                 [] 2018-12-16   \n",
       "3635    350  [mas lowered to 37 at nomura mnk lowered to 22... 2018-12-17   \n",
       "3636    351                                                 [] 2018-12-18   \n",
       "3637    352  [made 25k today thanks to the option trades at... 2018-12-19   \n",
       "3638    353  [these 11 stocks are set to rocket higher in 2... 2018-12-20   \n",
       "3639    354  [flight delayed honeywell is working on a fix ... 2018-12-21   \n",
       "3640    355  [long short bitcoin volatility with up to 1 x ... 2018-12-22   \n",
       "3641    356                                                 [] 2018-12-23   \n",
       "3642    357  [hon honeywell traded at 166 share in 1 2 18 d... 2018-12-24   \n",
       "3643    358  [hard to top hon gotti out here dropping class... 2018-12-25   \n",
       "3644    359  [christmas rally googl fb jpm c bac wfc aapl p... 2018-12-26   \n",
       "3645    360  [call put highest volume changes calls col avg... 2018-12-27   \n",
       "3646    361                                                 [] 2018-12-28   \n",
       "3647    362  [copy expert traders systematically using cryp... 2018-12-29   \n",
       "3648    363  [forget 3m ge is a better value stock user sto... 2018-12-30   \n",
       "3649    364                                                 [] 2018-12-31   \n",
       "\n",
       "                emot      mean  imputed_mean  MA_w_imputation  \\\n",
       "0                NaN       NaN      0.000000              NaN   \n",
       "1                NaN       NaN      0.000000              NaN   \n",
       "2                NaN       NaN      0.000000              NaN   \n",
       "3                NaN       NaN      0.000000              NaN   \n",
       "4                NaN       NaN      0.000000              NaN   \n",
       "5                NaN       NaN      0.000000              NaN   \n",
       "6                NaN       NaN      0.000000              NaN   \n",
       "7                NaN       NaN      0.000000              NaN   \n",
       "8                NaN       NaN      0.000000              NaN   \n",
       "9                NaN       NaN      0.000000         0.000000   \n",
       "10               NaN       NaN      0.000000         0.000000   \n",
       "11               NaN       NaN      0.000000         0.000000   \n",
       "12               NaN       NaN      0.000000         0.000000   \n",
       "13               NaN       NaN      0.000000         0.000000   \n",
       "14               NaN       NaN      0.000000         0.000000   \n",
       "15               NaN       NaN      0.000000         0.000000   \n",
       "16               NaN       NaN      0.000000         0.000000   \n",
       "17               NaN       NaN      0.000000         0.000000   \n",
       "18               NaN       NaN      0.000000         0.000000   \n",
       "19               NaN       NaN      0.000000         0.000000   \n",
       "20               NaN       NaN      0.000000         0.000000   \n",
       "21               NaN       NaN      0.000000         0.000000   \n",
       "22               NaN       NaN      0.000000         0.000000   \n",
       "23               NaN       NaN      0.000000         0.000000   \n",
       "24               NaN       NaN      0.000000         0.000000   \n",
       "25               NaN       NaN      0.000000         0.000000   \n",
       "26          [[0, 1]]  0.500000      0.500000         0.050000   \n",
       "27               NaN       NaN      0.000000         0.050000   \n",
       "28               NaN       NaN      0.000000         0.050000   \n",
       "29               NaN       NaN      0.000000         0.050000   \n",
       "...              ...       ...           ...              ...   \n",
       "3620             NaN       NaN      0.000000         0.100000   \n",
       "3621        [[1, 0]]  0.500000      0.500000         0.150000   \n",
       "3622  [[0, 0, 0, 0]]  0.000000      0.000000         0.150000   \n",
       "3623           [[0]]  0.000000      0.000000         0.150000   \n",
       "3624             NaN       NaN      0.000000         0.150000   \n",
       "3625             NaN       NaN      0.000000         0.150000   \n",
       "3626           [[0]]  0.000000      0.000000         0.150000   \n",
       "3627             NaN       NaN      0.000000         0.150000   \n",
       "3628           [[1]]  1.000000      1.000000         0.150000   \n",
       "3629           [[0]]  0.000000      0.000000         0.150000   \n",
       "3630           [[0]]  0.000000      0.000000         0.150000   \n",
       "3631           [[0]]  0.000000      0.000000         0.100000   \n",
       "3632             NaN       NaN      0.000000         0.100000   \n",
       "3633             NaN       NaN      0.000000         0.100000   \n",
       "3634             NaN       NaN      0.000000         0.100000   \n",
       "3635        [[1, 1]]  1.000000      1.000000         0.200000   \n",
       "3636             NaN       NaN      0.000000         0.200000   \n",
       "3637           [[0]]  0.000000      0.000000         0.200000   \n",
       "3638        [[0, 0]]  0.000000      0.000000         0.100000   \n",
       "3639     [[1, 0, 0]]  0.333333      0.333333         0.133333   \n",
       "3640  [[1, 0, 0, 0]]  0.250000      0.250000         0.158333   \n",
       "3641             NaN       NaN      0.000000         0.158333   \n",
       "3642        [[0, 0]]  0.000000      0.000000         0.158333   \n",
       "3643           [[0]]  0.000000      0.000000         0.158333   \n",
       "3644     [[0, 0, 0]]  0.000000      0.000000         0.158333   \n",
       "3645     [[0, 0, 0]]  0.000000      0.000000         0.058333   \n",
       "3646             NaN       NaN      0.000000         0.058333   \n",
       "3647        [[0, 0]]  0.000000      0.000000         0.058333   \n",
       "3648        [[0, 0]]  0.000000      0.000000         0.058333   \n",
       "3649             NaN       NaN      0.000000         0.025000   \n",
       "\n",
       "      MA_no_imputation  \n",
       "0                  NaN  \n",
       "1                  NaN  \n",
       "2                  NaN  \n",
       "3                  NaN  \n",
       "4                  NaN  \n",
       "5                  NaN  \n",
       "6                  NaN  \n",
       "7                  NaN  \n",
       "8                  NaN  \n",
       "9                  NaN  \n",
       "10                 NaN  \n",
       "11                 NaN  \n",
       "12                 NaN  \n",
       "13                 NaN  \n",
       "14                 NaN  \n",
       "15                 NaN  \n",
       "16                 NaN  \n",
       "17                 NaN  \n",
       "18                 NaN  \n",
       "19                 NaN  \n",
       "20                 NaN  \n",
       "21                 NaN  \n",
       "22                 NaN  \n",
       "23                 NaN  \n",
       "24                 NaN  \n",
       "25                 NaN  \n",
       "26                 NaN  \n",
       "27                 NaN  \n",
       "28                 NaN  \n",
       "29                 NaN  \n",
       "...                ...  \n",
       "3620               NaN  \n",
       "3621               NaN  \n",
       "3622               NaN  \n",
       "3623               NaN  \n",
       "3624               NaN  \n",
       "3625               NaN  \n",
       "3626               NaN  \n",
       "3627               NaN  \n",
       "3628               NaN  \n",
       "3629               NaN  \n",
       "3630               NaN  \n",
       "3631               NaN  \n",
       "3632               NaN  \n",
       "3633               NaN  \n",
       "3634               NaN  \n",
       "3635               NaN  \n",
       "3636               NaN  \n",
       "3637               NaN  \n",
       "3638               NaN  \n",
       "3639               NaN  \n",
       "3640               NaN  \n",
       "3641               NaN  \n",
       "3642               NaN  \n",
       "3643               NaN  \n",
       "3644               NaN  \n",
       "3645               NaN  \n",
       "3646               NaN  \n",
       "3647               NaN  \n",
       "3648               NaN  \n",
       "3649               NaN  \n",
       "\n",
       "[3650 rows x 8 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3653\n"
     ]
    }
   ],
   "source": [
    "print(len(emot.index.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
